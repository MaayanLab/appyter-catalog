{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%appyter init\n",
    "import os, sys; sys.path.insert(0, os.path.realpath('..'))\n",
    "from appyter import magic\n",
    "magic.init(lambda _=globals: _())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Imports\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "## Machine Learning\n",
    "import sklearn as sk\n",
    "from sklearn import (\n",
    "    calibration,\n",
    "    decomposition,\n",
    "    ensemble,\n",
    "    feature_selection,\n",
    "    linear_model,\n",
    "    manifold,\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    multioutput,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    "    tree,\n",
    "    feature_extraction,\n",
    "    neural_network,\n",
    ")\n",
    "from split import StratifiedGroupKFold, RepeatedStratifiedGroupKFold\n",
    "import umap\n",
    "## Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "## Drugmonizome API\n",
    "from drugmonizome import Drugmonizome\n",
    "## SEP-L1000 data retrieval\n",
    "from sepl1000 import SEPL1000\n",
    "## L1000FWD queries\n",
    "import querysepl1000fwd\n",
    "## Match drug name inputs using PubChem API\n",
    "from DrugNameConverter import DrugNameConverter\n",
    "# Utility\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from functools import reduce\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "# Interactive tables\n",
    "from itables import show\n",
    "# Plotly fix\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = 2020\n",
    "np.random.seed(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook display util functions (adapted from Nicole Moiseyev's Patient Cohorts RNA-Seq Viewer appyter)\n",
    "\n",
    "def make_clickable(link):\n",
    "    return f'<a target=\"_blank\" href=\"{link}\">{link}</a>'\n",
    "\n",
    "table_number = 0\n",
    "figure_number = 0\n",
    "def figure_header(label,title):\n",
    "    global table_number\n",
    "    global figure_number\n",
    "    if label == 'Table':\n",
    "        table_number += 1\n",
    "        label = f'Table {table_number}'\n",
    "    elif label == 'Figure':\n",
    "        figure_number += 1\n",
    "        label = f'Figure {figure_number}'\n",
    "    display(HTML(f\"<div style='font-size:1.5rem; padding:1rem 0;'><b>{label}</b>: {title}</div>\"))\n",
    "    \n",
    "def figure_legend(label,title,content=''):\n",
    "    global table_number\n",
    "    global figure_number\n",
    "    if label == 'Table':\n",
    "        label = f'Table {table_number}'\n",
    "    elif label == 'Figure':\n",
    "        label = f'Figure {figure_number}'\n",
    "    display(HTML(f'<style>div.caption {{text-align: center;}}</style><div class=caption><b>{label}</b>: <i>{title}</i>. {content} </div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Input Datasets and Target Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected drug set libraries and phenotypic datasets are downloaded and joined on the compound InChI Key to produce a large input feature matrix. A machine learning model will be trained to predict the specified target labels from these features. This is a binary classification task that can be used to predict compounds that are likely to be associated with the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% do SectionField(\n",
    "    title='Input Dataset Selection',\n",
    "    subtitle='Select the input datasets to use for learning and classification. \\\n",
    "              A model will be trained to predict the target labels from the selected features. \\\n",
    "              If no datasets are selected, default features will be used.',\n",
    "    name='ATTRIBUTES',\n",
    "    img='attributes.png',\n",
    ") %}\n",
    "\n",
    "{% set sepl1000_phenotypic_datasets = MultiCheckboxField(\n",
    "    name='sepl1000_phenotypic_datasets',\n",
    "    label='Transcriptomic and Imaging Datasets after Perturbation (From the SEP-L1000 project)',\n",
    "    description='These input datasets were used previously for side effect prediction (https://maayanlab.net/SEP-L1000/).',\n",
    "    choices=[\n",
    "        'LINCS Gene Expression Signatures',\n",
    "        'GO Transformed Signatures (PAEA)',\n",
    "        'MLPCN Cell Morphological Profiling',\n",
    "    ],\n",
    "    descriptions={\n",
    "        'LINCS Gene Expression Signatures': 'Gene expression signatures for drugs/small molecule compounds in the landmark gene space. The Characteristic Direction (CD) method was used to compute gene expression signatures. Contains 20338 compounds with 978 features (genes).',\n",
    "        'GO Transformed Signatures (PAEA)': 'Gene Ontology (GO) transformed gene expression profiles of drug/small molecule compound perturbations. Principal Angle Enrichment Analysis (PAEA) was used to compute enrichment p-values for each CD signature in the space of all genes against gene sets created from the Gene Ontology including Biological Processes, Cellular Components and Molecular Function. Contains 20337 compounds with 4438 features (GO terms).',\n",
    "        'MLPCN Cell Morphological Profiling': 'Drug/small molecule compound induced cell morphological profiles. Contains 19864 compounds with 812 features (from imaging).',\n",
    "    },\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set sepl1000_structural_datasets = MultiCheckboxField(\n",
    "    name='sepl1000_structural_datasets',\n",
    "    label='Chemical Fingerprints Generated for Compounds from SEP-L1000',\n",
    "    description='These input datasets were used previously for side effect prediction (https://maayanlab.net/SEP-L1000/).',\n",
    "    choices=[\n",
    "        'MACCS Chemical Fingerprint',\n",
    "        'Morgan Chemical Fingerprint',\n",
    "    ],\n",
    "    descriptions={\n",
    "        'MACCS Chemical Fingerprint': '166-bit MACCS chemical fingerprint matrix for drugs/small molecule compounds computed using Open Babel. Contains 41701 compounds with 166 binary features (structural keys).',\n",
    "        'Morgan Chemical Fingerprint': '2048-bit Morgan chemical fingerprints (circular fingerprints) computed using RDKIT with a radius of 4. Contains 19878 compounds and 2048 binary features (hashed bits).',\n",
    "    },\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set exprdatasets = MultiCheckboxField(\n",
    "    name='exprdatasets',\n",
    "    label='L1000FWD (Drug set libraries from Drugmonizome)',\n",
    "    description='Binary features were generated from Drugmonizome drug sets based on top up- and down-regulated genes after perturbation, along with enriched pathways, using data from the L1000 fireworks display (L1000FWD). L1000FWD is a web application that provides interactive visualization of over 16,000 drug and small-molecule induced gene expression signatures.',\n",
    "    choices=[\n",
    "        'L1000FWD Downregulated Signatures',\n",
    "        'L1000FWD Upregulated Signatures',\n",
    "        'L1000FWD Downregulated GO Biological Processes',\n",
    "        'L1000FWD Upregulated GO Biological Process',\n",
    "        'L1000FWD Downregulated GO Cellular Components',\n",
    "        'L1000FWD Upregulated GO Cellular Components',\n",
    "        'L1000FWD Downregulated GO Molecular Function',\n",
    "        'L1000FWD Upregulated GO Molecular Function',\n",
    "        'L1000FWD Downregulated KEGG Pathways',\n",
    "        'L1000FWD Upregulated KEGG Pathways',\n",
    "        'L1000FWD Predicted Side Effects',\n",
    "    ],\n",
    "    descriptions={\n",
    "        'L1000FWD Downregulated GO Biological Processes': 'Downregulated Gene Ontology (GO) Biological Process terms retrieved from querying gene signatures of drugs through Enrichr. Contains 4013 compounds with 1068 binary features (GO terms).',\n",
    "        'L1000FWD Downregulated GO Cellular Components': 'Downregulated Gene Ontology (GO) Cellular Component terms retrieved from querying gene signatures of drugs through Enrichr. Contains 3246 compounds with 157 binary features (GO terms).',\n",
    "        'L1000FWD Downregulated GO Molecular Function': 'Downregulated Gene Ontology (GO) Molecular Function terms retrieved from querying gene signatures of drugs through Enrichr. Contains 2158 compounds with 158 binary features (GO terms).',\n",
    "        'L1000FWD Downregulated KEGG Pathways': 'Downregulated KEGG pathways retrieved from querying gene signatures of drugs through Enrichr. Contains 3309 compounds with 236 binary features (KEGG pathways).',\n",
    "        'L1000FWD Downregulated Signatures': 'Drug-induced downregulated genes extracted from L1000FWD. Contains 4884 compounds with 7622 binary features (genes).',\n",
    "        'L1000FWD Predicted Side Effects': 'Side effect associations predicted by drug-induced gene expression signatures. Contains 4852 compounds with 1013 binary features (predicted side effects).',\n",
    "        'L1000FWD Upregulated GO Biological Process': 'Upregulated Gene Ontology (GO) Biological Process terms retrieved from querying gene signatures of drugs through Enrichr. Contains 4195 compounds with 1228 binary features (GO terms).',\n",
    "        'L1000FWD Upregulated GO Cellular Components': 'Upregulated Gene Ontology (GO) Cellular Component terms retrieved from querying gene signatures of drugs through Enrichr. Contains 3366 compounds with 153 binary features (GO terms).',\n",
    "        'L1000FWD Upregulated GO Molecular Function': 'Upregulated Gene Ontology (GO) Molecular Function terms retrieved from querying gene signatures of drugs through Enrichr. Contains 2427 compounds with 183 binary features (GO terms).',\n",
    "        'L1000FWD Upregulated KEGG Pathways': 'Upregulated KEGG pathways retrieved from querying gene signatures of drugs through Enrichr. Contains 3662 compounds with 245 binary features (KEGG pathways).',\n",
    "        'L1000FWD Upregulated Signatures': 'Drug-induced upregulated genes extracted from L1000FWD. Contains 4884 compounds with 7611 binary features (genes).',\n",
    "    },\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set targetdatasets = MultiCheckboxField(\n",
    "    name='targetdatasets',\n",
    "    label='Drug Targets and Associated Genes (Drug set libraries from Drugmonizome)',\n",
    "    description='Binary features were generated from Drugmonizome drug sets based on known drug targets and associated genes from literature.',\n",
    "    choices=[\n",
    "        'Downregulated CREEDS Signatures',\n",
    "        'Upregulated CREEDS Signatures',\n",
    "        'DrugCentral Targets',\n",
    "        'DrugRepurposingHub Drug Targets',\n",
    "        'Drugbank Small Molecule Carriers',\n",
    "        'Drugbank Small Molecule Enzymes',\n",
    "        'Drugbank Small Molecule Targets',\n",
    "        'Drugbank Small Molecule Transporters',\n",
    "        'Geneshot Associated Genes',\n",
    "        'Geneshot Predicted AutoRIF Genes',\n",
    "        'Geneshot Predicted Coexpression Genes',\n",
    "        'Geneshot Predicted Enrichr Genes',\n",
    "        'Geneshot Predicted GeneRIF Genes',\n",
    "        'Geneshot Predicted Tagger Genes',\n",
    "        'KinomeScan Kinases',\n",
    "        'PharmGKB Single Nucleotide Polymorphisms',\n",
    "        'STITCH Targets',\n",
    "    ],\n",
    "    descriptions={\n",
    "        'Downregulated CREEDS Signatures': 'Downregulated drug-induced gene expression signatures from CREEDS, a crowdsourcing resource for the curation and reanalysis of gene expression profiles from GEO. Contains 72 compounds with 2532 binary features (genes).',\n",
    "        'Upregulated CREEDS Signatures': 'Upregulated drug-induced gene expression signatures from CREEDS, a crowdsourcing resource for the curation and reanalysis of gene expression profiles from GEO. Contains 71 compounds with 2535 binary features (genes).',\n",
    "        'DrugCentral Targets': 'Drug targets for approved and unapproved drugs curated from the literature. Contains 1555 compounds with 540 binary features (genes).',\n",
    "        'DrugRepurposingHub Drug Targets': 'Associated drug targets of approved drugs and drugs in clinical trials. Contains 1720 compounds with 375 binary features (genes).',\n",
    "        'Drugbank Small Molecule Carriers': 'Genes encoding carriers associated with Drugbank small molecules. Contains 458 compounds with 14 binary features (genes).',\n",
    "        'Drugbank Small Molecule Enzymes': 'Genes encoding enzymes associated with Drugbank small molecules. Contains 1473 compounds with 72 binary features (genes).',\n",
    "        'Drugbank Small Molecule Targets': 'Drug targets of Drugbank small molecules. Contains 4467 compounds with 611 binary features (genes).',\n",
    "        'Drugbank Small Molecule Transporters': 'Genes encoding transporters associated with Drugbank small molecules. Contains 832 compounds with 51 binary features (genes).',\n",
    "        'Geneshot Associated Genes': 'Associated genes based on co-mentions with drugs in the literature. Contains 3938 compounds with 7503 binary features (genes).',\n",
    "        'Geneshot Predicted AutoRIF Genes': 'Predicted genes based on AutoRIF co-occurrence. Contains 3938 compounds with 11695 binary features (genes).',\n",
    "        'Geneshot Predicted Coexpression Genes': 'Predicted genes based on ARCHS4 coexpression. Contains 3938 compounds with 9087 binary features (genes).',\n",
    "        'Geneshot Predicted Enrichr Genes': 'Predicted genes based on Enrichr co-occurrence. Contains 3938 compounds with 11845 binary features (genes).',\n",
    "        'Geneshot Predicted GeneRIF Genes': 'Predicted genes based on GeneRIF co-occurrence. Contains 3938 compounds with 9193 binary features (genes).',\n",
    "        'Geneshot Predicted Tagger Genes': 'Predicted genes based on Tagger co-occurrence. Contains 3938 compounds with 13882 binary features (genes).',\n",
    "        'KinomeScan Kinases': 'Kinases associated with drugs elucidated from KINOMEscan kinase profiling assay. KINOMEscan is a biochemical kinase profiling assay that measures drug binding using a panel of ~440 purified kinases. Contains 54 compounds with 301 binary features (genes).',\n",
    "        'PharmGKB Single Nucleotide Polymorphisms': 'Potentially clinically actionable gene-SNP associations. Contains 483 compounds with 554 binary features (SNPs).',\n",
    "        'STITCH Targets': 'Gene-drug interactions from computational prediction and aggregation from primary databases. Contains 7303 compounds with 9063 binary features (genes).',\n",
    "    },\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set indicationdatasets = MultiCheckboxField(\n",
    "    name='indicationdatasets',\n",
    "    label='Indications, Modes of Action, and Side Effects (Drug set libraries from Drugmonizome)',\n",
    "    description='Binary features were generated from Drugmonizome drug sets based on known mechanisms of action and side effects.',\n",
    "    choices=[\n",
    "        'ATC Codes Drugsetlibrary',\n",
    "        'DrugRepurposingHub Mechanisms of Action',\n",
    "        'PharmGKB OFFSIDES Side Effects',\n",
    "        'SIDER Indications',\n",
    "        'SIDER Side Effects',\n",
    "    ],\n",
    "    descriptions={\n",
    "        'ATC Codes Drugsetlibrary': 'A classification system used to organize chemicals by chemical, therapeutic, pharmacological subgroups, cut off at the fourth level. Contains 2233 compounds with 308 binary features (mechanisms of action).',\n",
    "        'DrugRepurposingHub Mechanisms of Action': 'Associated mechanisms of action of approved drugs and drugs in clinical trials. Contains 1854 compounds with 154 binary features (mechanisms of action).',\n",
    "        'PharmGKB OFFSIDES Side Effects': 'Side effects mined from adverse event reporting databases predicted by a unique detection algorithm. Contains 1435 compounds with 7137 binary features (side effects).',\n",
    "        'SIDER Indications': 'Approved drug indications mined from FDA package inserts and public documents. Contains 1546 compounds with 867 binary features (mechanisms of action).',\n",
    "        'SIDER Side Effects': 'Approved drug side effects mined from FDA package inserts and public documents. Contains 1635 compounds with 2078 binary features (side effects).',\n",
    "    },\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set structuraldatasets = MultiCheckboxField(\n",
    "    name='structuraldatasets',\n",
    "    label='Structural Features (Drug set libraries from Drugmonizome)',\n",
    "    description='Binary features were generated from Drugmonizome drug sets based on molecular fingerprints.',\n",
    "    choices=[\n",
    "        'RDKIT MACCS Chemical Fingerprints',\n",
    "        'PubChem Chemical Fingerprints',\n",
    "    ],\n",
    "    descriptions={\n",
    "        'RDKIT MACCS Chemical Fingerprints': 'Chemical structure motifs generated from SMILEs strings of small molecules. Computed for Drugmonizome compounds using RDKIT. Contains 14308 compounds with 163 binary features (chemical structure motifs).',\n",
    "        'PubChem Chemical Fingerprints': '881-bit PubChem chemical structure motifs generated from SMILEs strings of small molecules. Contains 13379 compounds with 669 binary features (chemical structure motifs).',\n",
    "    },\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set keepmissing = BoolField(\n",
    "    name='keepmissing',\n",
    "    label='Keep drugs with missing data when joining datasets',\n",
    "    description='Keep drugs that appear in some datasets and not in others. \\\n",
    "                 Missing data is filled in with zeros. Otherwise, only drugs \\\n",
    "                 that are present in all datasets are preserved.',\n",
    "    default=False,\n",
    "    section='ATTRIBUTES',\n",
    ") %}\n",
    "\n",
    "{% set tfidf = BoolField(\n",
    "    name='tfidf',\n",
    "    label='Apply tf–idf normalization to binary inputs',\n",
    "    description='For binary drug-attribute associations in the input matrix, \\\n",
    "                 apply tf-idf transformation to normalize data.',\n",
    "    default=True,\n",
    "    section='ATTRIBUTES',\n",
    ") %}\n",
    "\n",
    "{% set attribute_datasets = exprdatasets.value +\n",
    "                             targetdatasets.value +\n",
    "                             indicationdatasets.value +\n",
    "                             structuraldatasets.value %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "To construct the input matrix, we download drug set libraries and phenotypic datasets and join them on the InChI Key.\n",
    "{% if keepmissing.value %} Drugs that appear in some datasets and not in others are retained, and missing data is filled in with zeros.\n",
    "{% else %} Only drugs that are present in all datasets are retained.\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% do SectionField(\n",
    "    title='Target Label Selection',\n",
    "    subtitle='Upload a list of compounds or select an attribute from Drugmonizome to be assigned a positive class label for binary classification.',\n",
    "    name='TARGET',\n",
    "    img='target.png',\n",
    ") %}\n",
    "\n",
    "{% set target_field = TabField(\n",
    "    name='target_field',\n",
    "    label='Target Selection',\n",
    "    default='Attribute',\n",
    "    description='Select input method',\n",
    "    choices={\n",
    "        'List': [\n",
    "            ChoiceField(\n",
    "                name='drugformat',\n",
    "                label='Drug Identifier Format',\n",
    "                description='Compounds can be specified by either drug name or InChI Key.',\n",
    "                default='InChI Key',\n",
    "                choices=[\n",
    "                    'Drug Name',\n",
    "                    'InChI Key'\n",
    "                ],\n",
    "                section='TARGET'\n",
    "            ),\n",
    "            FileField(\n",
    "                name='drughitlist',\n",
    "                label='Upload List of Compounds',\n",
    "                description='Upload a list of compounds to be assigned positive class labels for binary classification. \\\n",
    "                             Compounds should be in a text file, specified by either drug name or InChI Key and separated by newlines.',\n",
    "                default='COVID19ScreenHitsInChIKeys.txt',\n",
    "                examples={\n",
    "                    'COVID19ScreenHits.txt': 'https://appyters.maayanlab.cloud/storage/Drugmonizome_ML/COVID19ScreenHits.txt',\n",
    "                    'COVID19ScreenHitsInChIKeys.txt': 'https://appyters.maayanlab.cloud/storage/Drugmonizome_ML/COVID19ScreenHitsInChIKeys.txt',\n",
    "                },\n",
    "                section='TARGET'\n",
    "            ),\n",
    "        ],\n",
    "        'Attribute': [\n",
    "            AutocompleteField(\n",
    "                name='target_attribute',\n",
    "                description='Enter a small molecule attribute from one of the Drugmonizome datasets that should be predicted.',\n",
    "                file_path=\"https://appyters.maayanlab.cloud/storage/Drugmonizome_ML/drugmonizome_terms.json\",\n",
    "                label='Attribute',\n",
    "                hint='Enter Drugmonizome term...',\n",
    "                default='neuropathy peripheral (from SIDER Side Effects)',\n",
    "                constraint='(^(.+) \\\\(from (.+)\\\\)$|^$)',\n",
    "        )],\n",
    "    },\n",
    "    section='TARGET',\n",
    ") %}\n",
    "\n",
    "{% set includestereo = BoolField(\n",
    "    name='includestereo',\n",
    "    label='Include stereoisomers',\n",
    "    description='If true, compounds are matched to entries in the datasets by the first 14 characters of their InChI Keys, \\\n",
    "                 so stereoisomers of the compounds in the input list or with a particular attritube are also counted as hits. \\\n",
    "                 Note that different resources record different details for charge and stereochemistry, \\\n",
    "                 causing some compounds to have different full-length InChI Keys in different datasets. \\\n",
    "                 Selecting this option may allow such drugs to be better matched to entries in the datasets.',\n",
    "    default=False,\n",
    "    section='TARGET',\n",
    ") %}\n",
    "\n",
    "{% set target_name, target_dataset = '', '' %}\n",
    "{% if target_field.raw_value == 'Attribute' %}\n",
    "{% set target_name, target_dataset = target_field.value[0].value|re_match('^(.+) \\\\(from (.+)\\\\)$') %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if sepl1000_phenotypic_datasets.value == [] and sepl1000_structural_datasets.value == [] and attribute_datasets == [] %}\n",
    "# No datasets selected, so use default datasets\n",
    "{% set sepl1000_phenotypic_datasets, sepl1000_structural_datasets = ['LINCS Gene Expression Signatures'], ['Morgan Chemical Fingerprint'] %}\n",
    "sepl1000_phenotypic_datasets = {{ sepl1000_phenotypic_datasets }}\n",
    "sepl1000_structural_datasets = {{ sepl1000_structural_datasets }}\n",
    "{% else %}\n",
    "# Use the selected SEP-L1000 datasets\n",
    "sepl1000_phenotypic_datasets = {{ sepl1000_phenotypic_datasets }}\n",
    "sepl1000_structural_datasets = {{ sepl1000_structural_datasets }}\n",
    "{% endif %}\n",
    "\n",
    "dataset_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "{% if sepl1000_phenotypic_datasets.value != [] %}\n",
    "phenotypic_datasets = {\n",
    "    'LINCS Gene Expression Signatures': 'LINCS_Gene_Experssion_signatures_CD.csv.gz',\n",
    "    'GO Transformed Signatures (PAEA)': 'GO_transformed_signatures_PAEA.csv.gz',\n",
    "    'MLPCN Cell Morphological Profiling': 'MLPCN_morplological_profiles.csv.gz'\n",
    "}\n",
    "\n",
    "df_sepl1000_phenotypic = list(SEPL1000.download_df(list(phenotypic_datasets[dataset] for dataset in sepl1000_phenotypic_datasets),\n",
    "                                             index_col=0))\n",
    "dataset_sizes += list(zip(sepl1000_phenotypic_datasets, [dataset.shape[1] for dataset in df_sepl1000_phenotypic]))\n",
    "\n",
    "# Assemble all phenotypic SEP-L1000 datasets\n",
    "if len(df_sepl1000_phenotypic) > 1:\n",
    "    # Obtain merged dataframe with omics and target data\n",
    "    df_sepl1000 = reduce(\n",
    "        lambda a, b: pd.merge( # Merge two dataframes item by item\n",
    "            a, # left\n",
    "            b, # right\n",
    "            # Items with the same left and right index are merged\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            {% if keepmissing.value %}\n",
    "            how='outer', # Keep mis-matched indices\n",
    "            {% else %}\n",
    "            how='inner', # Keep only matched indices\n",
    "            {% endif %}\n",
    "        ),\n",
    "        df_sepl1000_phenotypic,\n",
    "    )\n",
    "else:\n",
    "    df_sepl1000 = df_sepl1000_phenotypic[0]\n",
    "    \n",
    "# Mean-fill infinite and missing values\n",
    "df_sepl1000 = df_sepl1000.replace([np.inf, -np.inf], np.nan)\n",
    "df_sepl1000 = df_sepl1000.fillna(np.mean(df_sepl1000))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "{% if sepl1000_structural_datasets.value != [] %}\n",
    "# Structural dataset processing\n",
    "structural_datasets = {\n",
    "    'MACCS Chemical Fingerprint': 'MACCS_bitmatrix.csv.gz',\n",
    "    'Morgan Chemical Fingerprint': 'Morgan_bitmatrix.csv.gz',\n",
    "}\n",
    "\n",
    "df_sepl1000_structural = list(SEPL1000.download_df(list(structural_datasets[dataset] for dataset in sepl1000_structural_datasets),\n",
    "                                             index_col=0))\n",
    "dataset_sizes += list(zip(sepl1000_structural_datasets, [dataset.shape[1] for dataset in df_sepl1000_structural]))\n",
    "\n",
    "# Assemble all structural SEP-L1000 datasets\n",
    "if len(df_sepl1000_structural) > 1:\n",
    "    # Obtain merged dataframe with omics and target data\n",
    "    df_sepl1000_fingerprints = reduce(\n",
    "        lambda a, b: pd.merge( # Merge two dataframes item by item\n",
    "            a, # left\n",
    "            b, # right\n",
    "            # Items with the same left and right index are merged\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            {% if keepmissing.value %}\n",
    "            how='outer', # Keep mis-matched indices\n",
    "            {% else %}\n",
    "            how='inner', # Keep only matched indices\n",
    "            {% endif %}\n",
    "        ),\n",
    "        df_sepl1000_structural,\n",
    "    )\n",
    "else:\n",
    "    df_sepl1000_fingerprints = df_sepl1000_structural[0]\n",
    "\n",
    "{% if tfidf.value %}\n",
    "# Apply tf-idf normalization\n",
    "transformer = feature_extraction.text.TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(df_sepl1000_fingerprints).toarray()\n",
    "df_sepl1000_fingerprints = pd.DataFrame(X_tfidf, columns=df_sepl1000_fingerprints.columns, index=df_sepl1000_fingerprints.index)\n",
    "{% endif %}\n",
    "{% if sepl1000_phenotypic_datasets.value != [] %}\n",
    "# Concatenate structural features with phenotypic features\n",
    "{% if keepmissing.value %}\n",
    "df_sepl1000 = pd.merge(df_sepl1000, df_sepl1000_fingerprints, left_index=True, right_index=True, how='outer') # Keep mis-matched indices\n",
    "{% else %}\n",
    "df_sepl1000 = pd.merge(df_sepl1000, df_sepl1000_fingerprints, left_index=True, right_index=True) # Keep only matched indices\n",
    "{% endif %}\n",
    "{% else %}\n",
    "df_sepl1000 = df_sepl1000_fingerprints\n",
    "{% endif %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "# Use the selected attribute datasets\n",
    "attribute_datasets = {{ attribute_datasets }}\n",
    "\n",
    "{% if attribute_datasets == [] %}\n",
    "X = df_sepl1000\n",
    "{% else %}\n",
    "df_attributes = list(Drugmonizome.download_df(\n",
    "    [dataset\n",
    "     for dataset in attribute_datasets]\n",
    "))\n",
    "dataset_sizes += list(zip(attribute_datasets, [dataset.shape[1] for dataset in df_attributes]))\n",
    "\n",
    "# Assemble all attribute datasets\n",
    "if len(df_attributes) > 1:\n",
    "    # Obtain merged dataframe with omics and target data\n",
    "    df = reduce(\n",
    "        lambda a, b: pd.merge( # Merge two dataframes item by item\n",
    "            a, # left\n",
    "            b, # right\n",
    "            # Items with the same left and right index are merged\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            {% if keepmissing.value %}\n",
    "            how='outer', # Keep mis-matched indices\n",
    "            {% else %}\n",
    "            how='inner', # Keep only matched indices\n",
    "            {% endif %}\n",
    "        ),\n",
    "        df_attributes,\n",
    "    )\n",
    "else:\n",
    "    df = df_attributes[0]\n",
    "\n",
    "del(df_attributes)\n",
    "\n",
    "df = df.fillna(0)\n",
    "X = df.applymap(lambda f: 1 if f!=0 else 0)\n",
    "{% if tfidf.value %}\n",
    "# Apply tf-idf normalization\n",
    "transformer = feature_extraction.text.TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X).toarray()\n",
    "X = pd.DataFrame(X_tfidf, columns=X.columns, index=X.index)\n",
    "{% if sepl1000_phenotypic_datasets.value != [] or sepl1000_structural_datasets.value != [] %}\n",
    "{% if keepmissing.value %}\n",
    "X = pd.merge(df_sepl1000, X, left_index=True, right_index=True, how='outer') # Keep mis-matched indices\n",
    "{% else %}\n",
    "X = pd.merge(df_sepl1000, X, left_index=True, right_index=True) # Keep only matched indices\n",
    "{% endif %}\n",
    "{% endif %}\n",
    "{% endif %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View input data\n",
    "figure_header('Table', 'Input data')\n",
    "display(X.head())\n",
    "figure_legend('Table', 'Input data',\n",
    "              f'The input data contain {X.shape[0]} compounds and {X.shape[1]} features per compound, \\\n",
    "              taken from the following datasets: {\", \".join(sepl1000_phenotypic_datasets + sepl1000_structural_datasets + attribute_datasets)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if target_field.raw_value == 'List' %}\n",
    "The target labels are produced from the uploaded list of hits: 1 if the drug is specified as a hit, 0 otherwise.\n",
    "{% if target_field.value[0].value == 'Drug Name' %} Drug names are matched to InChI Keys from PubChem, L1000FWD, and the Drugmonizome metadata.\n",
    "{% endif %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if target_field.raw_value == 'List' %}\n",
    "{% if target_field.value[1].value == '' %}\n",
    "# Using default list of hits from COVID-19 in vitro drug screens\n",
    "hits_filename = '../../COVID19ScreenHits.txt'\n",
    "{% else %}\n",
    "# Using user-specified list of positive drug hits\n",
    "hits_filename = {{target_field.value[1]}}\n",
    "{% endif %}\n",
    "\n",
    "{% if target_field.value[0].value == 'InChI Key' %}\n",
    "def save_items(out_file, items):\n",
    "    \"\"\"\n",
    "    Saves list of items as rows in a file.\n",
    "    \"\"\"\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(len(items)):\n",
    "            if i < len(items) - 1:\n",
    "                f.write(items[i] + '\\n')\n",
    "            else:\n",
    "                f.write(items[i])\n",
    "                \n",
    "# Read InChI Keys from file\n",
    "with open(hits_filename, 'r') as hits_file:\n",
    "    drug_hits = set(drug.strip().upper() for drug in hits_file.read().strip().split('\\n') \n",
    "                    if len(drug.strip()) > 0)\n",
    "\n",
    "{% elif target_field.value[0].value == 'Drug Name' %}\n",
    "# Helper functions\n",
    "def merge(A, B, f):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries, where items from shared keys are merged using a custom function.\n",
    "    \"\"\"\n",
    "    merged = {k: A.get(k, B.get(k)) for k in A.keys() ^ B.keys()}\n",
    "    merged.update({k: f(A[k], B[k]) for k in A.keys() & B.keys()})\n",
    "    return merged\n",
    "\n",
    "def save_items(out_file, items):\n",
    "    \"\"\"\n",
    "    Saves list of items as rows in a file.\n",
    "    \"\"\"\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(len(items)):\n",
    "            if i < len(items) - 1:\n",
    "                f.write(items[i] + '\\n')\n",
    "            else:\n",
    "                f.write(items[i])\n",
    "\n",
    "def save_gmt(out_file, keys_to_sets, sep='\\t'):\n",
    "    \"\"\"\n",
    "    Saves dict with key-set pairs as gmt file format.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for key in sorted(keys_to_sets):\n",
    "        lines.append(key + sep*2 + sep.join(sorted(keys_to_sets[key])))\n",
    "    save_items(out_file, lines)\n",
    "\n",
    "# Read drug names from file\n",
    "with open(hits_filename, 'r') as hits_file:\n",
    "    drug_hits = set(drug.strip().lower() for drug in hits_file.read().strip().split('\\n') \n",
    "                    if len(drug.strip()) > 0)\n",
    "\n",
    "# Query PubChem API to map drug names to InChI Keys\n",
    "print('Querying PubChem API...')\n",
    "drug_hits_inchi_pubchem = DrugNameConverter.batch_to_inchi_keys(drug_hits)\n",
    "# Query Drugmonizome API to map drug names to InChI Keys\n",
    "print('Querying Drugmonizome API...')\n",
    "drug_hits_inchi_drugmonizome = Drugmonizome.map_names_to_inchi_keys(drug_hits)\n",
    "# Query L1000FWD API to map drug names to InChI Keys\n",
    "print('Querying L1000FWD API...')\n",
    "drug_hits_inchi_l1000fwd = querysepl1000fwd.map_names_to_inchi_keys(drug_hits)\n",
    "\n",
    "# Combine InChI Keys from all resources\n",
    "drug_hits_inchi = merge(drug_hits_inchi_pubchem, drug_hits_inchi_drugmonizome, lambda s1, s2: s1 | s2)\n",
    "drug_hits_inchi = merge(drug_hits_inchi, drug_hits_inchi_l1000fwd, lambda s1, s2: s1 | s2)\n",
    "save_gmt('hits_drug_name_to_inchi_keys.gmt', drug_hits_inchi)\n",
    "# Unmatched drug names\n",
    "unmatched_drugs = set(drug for drug in drug_hits\n",
    "                      if drug not in drug_hits_inchi or len(drug_hits_inchi[drug]) == 0)\n",
    "print(f'Drugs without InChI Keys ({ len(unmatched_drugs) }/{ len(drug_hits) }):', unmatched_drugs)\n",
    "\n",
    "# Set of InChI Keys for user-specified hits\n",
    "drug_hits = set(key for drug in drug_hits_inchi\n",
    "                    for key in drug_hits_inchi[drug])\n",
    "save_items('hits_inchi_keys.txt', sorted(drug_hits))\n",
    "{% endif %}\n",
    "\n",
    "{% else %}\n",
    "\n",
    "df_target = list(Drugmonizome.download_df(\n",
    "    ['{{ target_dataset }}']\n",
    "))\n",
    "df = df_target[0]\n",
    "df = df.fillna(0)\n",
    "Y = df.applymap(lambda f: 1 if f!=0 else 0)\n",
    "drug_hits = set(Y[Y['{{ target_name }}'] == 1].index)\n",
    "\n",
    "# Helper function\n",
    "def save_items(out_file, items):\n",
    "    \"\"\"\n",
    "    Saves list of items as rows in a file.\n",
    "    \"\"\"\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(len(items)):\n",
    "            if i < len(items) - 1:\n",
    "                f.write(items[i] + '\\n')\n",
    "            else:\n",
    "                f.write(items[i])\n",
    "save_items('hits_inchi_keys.txt', sorted(drug_hits))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if target_field.raw_value == 'List' %}\n",
    "{% if target_field.value[0].value == 'Drug Name' %}\n",
    "For the user-inputted drug names:\n",
    "* Mapping of drug name to InChI Key: [hits_drug_name_to_inchi_keys.gmt](./hits_drug_name_to_inchi_keys.gmt)\n",
    "* List of InChI Keys: [hits_inchi_keys.txt](./hits_inchi_keys.txt)\n",
    "{% endif %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if target_field.raw_value == 'List' %}\n",
    "We produce a target array containing 1 if the compound is specified as a hit and 0 otherwise.\n",
    "{% else %}\n",
    "We produce a target array containing 1 if the compound is associated with the attribute _{{ target_name }}_ in the Drugmonizome resource _{{ target_dataset }}_ and 0 otherwise.\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if includestereo.value %}\n",
    "# Match first 14 characters of InChI Keys (hash of InChI connectivity information)\n",
    "drug_hits_inchi_main_layer = set(key[:14] for key in drug_hits)\n",
    "y = np.array([drug[:14] in drug_hits_inchi_main_layer for drug in X.index]).astype(np.int8)\n",
    "unmatched = list(set([drug[:14] for drug in drug_hits]) - set(drug[:14] for drug in X.index))\n",
    "{% else %}\n",
    "# Match full InChI Keys\n",
    "y = np.array([drug in drug_hits for drug in X.index]).astype(np.int8)\n",
    "unmatched = list(set(drug_hits) - set(X.index))\n",
    "{% endif %}\n",
    "save_items('unmatched_inchikeys.txt', unmatched)\n",
    "print('Number of hits matched in input: %d (%0.3f %%)' % (y.sum(), 100*y.sum()/len(y)))\n",
    "print('Number of unmatched hits: %d' % (len(unmatched)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "* File of unmatched InChI keys: [unmatched_inchikeys.txt](./unmatched_inchikeys.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data shapes\n",
    "print('Input shape:', X.shape)\n",
    "print('Target shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% do SectionField(\n",
    "    title='Machine Learning Pipeline',\n",
    "    subtitle='Select from available machine learning algorithms, their unique settings, and methods to use to evaluate the classifier.',\n",
    "    name='SETTINGS',\n",
    "    img='settings.png',\n",
    ") %}\n",
    "\n",
    "{% set visualization_reduction = ChoiceField(\n",
    "    name='visualization_reduction',\n",
    "    label='Data Visualization Method',\n",
    "    description='Select a dimensionality reduction algorithm for data visualization.',\n",
    "    default='UMAP',\n",
    "    choices={\n",
    "        'UMAP': 'umap.UMAP(low_memory=True, random_state=rng)',\n",
    "        'NMF': 'sk.decomposition.NMF(n_components=2)',\n",
    "        'PCA': 'sk.decomposition.PCA(n_components=2)',\n",
    "        'TruncatedSVD': 'sk.decomposition.TruncatedSVD(n_components=2)',\n",
    "        'IncrementalPCA': 'sk.decomposition.IncrementalPCA(n_components=2)',\n",
    "        'ICA': 'sk.decomposition.FastICA(n_components=2)',\n",
    "        'SparsePCA': 'sk.decomposition.SparsePCA(n_components=2)',\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "We reduce the dimensionality of our omics feature space for visualization with {{ visualization_reduction.raw_value }}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "clf_dimensionality_reduction = {{ visualization_reduction }}\n",
    "X_reduced = clf_dimensionality_reduction.fit_transform(X.values)\n",
    "{% if visualization_reduction.raw_value == 'PCA' %}\n",
    "print('Explained variance:', np.sum(clf_dimensionality_reduction.explained_variance_))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_df = pd.DataFrame(X_reduced, columns=['Component 1', 'Component 2'])\n",
    "X_reduced_df['Drug Name'] = querysepl1000fwd.get_drug_names(X.index)\n",
    "X_reduced_df['InChI Key'] = X.index\n",
    "X_reduced_df['Label'] = y\n",
    "X_reduced_df['marker symbol'] = ['x' if label else 'circle' for label in X_reduced_df['Label']]\n",
    "X_reduced_df['text'] = ['<br>'.join(['Drug Name: ' + str(name),\n",
    "                                     'InChI Key: ' + str(inchi),\n",
    "                                     'Label: ' + str(label)])\n",
    "                        for name, inchi, label in zip(X_reduced_df['Drug Name'],\n",
    "                                                      X_reduced_df['InChI Key'],\n",
    "                                                      X_reduced_df['Label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "fig = go.Figure()\n",
    "for label in set(X_reduced_df['Label']):\n",
    "    X_plot = X_reduced_df[X_reduced_df['Label'] == label].sort_values('Label')\n",
    "    fig.add_trace(go.Scatter(mode='markers',\n",
    "                             x=X_plot['Component 1'], y=X_plot['Component 2'],\n",
    "                             text=X_plot['text'],\n",
    "                             name=label,\n",
    "                             marker=dict(\n",
    "                                 color=['#0d0887', '#f0f921'][label%2],\n",
    "                                 size=8,\n",
    "                                 symbol=X_plot['marker symbol'],\n",
    "                                 line_width=1,\n",
    "                                 line_color='white'\n",
    "                             )))\n",
    "fig.update_layout(height=600, width=800,\n",
    "                  xaxis_title='Component 1',\n",
    "                  yaxis_title='Component 2',\n",
    "                  title_text='Known Labels ({{ visualization_reduction.raw_value }})',\n",
    "                  legend_title_text='Target Label',\n",
    "                  template='simple_white')\n",
    "figure_header('Figure', 'Input feature space with {{ visualization_reduction.raw_value }} dimensionality reduction')\n",
    "fig.show()\n",
    "figure_legend('Figure', 'Input feature space with {{ visualization_reduction.raw_value }} dimensionality reduction',\n",
    "              f'Each point represents one of {X.shape[0]} compounds, with {X.shape[1]} features per compound, \\\n",
    "              taken from the following datasets: {\", \".join(sepl1000_phenotypic_datasets + sepl1000_structural_datasets + attribute_datasets)}. \\\n",
    "              Compounds with known positive labels are marked by X\\'s.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train and evaluate a machine learning model across multiple cross-validation splits by randomly dividing the input dataset into training and validation sets. For each round of cross-validation, a model is trained on the training set and is then used to make predictions for the compounds in the validation set. Each compound appears in at least one validation set, so the validation set predictions are used to assess model performance based on existing labels and to suggest novel predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% set dimensionality_reduction = ChoiceField(\n",
    "    name='dimensionality_reduction',\n",
    "    label='Dimensionality Reduction Algorithm',\n",
    "    description='Optionally select a dimensionality reduction algorithm as a data preprocessing step in the ML pipeline.',\n",
    "    default='None',\n",
    "    choices={\n",
    "        'None': 'None',\n",
    "        'PCA': 'sk.decomposition.PCA(n_components=64)',\n",
    "        'TruncatedSVD': 'sk.decomposition.TruncatedSVD(n_components=64)',\n",
    "        'IncrementalPCA': 'sk.decomposition.IncrementalPCA(n_components=64)',\n",
    "        'ICA': 'sk.decomposition.FastICA(n_components=64)',\n",
    "        'SparsePCA': 'sk.decomposition.SparsePCA(n_components=64)',\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set feature_selection = ChoiceField(\n",
    "    name='feature_selection',\n",
    "    label='Machine Learning Feature Selection',\n",
    "    description='Optionally select a feature selection algorithm to include in the ML pipeline. \\\n",
    "                 If RecursiveSelectionFromExtraTrees is chosen, additional information can be obtained \\\n",
    "                 on the relative importance of different features based on which features are eliminated.',\n",
    "    default='None',\n",
    "    choices={\n",
    "        'None': 'None',\n",
    "        'SelectFromLinearSVC': 'sk.feature_selection.SelectFromModel(sk.svm.LinearSVC(loss=\"squared_hinge\", penalty=\"l1\", dual=False, class_weight=\"balanced\"))',\n",
    "        'SelectFromExtraTrees': 'sk.feature_selection.SelectFromModel(sk.ensemble.ExtraTreesClassifier(class_weight=\"balanced\"))',\n",
    "        'RecursiveSelectionFromExtraTrees': 'sk.feature_selection.RFE(sk.ensemble.ExtraTreesClassifier(class_weight=\"balanced\"), n_features_to_select=256, step=0.1)',\n",
    "        'SelectKBest': 'sk.feature_selection.SelectKBest(\"f_classif\")',\n",
    "        'SelectKBestChi2': 'sk.feature_selection.SelectKBest(\"chi2\")',\n",
    "        'SelectKBestMultiInfo': 'sk.feature_selection.SelectKBest(\"mutual_info_classif\")',\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set algorithm = TabField(\n",
    "    name='algorithm',\n",
    "    label='Machine Learning Algorithm',\n",
    "    default='ExtraTreesClassifier',\n",
    "    description='Select a machine learning algorithm to construct the predictive model. \\\n",
    "                 (See scikit-learn User Guide for details.)',\n",
    "    choices={\n",
    "        'GradientBoostingClassifier': [\n",
    "            ChoiceField(\n",
    "                name='loss_gb',\n",
    "                label='loss',\n",
    "                description='Loss function to be optimized.',\n",
    "                default=\"deviance\",\n",
    "                choices=[\"deviance\", \"exponential\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='learning_rate_gb',\n",
    "                label='learning_rate',\n",
    "                description='Shrinks the contribution of each tree by learning_rate.',\n",
    "                default=0.1,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='n_estimators_gb',\n",
    "                label='n_estimators',\n",
    "                description='Number of boosting stages to perform.',\n",
    "                default=100,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='subsample_gb',\n",
    "                label='subsample',\n",
    "                description='Fraction of samples to be used for fitting individual base learners.',\n",
    "                default=1.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='criterion_gb',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"friedman_mse\",\n",
    "                choices=[\"friedman_mse\", \"mse\", \"mae\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='tol_gb',\n",
    "                label='tol',\n",
    "                description='Tolerance for early stopping.',\n",
    "                default=1e-4,\n",
    "            ),\n",
    "        ],\n",
    "        'RandomForestClassifier': [\n",
    "            IntField(\n",
    "                name='n_estimators_rf',\n",
    "                label='n_estimators',\n",
    "                description='Number of trees in the forest.',\n",
    "                default=100,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='criterion_rf',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"gini\",\n",
    "                choices=[\"gini\", \"entropy\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_split_rf',\n",
    "                label='min_samples_split',\n",
    "                description='Minimum number of samples required to split an internal node. \\\n",
    "                             If int, then min_samples_split specifies the minimum number. \\\n",
    "                             If float, then min_samples_split specifies a fraction of the total number of samples.',\n",
    "                default=2,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_leaf_rf',\n",
    "                label='min_samples_leaf',\n",
    "                description='Minimum number of samples required to be at a leaf node. \\\n",
    "                             If int, then min_samples_leaf specifies the minimum number. \\\n",
    "                             If float, then min_samples_leaf specifies a fraction of the total number of samples.',\n",
    "                default=1,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='max_features_rf',\n",
    "                label='max_features',\n",
    "                description='The number of features to consider when looking for the best split.',\n",
    "                default=\"None\",\n",
    "                choices=[\"None\", '\"auto\"', '\"sqrt\"', '\"log2\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_impurity_decrease_rf',\n",
    "                label='min_impurity_decrease',\n",
    "                description='A node will be split if this split induces a decrease of the impurity greater than or equal to this value.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='class_weight_rf',\n",
    "                label='class_weight',\n",
    "                description='Weights associated with classes. If None, then all classes have weight one. \\\n",
    "                             The balanced mode adjusts weights inversely proportional to class frequencies in the input data. \\\n",
    "                             The balanced_subsample mode is the same as balanced except weights are computed based on the bootstrap sample for each tree.',\n",
    "                default='\"balanced\"',\n",
    "                choices=[\"None\", '\"balanced\"', '\"balanced_subsample\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='ccp_alpha_rf',\n",
    "                label='ccp_alpha',\n",
    "                description='Complexity parameter used for Minimal Cost-Complexity Pruning. \\\n",
    "                             The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. \\\n",
    "                             By default, no pruning is performed.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "        ],\n",
    "        'AdaBoostClassifier': [\n",
    "            IntField(\n",
    "                name='max_depth_ab',\n",
    "                label='max_depth',\n",
    "                description='Maximum depth of the decision tree used as the base estimator.',\n",
    "                default=1,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='n_estimators_ab',\n",
    "                label='n_estimators',\n",
    "                description='Maximum number of estimators at which boosting is terminated.',\n",
    "                default=50,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='learning_rate_ab',\n",
    "                label='learning_rate',\n",
    "                description='Shrinks the contribution of each classifier by learning_rate.',\n",
    "                default=1.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='algorithm_ab',\n",
    "                label='algorithm',\n",
    "                description='Select the real or discrete boosting algorithm to use.',\n",
    "                default=\"SAMME.R\",\n",
    "                choices=[\"SAMME\", \"SAMME.R\"],\n",
    "            ),\n",
    "        ],\n",
    "        'ExtraTreesClassifier': [\n",
    "            IntField(\n",
    "                name='n_estimators_et',\n",
    "                label='n_estimators',\n",
    "                description='Number of trees in the forest.',\n",
    "                default=1250,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='criterion_et',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"entropy\",\n",
    "                choices=[\"gini\", \"entropy\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_split_et',\n",
    "                label='min_samples_split',\n",
    "                description='Minimum number of samples required to split an internal node. \\\n",
    "                             If int, then min_samples_split specifies the minimum number. \\\n",
    "                             If float, then min_samples_split specifies a fraction of the total number of samples.',\n",
    "                default=2,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_leaf_et',\n",
    "                label='min_samples_leaf',\n",
    "                description='Minimum number of samples required to be at a leaf node. \\\n",
    "                             If int, then min_samples_leaf specifies the minimum number. \\\n",
    "                             If float, then min_samples_leaf specifies a fraction of the total number of samples.',\n",
    "                default=1,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='max_features_et',\n",
    "                label='max_features',\n",
    "                description='The number of features to consider when looking for the best split.',\n",
    "                default='\"log2\"',\n",
    "                choices=[\"None\", '\"auto\"', '\"sqrt\"', '\"log2\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_impurity_decrease_et',\n",
    "                label='min_impurity_decrease',\n",
    "                description='A node will be split if this split induces a decrease of the impurity greater than or equal to this value.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='class_weight_et',\n",
    "                label='class_weight',\n",
    "                description='Weights associated with classes. If None, then all classes have weight one. \\\n",
    "                             The balanced mode adjusts weights inversely proportional to class frequencies in the input data. \\\n",
    "                             The balanced_subsample mode is the same as balanced except weights are computed based on the bootstrap sample for each tree.',\n",
    "                default='\"balanced\"',\n",
    "                choices=[\"None\", '\"balanced\"', '\"balanced_subsample\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='ccp_alpha_et',\n",
    "                label='ccp_alpha',\n",
    "                description='Complexity parameter used for Minimal Cost-Complexity Pruning. \\\n",
    "                             The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. \\\n",
    "                             By default, no pruning is performed.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "        ],\n",
    "        'DecisionTreeClassifier': [\n",
    "            ChoiceField(\n",
    "                name='criterion_dt',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"gini\",\n",
    "                choices=[\"gini\", \"entropy\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='splitter_dt',\n",
    "                label='splitter',\n",
    "                description='Strategy used to choose the split at each node.',\n",
    "                default=\"best\",\n",
    "                choices=[\"best\", \"random\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_split_dt',\n",
    "                label='min_samples_split',\n",
    "                description='Minimum number of samples required to split an internal node. \\\n",
    "                             If int, then min_samples_split specifies the minimum number. \\\n",
    "                             If float, then min_samples_split specifies a fraction of the total number of samples.',\n",
    "                default=2,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_leaf_dt',\n",
    "                label='min_samples_leaf',\n",
    "                description='Minimum number of samples required to be at a leaf node. \\\n",
    "                             If int, then min_samples_leaf specifies the minimum number. \\\n",
    "                             If float, then min_samples_leaf specifies a fraction of the total number of samples.',\n",
    "                default=1,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='max_features_dt',\n",
    "                label='max_features',\n",
    "                description='The number of features to consider when looking for the best split.',\n",
    "                default=\"None\",\n",
    "                choices=[\"None\", '\"auto\"', '\"sqrt\"', '\"log2\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_impurity_decrease_dt',\n",
    "                label='min_impurity_decrease',\n",
    "                description='A node will be split if this split induces a decrease of the impurity greater than or equal to this value.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='class_weight_dt',\n",
    "                label='class_weight',\n",
    "                description='Weights associated with classes. If None, then all classes have weight one. \\\n",
    "                             The balanced mode adjusts weights inversely proportional to class frequencies in the input data. \\\n",
    "                             The balanced_subsample mode is the same as balanced except weights are computed based on the bootstrap sample for each tree.',\n",
    "                default='\"balanced\"',\n",
    "                choices=[\"None\", '\"balanced\"', '\"balanced_subsample\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='ccp_alpha_dt',\n",
    "                label='ccp_alpha',\n",
    "                description='Complexity parameter used for Minimal Cost-Complexity Pruning. \\\n",
    "                             The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. \\\n",
    "                             By default, no pruning is performed.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "        ],\n",
    "        'KNeighborsClassifier': [\n",
    "            IntField(\n",
    "                name='n_neighbors_knn',\n",
    "                label='n_neighbors',\n",
    "                description='Number of neighbors to use for queries.',\n",
    "                default=5,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='weights_knn',\n",
    "                label='weights',\n",
    "                description='Weight function used in prediction. \\\n",
    "                             If uniform, all points in each neighborhood are weighted equally. \\\n",
    "                             If distance, points are weighted by the inverse of their distance.',\n",
    "                default=\"uniform\",\n",
    "                choices=[\"uniform\", \"distance\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='algorithm_knn',\n",
    "                label='algorithm',\n",
    "                description='Algorithm used to compute the nearest neighbors.',\n",
    "                default=\"auto\",\n",
    "                choices=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "            ),\n",
    "            IntField(\n",
    "                name='leaf_size_knn',\n",
    "                label='leaf_size',\n",
    "                description='Leaf size passed to BallTree or KDTree.',\n",
    "                default=30,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='p_knn',\n",
    "                label='p',\n",
    "                description='Power parameter for the Minkowski metric.',\n",
    "                default=2,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='metric_knn',\n",
    "                label='metric',\n",
    "                description='Distance metric to use for the tree.',\n",
    "                default=\"minkowski\",\n",
    "                choices=[\"minkowski\", \"euclidean\", \"manhattan\", \"chebyshev\"],\n",
    "            ),\n",
    "        ],\n",
    "        'RadiusNeighborsClassifier': [\n",
    "            FloatField(\n",
    "                name='radius_rn',\n",
    "                label='radius',\n",
    "                description='Range of parameter space to use for queries.',\n",
    "                default=1.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='weights_rn',\n",
    "                label='weights',\n",
    "                description='Weight function used in prediction. \\\n",
    "                             If uniform, all points in each neighborhood are weighted equally. \\\n",
    "                             If distance, points are weighted by the inverse of their distance.',\n",
    "                default=\"uniform\",\n",
    "                choices=[\"uniform\", \"distance\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='algorithm_rn',\n",
    "                label='algorithm',\n",
    "                description='Algorithm used to compute the nearest neighbors.',\n",
    "                default=\"auto\",\n",
    "                choices=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "            ),\n",
    "            IntField(\n",
    "                name='leaf_size_rn',\n",
    "                label='leaf_size',\n",
    "                description='Leaf size passed to BallTree or KDTree.',\n",
    "                default=30,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='p_rn',\n",
    "                label='p',\n",
    "                description='Power parameter for the Minkowski metric.',\n",
    "                default=2,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='metric_rn',\n",
    "                label='metric',\n",
    "                description='Distance metric to use for the tree.',\n",
    "                default=\"minkowski\",\n",
    "                choices=[\"minkowski\", \"euclidean\", \"manhattan\", \"chebyshev\"],\n",
    "            ),\n",
    "        ],\n",
    "        'MLPClassifier': [\n",
    "            StringField(\n",
    "                name='hidden_layer_sizes_mlp',\n",
    "                label='hidden_layer_sizes',\n",
    "                description='Enter a tuple, where the ith element represents the number of neurons in the ith hidden layer.',\n",
    "                hint='Enter a tuple: e.g. (128, 64)',\n",
    "                default='(100,)',\n",
    "                constraint='^\\\\(\\\\s*(?:\\\\d+,\\\\s*)+(?:\\\\d+,?\\\\s*)?\\\\)$',\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='activation_mlp',\n",
    "                label='activation',\n",
    "                description='Activation function for the hidden layer.',\n",
    "                default=\"relu\",\n",
    "                choices=[\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='solver_mlp',\n",
    "                label='solver',\n",
    "                description='Solver for weight optimization.',\n",
    "                default=\"adam\",\n",
    "                choices=[\"lbfgs\", \"sgd\", \"adam\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='alpha_mlp',\n",
    "                label='alpha',\n",
    "                description='L2 penality (regularization term) parameter.',\n",
    "                default=0.0001,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='learning_rate_mlp',\n",
    "                label='learning_rate',\n",
    "                description='Learning rate schedule for weight updates. Only used for sgd solver.',\n",
    "                default=\"constant\",\n",
    "                choices=[\"constant\", \"invscaling\", \"adaptive\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='learning_rate_init_mlp',\n",
    "                label='learning_rate_init',\n",
    "                description='The initial learning rate used. Controls the step-size in updating the weights. Only used for sgd or adam solver.',\n",
    "                default=0.001,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='power_t_mlp',\n",
    "                label='power_t',\n",
    "                description='Exponent for inverse scaling learning rate. Only used for sgd solver with invscaling for learning_rate.',\n",
    "                default=0.5,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='max_iter_mlp',\n",
    "                label='max_iter',\n",
    "                description='Maximum number of iterations. The solver iterates until convergence (determined by tol) or this number of iterations.',\n",
    "                default=200,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='tol_mlp',\n",
    "                label='tol',\n",
    "                description='Tolerance for the optimization.',\n",
    "                default=1e-4,\n",
    "            ),\n",
    "            BoolField(\n",
    "                name='early_stopping_mlp',\n",
    "                label='early_stopping',\n",
    "                description='Whether to use early stopping to terminate training when validation score is not improving.',\n",
    "                default=False,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='validation_fraction_mlp',\n",
    "                label='validation_fraction',\n",
    "                description='The proportion of training data to set aside as validation set for early stopping.',\n",
    "                default=0.1,\n",
    "            ),\n",
    "        ],\n",
    "        'SVC': [\n",
    "            FloatField(\n",
    "                name='C_svm',\n",
    "                label='C',\n",
    "                description='Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.',\n",
    "                default=1.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='kernel_svm',\n",
    "                label='kernel',\n",
    "                description='Specifies the kernel type to be used in the algorithm.',\n",
    "                default=\"rbf\",\n",
    "                choices=[\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"],\n",
    "            ),\n",
    "            IntField(\n",
    "                name='degree_svm',\n",
    "                label='degree',\n",
    "                description='Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.',\n",
    "                default=3,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='gamma_svm',\n",
    "                label='gamma',\n",
    "                description='Kernel coefficient for rbf, poly and sigmoid kernels.',\n",
    "                default=\"scale\",\n",
    "                choices=[\"scale\", \"auto\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='coef0_svm',\n",
    "                label='coef0',\n",
    "                description='Independent term in kernel function. It is only significant in poly and sigmoid.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            BoolField(\n",
    "                name='shrinking_svm',\n",
    "                label='shrinking',\n",
    "                description='Whether to use the shrinking heuristic.',\n",
    "                default=True,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='tol_svm',\n",
    "                label='tol',\n",
    "                description='Tolerance for stopping criterion.',\n",
    "                default=1e-3,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='class_weight_svm',\n",
    "                label='class_weight',\n",
    "                description='Weights associated with classes. If None, then all classes have weight one. \\\n",
    "                             The balanced mode adjusts weights inversely proportional to class frequencies in the input data.',\n",
    "                default='\"balanced\"',\n",
    "                choices=[\"None\", '\"balanced\"'],\n",
    "            ),\n",
    "            IntField(\n",
    "                name='max_iter_svm',\n",
    "                label='max_iter',\n",
    "                description='Hard limit on iterations within solver, or -1 for no limit.',\n",
    "                default=-1,\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set calibrated = BoolField(\n",
    "    name='calibrated',\n",
    "    label='Calibrate algorithm predictions',\n",
    "    description='Calibrate the prediction probabilities, eliminating model-imparted bias.',\n",
    "    default=True,\n",
    "    section='SETTINGS',\n",
    ") %}\n",
    "{% set cv_algorithm = ChoiceField(\n",
    "    name='cv_algorithm',\n",
    "    label='Cross-Validation Algorithm',\n",
    "    description='Select a cross-validation method for training and evaluating the pipeline, and for making predictions. \\\n",
    "                 StratifiedGroupKFold or RepeatedStratifiedGroupKFold are recommended because they will maintain class ratios \\\n",
    "                 across train/validation splits (stratification of labels) and will group compounds by the first 14 characters of their \\\n",
    "                 InChI Keys to avoid compounds with multiple entries from appearing in both the train and validation sets.',\n",
    "    default='RepeatedStratifiedGroupKFold',\n",
    "    choices={\n",
    "        'KFold': 'sk.model_selection.KFold',\n",
    "        'GroupKFold': 'sk.model_selection.GroupKFold',\n",
    "        'RepeatedKFold': 'sk.model_selection.RepeatedKFold',\n",
    "        'StratifiedKFold': 'sk.model_selection.StratifiedKFold',\n",
    "        'StratifiedGroupKFold': 'StratifiedGroupKFold',\n",
    "        'RepeatedStratifiedKFold': 'sk.model_selection.RepeatedStratifiedKFold',\n",
    "        'RepeatedStratifiedGroupKFold': 'RepeatedStratifiedGroupKFold'\n",
    "    },\n",
    "    section='SETTINGS',\n",
    ") %}\n",
    "{% set cross_validation_n_folds = IntField(\n",
    "    name='cross_validation_n_folds',\n",
    "    label='Number of Cross-Validated Folds',\n",
    "    description='Cross-validation is employed as a strategy to train the model on data that the model has not seen before, more folds will ensure that the model is generalizing well.',\n",
    "    default=10,\n",
    "    min=2,\n",
    "    max=10,\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set cross_validation_n_repeats = IntField(\n",
    "    name='cross_validation_n_repeats',\n",
    "    label='Number of Cross-Validated Repetitions',\n",
    "    description='Number of repetitions of cross-validation to perform. \\\n",
    "                 Only used for RepeatedKFold, RepeatedStratifiedKFold, or RepeatedStratifiedGroupKFold cross-validation algorithms, \\\n",
    "                 which repeat cross-validation with different randomizations. This yields multiple predictions per compound, which can be evaluated for consistency.',\n",
    "    default=3,\n",
    "    min=2,\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set primary_metric = ChoiceField(\n",
    "    name='primary_metric',\n",
    "    label='Primary Evaluation Metric',\n",
    "    default='roc_auc',\n",
    "    description='The primary evaluation metric is used for deciding how we assess the performance of our model. \\\n",
    "                 Area under the receiver operating characteristic curve (roc_auc) is recommended for most tasks.',\n",
    "    choices=[\n",
    "        'accuracy',\n",
    "        'adjusted_mutual_info_score',\n",
    "        'adjusted_rand_score',\n",
    "        'average_precision',\n",
    "        'balanced_accuracy',\n",
    "        'completeness_score',\n",
    "        'explained_variance',\n",
    "        'f1',\n",
    "        'f1_macro',\n",
    "        'f1_micro',\n",
    "        'f1_weighted',\n",
    "        'fowlkes_mallows_score',\n",
    "        'homogeneity_score',\n",
    "        'jaccard',\n",
    "        'jaccard_macro',\n",
    "        'jaccard_micro',\n",
    "        'jaccard_weighted',\n",
    "        'max_error',\n",
    "        'mutual_info_score',\n",
    "        'neg_brier_score',\n",
    "        'neg_log_loss',\n",
    "        'neg_mean_absolute_error',\n",
    "        'neg_mean_squared_error',\n",
    "        'neg_mean_squared_log_error',\n",
    "        'neg_median_absolute_error',\n",
    "        'neg_root_mean_squared_error',\n",
    "        'normalized_mutual_info_score',\n",
    "        'precision',\n",
    "        'precision_macro',\n",
    "        'precision_micro',\n",
    "        'precision_weighted',\n",
    "        'r2',\n",
    "        'recall',\n",
    "        'recall_macro',\n",
    "        'recall_micro',\n",
    "        'recall_weighted',\n",
    "        'roc_auc',\n",
    "        'roc_auc_ovo',\n",
    "        'roc_auc_ovo_weighted',\n",
    "        'roc_auc_ovr',\n",
    "        'roc_auc_ovr_weighted',\n",
    "        'v_measure_score'\n",
    "    ],\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set evaluation_metrics = MultiChoiceField(\n",
    "    name='evaluation_metrics',\n",
    "    label='Evaluation Metrics',\n",
    "    default=[],\n",
    "    description='Additional evaluation metrics can be specified, these metrics will also be reported for all models trained.',\n",
    "    value=[],\n",
    "    choices=[\n",
    "        'accuracy',\n",
    "        'adjusted_mutual_info_score',\n",
    "        'adjusted_rand_score',\n",
    "        'average_precision',\n",
    "        'balanced_accuracy',\n",
    "        'completeness_score',\n",
    "        'explained_variance',\n",
    "        'f1',\n",
    "        'f1_macro',\n",
    "        'f1_micro',\n",
    "        'f1_weighted',\n",
    "        'fowlkes_mallows_score',\n",
    "        'homogeneity_score',\n",
    "        'jaccard',\n",
    "        'jaccard_macro',\n",
    "        'jaccard_micro',\n",
    "        'jaccard_weighted',\n",
    "        'max_error',\n",
    "        'mutual_info_score',\n",
    "        'neg_brier_score',\n",
    "        'neg_log_loss',\n",
    "        'neg_mean_absolute_error',\n",
    "        'neg_mean_squared_error',\n",
    "        'neg_mean_squared_log_error',\n",
    "        'neg_median_absolute_error',\n",
    "        'neg_root_mean_squared_error',\n",
    "        'normalized_mutual_info_score',\n",
    "        'precision',\n",
    "        'precision_macro',\n",
    "        'precision_micro',\n",
    "        'precision_weighted',\n",
    "        'r2',\n",
    "        'recall',\n",
    "        'recall_macro',\n",
    "        'recall_micro',\n",
    "        'recall_weighted',\n",
    "        'roc_auc',\n",
    "        'roc_auc_ovo',\n",
    "        'roc_auc_ovo_weighted',\n",
    "        'roc_auc_ovr',\n",
    "        'roc_auc_ovr_weighted',\n",
    "        'v_measure_score'\n",
    "    ],\n",
    "    section='SETTINGS',\n",
    ") %}\n",
    "{% set all_metrics = [primary_metric.value] + evaluation_metrics.value %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_hide\n",
    "\n",
    "{% set algorithm_code = {\n",
    "    'GradientBoostingClassifier': 'sk.ensemble.GradientBoostingClassifier(loss=\"{}\", learning_rate={}, n_estimators={}, subsample={}, criterion=\"{}\", tol={})',\n",
    "    'RandomForestClassifier': 'sk.ensemble.RandomForestClassifier(n_estimators={}, criterion=\"{}\", min_samples_split={}, min_samples_leaf={}, max_features={}, min_impurity_decrease={}, n_jobs=-1, class_weight={}, ccp_alpha={})',\n",
    "    'AdaBoostClassifier': 'sk.ensemble.AdaBoostClassifier(sk.tree.DecisionTreeClassifier(max_depth={}), n_estimators={}, learning_rate={}, algorithm=\"{}\")',\n",
    "    'ExtraTreesClassifier': 'sk.ensemble.ExtraTreesClassifier(n_estimators={}, criterion=\"{}\", min_samples_split={}, min_samples_leaf={}, max_features={}, min_impurity_decrease={}, n_jobs=-1, class_weight={}, ccp_alpha={})',\n",
    "    'DecisionTreeClassifier': 'sk.tree.DecisionTreeClassifier(criterion=\"{}\", splitter=\"{}\", min_samples_split={}, min_samples_leaf={}, max_features={}, min_impurity_decrease={}, class_weight={}, ccp_alpha={})',\n",
    "    'KNeighborsClassifier': 'sk.neighbors.KNeighborsClassifier(n_neighbors={}, weights=\"{}\", algorithm=\"{}\", leaf_size={}, p={}, metric=\"{}\", n_jobs=-1)',\n",
    "    'RadiusNeighborsClassifier': 'sk.neighbors.RadiusNeighborsClassifier(radius={}, weights=\"{}\", algorithm=\"{}\", leaf_size={}, p={}, metric=\"{}\", outlier_label=\"most_frequent\", n_jobs=-1)',\n",
    "    'MLPClassifier': 'sk.neural_network.MLPClassifier(hidden_layer_sizes={}, activation=\"{}\", solver=\"{}\", alpha={}, learning_rate=\"{}\", learning_rate_init={}, power_t={}, max_iter={}, tol={}, early_stopping={}, validation_fraction={})',\n",
    "    'SVC': 'sk.svm.SVC(C={}, kernel=\"{}\", degree={}, gamma=\"{}\", coef0={}, shrinking={}, tol={}, class_weight={}, max_iter={})',\n",
    "} %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "We apply a sklearn pipeline with a dimensionality reduction step of {{ dimensionality_reduction.raw_value }}\n",
    "{% if feature_selection.value != 'None' %}and a feature selection step of {{ feature_selection.raw_value }}\n",
    "{% endif %} and a{% if calibrated.value %} calibrated{%endif %} {{ algorithm.raw_value }} classifier\n",
    "using {{ cross_validation_n_folds.value }}-fold {{ cv_algorithm.raw_value }} cross-validation,\n",
    "optimizing {{ primary_metric.value }}{% if evaluation_metrics.value %} and computing {{ ', '.join(evaluation_metrics.value) }}{% endif %}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that training can take a long time as we are training a model for each of multiple cross-validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "cv = {{ cv_algorithm }}(\n",
    "    n_splits={{ cross_validation_n_folds }},\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    n_repeats={{ cross_validation_n_repeats }},\n",
    "    {% else %}\n",
    "    shuffle=True,\n",
    "    {% endif %}\n",
    "    random_state=rng,\n",
    ")\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['GroupKFold', 'StratifiedGroupKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "groups=[key[:14] for key in X.index]    # Group compounds by atom connectivity\n",
    "{% endif %}\n",
    "\n",
    "# Scoring parameters\n",
    "primary_metric = '{{ primary_metric }}'\n",
    "evaluation_metrics = {{ evaluation_metrics }}\n",
    "scoring_params = {k: metrics.get_scorer(k)\n",
    "                  for k in [primary_metric, *evaluation_metrics]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "# Store performance on each split for computing ROC and PRC curves\n",
    "fprs = []\n",
    "tprs = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "# Store cross-validation test predictions and folds\n",
    "y_proba_cv = [[] for _ in range(len(y))]\n",
    "folds_cv = [[] for _ in range(len(y))]\n",
    "\n",
    "# Store models\n",
    "models = []\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['GroupKFold', 'StratifiedGroupKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "groups=[key[:14] for key in X.index]    # Group compounds by atom connectivity\n",
    "for fold, (train, test) in tqdm(enumerate(cv.split(X.values, y, groups=groups))):\n",
    "{% else %}\n",
    "for fold, (train, test) in tqdm(enumerate(cv.split(X.values, y))):\n",
    "{% endif %}\n",
    "    model = sk.pipeline.Pipeline([\n",
    "                {%- if dimensionality_reduction.value != 'None' %}\n",
    "                ('reduce_dim', {{ dimensionality_reduction }}),\n",
    "                {% endif %}\n",
    "                {%- if feature_selection.value != 'None' %}\n",
    "                ('feature_selection', {{ feature_selection }}),\n",
    "                {% endif %}\n",
    "                ('clf', {% if algorithm.raw_value == 'MLPClassifier' %}{{ algorithm_code.get(algorithm.raw_value).format(algorithm.value[0].value|str_to_tuple, *algorithm.value[1:]) }}\n",
    "                        {% elif algorithm.raw_value in ['DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier'] %}{{ algorithm_code.get(algorithm.raw_value).format(algorithm.value[0].value, algorithm.value[1].value, algorithm.value[2].value|int_or_float, algorithm.value[3].value|int_or_float, *algorithm.value[4:]) }}\n",
    "                        {% else %}{{ algorithm_code.get(algorithm.raw_value).format(*algorithm.value) }}{% endif %}\n",
    "                ),\n",
    "            ])\n",
    "    model.fit(X.values[train], y[train])\n",
    "    \n",
    "    {% if calibrated.value %}\n",
    "    calibrator = sk.calibration.CalibratedClassifierCV(model, cv='prefit')\n",
    "    calibrator.fit(X.values[test], y[test])\n",
    "    model = calibrator\n",
    "    {% endif %}\n",
    "    \n",
    "    {% for metric in all_metrics %}\n",
    "    df_results.loc[fold, '{{ metric }}'] = scoring_params['{{ metric }}'](model, X.values[test], y[test])\n",
    "    {% endfor %}\n",
    "    \n",
    "    y_proba = model.predict_proba(X.values[test]) # Probability prediction will be True\n",
    "    for i in range(len(test)):\n",
    "        y_proba_cv[test[i]].append(y_proba[i, 1])\n",
    "        folds_cv[test[i]].append(fold % {{ cross_validation_n_folds }})\n",
    "    model_fpr, model_tpr, _ = metrics.roc_curve(y[test], y_proba[:, 1])\n",
    "    model_prec, model_rec, _ = metrics.precision_recall_curve(y[test], y_proba[:, 1])\n",
    "    fprs.append(model_fpr)\n",
    "    tprs.append(model_tpr)\n",
    "    precs.append(model_prec)\n",
    "    recs.append(model_rec)\n",
    "    models.append(model)\n",
    "\n",
    "assert not(any(len(probs) == 0 for probs in y_proba_cv)), 'All probabilities should have been calculated'\n",
    "\n",
    "display(df_results.agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization shows the cross-validated performance of the model. Low fold variance and high AUC is desired in a well-generalized model.\n",
    "* ROC curve: [roc.svg](./roc.svg)\n",
    "* Precision-recall curve: [prc.svg](./prc.svg)\n",
    "* Confusion matrix: [confusion_matrix.svg](./confusion_matrix.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "tprs_interp = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for fold, (fpr, tpr) in enumerate(zip(fprs, tprs)):\n",
    "    tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
    "    tpr_interp[0] = 0.\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    tprs_interp.append(tpr_interp)\n",
    "    aucs.append(roc_auc)\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    ax.plot(fpr, tpr, alpha=0.4)\n",
    "    {% else %}\n",
    "    ax.plot(fpr, tpr, alpha=0.4, label='ROC Fold %d (AUC=%0.3f)' % (fold, roc_auc))\n",
    "    {% endif %}\n",
    "\n",
    "mean_tpr = np.mean(tprs_interp, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs_interp, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)\n",
    "\n",
    "ax.plot([0,1],[0,1],'--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.legend()\n",
    "plt.savefig('roc.svg')\n",
    "figure_header('Figure', 'Receiver operating characteristic (ROC) curves across cross-validation splits ({})'.format(make_clickable('roc.svg')))\n",
    "plt.show()\n",
    "figure_legend('Figure', 'Receiver operating characteristic (ROC) curves across cross-validation splits ({})'.format(make_clickable('roc.svg')),\n",
    "              'Individual curves are shown for each {{ cross_validation_n_folds }}-fold cross-validation split{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}, repeated with {{ cross_validation_n_repeats }} different randomizations{% endif %}. \\\n",
    "               Mean ROC shows the average and standard deviation across cross-validation splits.')\n",
    "\n",
    "z = (mean_auc - 0.5)/std_auc\n",
    "cl = sp.stats.norm.cdf(z) * 100\n",
    "ci = sp.stats.norm.interval(0.95, loc=mean_auc, scale=std_auc)\n",
    "print('Confidence interval (95%)', ci)\n",
    "print(\"We are %0.3f %% confident the model's results are not just chance.\" % (cl))\n",
    "if cl > 95:\n",
    "    print('This is statistically significant. These results can be trusted.')\n",
    "else:\n",
    "    print('This is not statistically significant. These results should not be trusted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "precs_interp = []\n",
    "prc_aucs = []\n",
    "mean_rec = np.linspace(0, 1, 100)\n",
    "\n",
    "for fold, (rec, prec) in enumerate(zip(recs, precs)):\n",
    "    prec_interp = np.interp(mean_rec, rec[::-1], prec[::-1])\n",
    "    prc_auc = metrics.auc(rec, prec)\n",
    "    precs_interp.append(prec_interp)\n",
    "    prc_aucs.append(prc_auc)\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    ax.plot(rec, prec, alpha=0.4)\n",
    "    {% else %}\n",
    "    ax.plot(rec, prec, alpha=0.4, label='PRC Fold %d (AUC=%0.3f)' % (fold, prc_auc))\n",
    "    {% endif %}\n",
    "    \n",
    "mean_prec = np.mean(precs_interp, axis=0)\n",
    "mean_auc = sk.metrics.auc(mean_rec, mean_prec)\n",
    "std_auc = np.std(prc_aucs)\n",
    "ax.plot(mean_rec, mean_prec, color='b',\n",
    "         label=r'Mean PRC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_prec = np.std(precs_interp, axis=0)\n",
    "precs_upper = np.minimum(mean_prec + std_prec, 1)\n",
    "precs_lower = np.maximum(mean_prec - std_prec, 0)\n",
    "plt.fill_between(mean_rec, precs_lower, precs_upper, color='grey', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend()\n",
    "plt.savefig('prc.svg')\n",
    "figure_header('Figure', 'Precision-recall curves (PRC) across cross-validation splits ({})'.format(make_clickable('prc.svg')))\n",
    "plt.show()\n",
    "figure_legend('Figure', 'Precision-recall curves (PRC) across cross-validation splits ({})'.format(make_clickable('prc.svg')),\n",
    "              'Individual curves are shown for each {{ cross_validation_n_folds }}-fold cross-validation split{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}, repeated with {{ cross_validation_n_repeats }} different randomizations{% endif %}. \\\n",
    "               Mean PRC shows the average and standard deviation across cross-validation splits.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    metrics.confusion_matrix(y, np.array([np.mean(probs) for probs in y_proba_cv]) > 0.5),\n",
    "    annot=True,\n",
    "    cmap=plt.cm.Blues,\n",
    "    fmt='g'\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix.svg')\n",
    "figure_header('Figure', 'Confusion matrix for cross-validation predictions ({})'.format(make_clickable('confusion_matrix.svg')))\n",
    "plt.show()\n",
    "figure_legend('Figure', 'Confusion matrix for cross-validation predictions ({})'.format(make_clickable('confusion_matrix.svg')),\n",
    "              'Note that the predicted probabilities can be greatly affected by imbalanced labels and by the model choice. \\\n",
    "               Thus, performance measures such as ROC and PRC, which evaluate performance across a range of prediction thresholds, \\\n",
    "               are more useful than the confusion-matrix, which uses an fixed cutoff of 0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the validation-set predictions, we can rank the positive compounds and identify additional compounds that were not known to be in the positive class, but nevertheless had high predictions. These may share similar properties with the known compounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can compare the distribution of predictions for positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "# Calculate mean and deviation of predictions\n",
    "y_probas = np.array([np.mean(probs) for probs in y_proba_cv])\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "y_probas_std = np.array([np.std(probs) for probs in y_proba_cv])\n",
    "# Find minimum non-zero standard deviation to avoid dividing by zero when computing t-statistic\n",
    "min_y_probas_std = max(np.min(y_probas_std[y_probas_std != 0]), 1e-10)\n",
    "t_stats = (y_probas - np.mean(y_probas)) / (np.maximum(y_probas_std, min_y_probas_std)/np.sqrt({{ cross_validation_n_repeats }}))\n",
    "# Calculate p-value using one-sample t-test\n",
    "p_vals_t = 1-sp.stats.t({{ cross_validation_n_repeats }}-1).cdf(t_stats)\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "# Simulate mean predictions by \n",
    "y_probas_means_{{ cross_validation_n_repeats }} = []\n",
    "y_probas_values = np.array(y_proba_cv).flatten()\n",
    "\n",
    "np.random.seed(rng)\n",
    "for i in tqdm(range(100000)):\n",
    "    y_probas_means_{{ cross_validation_n_repeats }}.append(np.mean(np.random.choice(y_probas_values, {{ cross_validation_n_repeats }})))\n",
    "    \n",
    "y_probas_means_{{ cross_validation_n_repeats }} = np.array(sorted(y_probas_means_{{ cross_validation_n_repeats }}))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "y_probas_ts_{{ cross_validation_n_repeats }} = []\n",
    "mean_y_probas = np.mean(y_probas)\n",
    "y_probas_values = np.array(y_proba_cv).flatten()\n",
    "\n",
    "np.random.seed(rng)\n",
    "for i in tqdm(range(100000)):\n",
    "    sample = np.random.choice(y_probas_values, {{ cross_validation_n_repeats }})\n",
    "    y_probas_ts_{{ cross_validation_n_repeats }}.append((np.mean(sample) - mean_y_probas) / (np.maximum(np.std(sample), min_y_probas_std)/np.sqrt({{ cross_validation_n_repeats }})))\n",
    "    \n",
    "y_probas_ts_{{ cross_validation_n_repeats }} = np.array(sorted(y_probas_ts_{{ cross_validation_n_repeats }}))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "max_mean = np.max(y_probas_means_{{ cross_validation_n_repeats }})\n",
    "p_vals = np.array(list(tqdm((1 - np.argwhere(y_probas_means_{{ cross_validation_n_repeats }} >= min(pred, max_mean))[0][0] / len(y_probas_means_{{ cross_validation_n_repeats }})\n",
    "                             for pred in y_probas), total=len(y_probas))))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "max_t = np.max(y_probas_ts_{{ cross_validation_n_repeats }})\n",
    "p_vals_t_sim = np.array(list(tqdm((1 - np.argwhere(y_probas_ts_{{ cross_validation_n_repeats }} >= min(t, max_t))[0][0] / len(y_probas_ts_{{ cross_validation_n_repeats }})\n",
    "                                   for t in t_stats), total=len(t_stats))))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "sns.histplot(y_probas[y == 0], bins=int(np.sqrt(np.sum(y == 0))*10), kde_kws={'gridsize':2000}, stat = 'density', label='Not known positive compound', color='blue')\n",
    "sns.histplot(y_probas[y == 1], bins=int(np.sqrt(np.sum(y == 1))*10), kde_kws={'gridsize':2000}, stat = 'density', label='Known positive compound', color='red')\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "sns.histplot(y_probas_means_{{ cross_validation_n_repeats }}, bins=int(np.sqrt(len(y_probas_means_{{ cross_validation_n_repeats }}))*10), kde_kws={'gridsize':2000}, label='Null distribution\\n(simulated)', stat = 'density', color='green')\n",
    "{% endif %}\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.xlim([np.min(y_probas), np.percentile(y_probas, 99)])\n",
    "plt.legend()\n",
    "plt.savefig('mean-prediction-distribution.svg')\n",
    "figure_header('Figure', 'Distribution of{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %} mean{% endif %} cross-validation predictions ({})'.format(make_clickable('mean-prediction-distribution.svg')))\n",
    "plt.show()\n",
    "figure_legend('Figure', 'Distribution of{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %} mean{% endif %} cross-validation predictions ({})'.format(make_clickable('mean-prediction-distribution.svg')),\n",
    "              'Distribution of{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %} mean{% endif %} cross-validation predictions for all {number_of_compounds} compounds, \\\n",
    "               including both those with known positive labels and other small molecules.\\\n",
    "               {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %} The null distribution was simulated by drawing independent samples of predictions with replacement from the distribution of all predictions.{% endif %}'.format(number_of_compounds=X.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "sns.histplot(t_stats[y == 0], bins=int(np.sqrt(np.sum(y == 0))*10), kde_kws={'gridsize':2000}, stat = 'density', label='Not known positive compound', color = 'blue')\n",
    "sns.histplot(t_stats[y == 1], bins=int(np.sqrt(np.sum(y == 1))*10), kde_kws={'gridsize':2000}, stat = 'density', label='Known positive compound', color = 'red')\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "sns.histplot(y_probas_ts_{{ cross_validation_n_repeats }}, bins=int(np.sqrt(len(y_probas_ts_{{ cross_validation_n_repeats }}))*10), kde_kws={'gridsize':2000}, stat = 'density', label='Null distribution\\n(simulated)', color = 'green')\n",
    "{% endif %}\n",
    "plt.xlabel('t-statistic')\n",
    "plt.xlim([-20,20])\n",
    "plt.legend()\n",
    "plt.savefig('t-statistic-distribution.svg')\n",
    "figure_header('Figure', 'Distribution of t-statistics ({})'.format(make_clickable('t-statistic-distribution.svg')))\n",
    "plt.show()\n",
    "figure_legend('Figure', 'Distribution of t-statistics ({})'.format(make_clickable('t-statistic-distribution.svg')),\n",
    "              'Distributions of t-statistics for all {number_of_compounds} compounds, \\\n",
    "               including both those with known positive labels and other small molecules. \\\n",
    "               The null distribution was simulated by drawing independent samples of predictions with replacement from the distribution of all predictions.'.format(number_of_compounds=X.shape[0]))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlaying the predictions on a visualization of the input space allows us to examine the predictions and may indicate groups of highly predicted compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "# Add attributes for plotting to Dataframe\n",
    "X_reduced_df['Predicted Probability'] = y_probas\n",
    "X_reduced_df['log10(pred)'] = np.log10(y_probas + 1e-10)\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "X_reduced_df['p-value'] = p_vals_t_sim\n",
    "X_reduced_df['log10(p-value)'] = np.log10(X_reduced_df['p-value'])\n",
    "X_reduced_df['Standard Deviation'] = y_probas_std\n",
    "{% endif %}\n",
    "X_reduced_df['Cross-validation fold'] = folds_cv\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "X_reduced_df['marker size'] = 2*np.minimum(2-np.log10(X_reduced_df['p-value']), 5)\n",
    "{% else %}\n",
    "max_p, min_p = np.min(-X_reduced_df['log10(pred)']), np.max(-X_reduced_df['log10(pred)'])\n",
    "X_reduced_df['marker size'] = (-X_reduced_df['log10(pred)'] - min_p) / (max_p - min_p) * 6 + 4\n",
    "{% endif %}\n",
    "X_reduced_df['text'] = ['<br>'.join(['Drug Name: ' + str(name),\n",
    "                                     'InChI Key: ' + str(inchi),\n",
    "                                     'Predicted Probability: {:.1e}'.format(p),\n",
    "                                     {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "                                     'Standard Deviation: {:.1e}'.format(s),\n",
    "                                     'p-value: {:.1e}'.format(p_val),\n",
    "                                     {% endif %}\n",
    "                                     'Label: ' + str(label),\n",
    "                                     'Cross-validation fold: ' + str(fold)])\n",
    "                  for name, inchi, p, {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}s, p_val, {% endif %}label, fold in zip(X_reduced_df['Drug Name'],\n",
    "                                                         X_reduced_df['InChI Key'],\n",
    "                                                         X_reduced_df['Predicted Probability'],\n",
    "                                                         {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "                                                         X_reduced_df['Standard Deviation'],\n",
    "                                                         X_reduced_df['p-value'],\n",
    "                                                         {% endif %}\n",
    "                                                         X_reduced_df['Label'],\n",
    "                                                         X_reduced_df['Cross-validation fold'])]\n",
    "X_reduced_df.to_csv('X_reduced_df.csv')\n",
    "\n",
    "# Helper function for formatting Plotly colorbar\n",
    "def colorbar_param(values_log10, **kwargs):\n",
    "    min_val = np.floor(np.min(values_log10))\n",
    "    max_val = np.ceil(np.max(values_log10))\n",
    "    \n",
    "    ticks1 = 10**np.arange(min_val, max_val+1)\n",
    "    ticks2 = 3*10**np.arange(min_val, max_val)\n",
    "    \n",
    "    ticktext = sorted(np.concatenate([ticks1, ticks2]))\n",
    "    tickvals = list(np.log10(ticktext))\n",
    "    ticktext = ['{:.0e}'.format(text) for text in ticktext]\n",
    "    \n",
    "    return dict(ticktext=ticktext, tickvals=tickvals, **kwargs)\n",
    "\n",
    "fig = go.Figure()\n",
    "for label in sorted(set(X_reduced_df['Label'])):\n",
    "    X_plot = X_reduced_df[X_reduced_df['Label'] == label].sort_values(['Predicted Probability'])\n",
    "    fig.add_trace(go.Scatter(mode='markers',\n",
    "                               x=X_plot['Component 1'], y=X_plot['Component 2'],\n",
    "                               text=X_plot['text'],\n",
    "                               name=label,\n",
    "                               marker=dict(\n",
    "                                   color=X_plot['log10(pred)'],\n",
    "                                   cmin=np.percentile(X_reduced_df['log10(pred)'], 50),\n",
    "                                   cmax=np.max(X_reduced_df['log10(pred)']),\n",
    "                                   size=X_plot['marker size'],\n",
    "                                   colorbar=colorbar_param(X_plot['log10(pred)'], title='Predicted Probability'),\n",
    "                                   symbol=X_plot['marker symbol'],\n",
    "                                   line_width=1,\n",
    "                                   colorscale='plasma'\n",
    "                               )))\n",
    "fig.update_layout(height=600, width=800,\n",
    "                  xaxis_title='Component 1',\n",
    "                  yaxis_title='Component 2',\n",
    "                  title_text='Predicted Probabilities ({{ visualization_reduction.raw_value }})',\n",
    "                  legend_title_text='Target Label',\n",
    "                  legend=dict(\n",
    "                      yanchor=\"top\",\n",
    "                      y=0.98,\n",
    "                      xanchor=\"left\",\n",
    "                      x=0.02\n",
    "                  ),\n",
    "                  template='simple_white')\n",
    "figure_header('Figure', '{{ visualization_reduction.raw_value }} dimensionality reduction of the input feature space overlayed with predictions')\n",
    "fig.show()\n",
    "figure_legend('Figure', '{{ visualization_reduction.raw_value }} dimensionality reduction of the input feature space overlayed with predictions',\n",
    "              f'Each point represents one of {X.shape[0]} compounds, with {X.shape[1]} features per compound, \\\n",
    "              taken from the following datasets: {\", \".join(sepl1000_phenotypic_datasets + sepl1000_structural_datasets + attribute_datasets)}. \\\n",
    "              Compounds with known positive labels are marked by X\\'s. The color and size of each point correspond to the{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %} mean{% endif %} predicted \\\n",
    "              probability {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}and its signficance (estimated from the simulated t-statistic null distribution), respectively{% endif %}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full tables of top-predicted compounds with and without known positive labels are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "# Obtain prediction results\n",
    "results = pd.DataFrame(np.array([\n",
    "    querysepl1000fwd.get_drug_names(X.index),\n",
    "    Drugmonizome.get_drug_names(X.index),\n",
    "    folds_cv,\n",
    "    y,\n",
    "    y_probas,\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    y_probas_std,\n",
    "    t_stats,\n",
    "    p_vals,\n",
    "    p_vals_t,\n",
    "    p_vals_t_sim,\n",
    "    {% endif %}\n",
    "], dtype='object').T, columns=[\n",
    "    'Name (L1000FWD)',\n",
    "    'Name (Drugmonizome)',\n",
    "    'Cross-validation fold',\n",
    "    'Known',\n",
    "    'Prediction Probability',\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    'Prediction Probability Std. Dev.',\n",
    "    't statistic',\n",
    "    'p value (simulated mean distribution)',\n",
    "    'p value (one sample t test)',\n",
    "    'p value (simulated t distribution)',\n",
    "    {% endif %}\n",
    "], index=X.index).astype({'Known': 'bool',\n",
    "                          'Prediction Probability': 'float64',\n",
    "                          {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "                          'Prediction Probability Std. Dev.': 'float64',\n",
    "                          't statistic': 'float64',\n",
    "                          'p value (simulated mean distribution)': 'float64',\n",
    "                          'p value (one sample t test)': 'float64',\n",
    "                          'p value (simulated t distribution)': 'float64',{% endif %}})\n",
    "\n",
    "results.to_csv('drug_cv_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank predictions\n",
    "figure_header('Table', 'Top-predicted compounds ({})'.format(make_clickable('drug_cv_predictions.csv')))\n",
    "show(results.reset_index(), maxBytes=0, order=[[ 5, \"desc\" ]], columnDefs=[{'width': '120px', 'targets': [0, 1]}])\n",
    "figure_legend('Table', 'Top-predicted compounds ({})'.format(make_clickable('drug_cv_predictions.csv')),\n",
    "              f'All {X.shape[0]} compounds ranked by cross-validation prediction probability. \\\n",
    "                Search \\'true\\' or \\'false\\' to filter compounds with known positive labels or not, respectively. \\\n",
    "                The table can also be sorted by other columns by selecting the column name in the header.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative contribution of each input feature to the final model predictions can be estimated for recursive feature selection and for a variety of tree-based models. Note that this analysis is not available if a dimensionality reduction algorithm is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' and dimensionality_reduction.raw_value == 'None' %}\n",
    "When recursive feature selection is performed, the features are ranked by the stage at which they were removed.\n",
    "Selected (i.e. estimated best) features are have importance 1. The ranks are averaged across cross-validation\n",
    "splits to produce an average importance score. The full feature importance table is available at\n",
    "[feature_importance.csv](./feature_importance.csv).\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if dimensionality_reduction.raw_value == 'None' %}\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' and dimensionality_reduction.raw_value == 'None' %}\n",
    "all_rankings = []\n",
    "{% endif %}\n",
    "{% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier'] %}\n",
    "all_feature_importances = []\n",
    "{% endif %}\n",
    "for model in models:\n",
    "    {% if calibrated.value %}\n",
    "    for calibrated_clf in model.calibrated_classifiers_:\n",
    "        pipeline = calibrated_clf.base_estimator\n",
    "    {% else %}\n",
    "        pipeline = model\n",
    "    {% endif %}\n",
    "        \n",
    "        {% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' %}\n",
    "        ranking = pipeline['feature_selection'].ranking_\n",
    "        all_rankings.append(ranking)\n",
    "        {% endif %}\n",
    "        \n",
    "        {% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier'] %}\n",
    "        {% if feature_selection.raw_value != 'None' %}\n",
    "        feature_importances = np.zeros(pipeline['feature_selection'].get_support().shape)\n",
    "        feature_importances[pipeline['feature_selection'].get_support()] = pipeline['clf'].feature_importances_\n",
    "        {% else %}\n",
    "        feature_importances = pipeline['clf'].feature_importances_\n",
    "        {% endif %}\n",
    "        all_feature_importances.append(feature_importances)\n",
    "        {% endif %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if dimensionality_reduction.raw_value == 'None' %}\n",
    "df_feat_imp = pd.DataFrame({'Feature': X.columns,\n",
    "                            'Dataset': reduce(lambda a,b: a+b, ([dataset]*size for dataset, size in dataset_sizes)),\n",
    "                            {% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' %}\n",
    "                            'Ranking Mean': np.mean(all_rankings, axis=0),\n",
    "                            'Ranking Std. Dev.': np.std(all_rankings, axis=0),\n",
    "                            {% endif %}\n",
    "                            {% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier'] %}\n",
    "                            'Importance Mean': np.mean(all_feature_importances, axis=0),\n",
    "                            'Importance Std. Dev.': np.std(all_feature_importances, axis=0),\n",
    "                            {% endif %}\n",
    "                            })\n",
    "df_feat_imp = df_feat_imp.set_index('Feature').sort_values('Importance Mean', ascending=False)\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' %}\n",
    "figure_header('Table', 'Input features ranked by relative importance ({})'.format(make_clickable('feature_importance.csv')))\n",
    "show(df_feat_imp.reset_index(), maxBytes=0, order=[[ 2, \"asc\"]])\n",
    "figure_legend('Table', 'Input features ranked by relative importance ({})'.format(make_clickable('feature_importance.csv')),\n",
    "              f'All {X.shape[1]} input features are ranked by their relative importance. \\\n",
    "                Feature ranking (Ranking Mean and Std. Dev.) specifies the round of recursive feature selection on which a given feature was eliminated. \\\n",
    "                A feature with lower ranking is more \\\n",
    "                important. {% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier'] %}Tree-based \\\n",
    "                models can also be used to calculate impurity-based feature importances (Importance Mean and Std. Dev.). {% endif %}Search a dataset \\\n",
    "                name to filter features from a given dataset. \\\n",
    "                The table can also be sorted by other columns by selecting the column name in the header.')\n",
    "{% elif algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier'] %}\n",
    "figure_header('Table', 'Input features ranked by relative importance ({})'.format(make_clickable('feature_importance.csv')))\n",
    "show(df_feat_imp.reset_index(), maxBytes=0, order=[[ 2, \"desc\"]])\n",
    "figure_legend('Table', 'Input features ranked by relative importance ({})'.format(make_clickable('feature_importance.csv')),\n",
    "              f'All {X.shape[1]} input features are ranked by their relative importance. \\\n",
    "                Tree-based models can be used to calculate impurity-based feature importances (Importance Mean and Std. Dev.). \\\n",
    "                Search a dataset name to filter features from a given dataset. \\\n",
    "                The table can also be sorted by other columns by selecting the column name in the header.')\n",
    "{% else %}\n",
    "figure_header('Table', 'Input features ({})'.format(make_clickable('feature_importance.csv')))\n",
    "show(df_feat_imp.reset_index(), maxBytes=0)\n",
    "figure_legend('Table', 'Input features ({})'.format(make_clickable('feature_importance.csv')),\n",
    "              f'All {X.shape[1]} input features. No ranking of features was possible for this pipeline.')\n",
    "{% endif %}\n",
    "df_feat_imp.to_csv('feature_importance.csv')\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' and dimensionality_reduction.raw_value == 'None' %}\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "df_feat_imp = df_feat_imp.sort_values('Ranking Mean')\n",
    "for dataset in set(df_feat_imp.Dataset):\n",
    "    importance_scores = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Ranking Mean'].values\n",
    "    importance_scores_std = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Ranking Std. Dev.'].values\n",
    "    lower = importance_scores - importance_scores_std\n",
    "    upper = importance_scores + importance_scores_std\n",
    "    axs[0].plot(importance_scores, label=dataset)\n",
    "    axs[0].fill_between(np.arange(len(importance_scores)), lower, upper, alpha=.2)\n",
    "    axs[1].plot(np.linspace(0, 1, len(importance_scores)), importance_scores, label=dataset)\n",
    "    axs[1].fill_between(np.linspace(0, 1, len(importance_scores)), lower, upper, alpha=.2)\n",
    "for i in [0, 1]:\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title('Distribution of feature ranking from recursive feature elimination')\n",
    "    axs[i].set_ylabel('Average feature ranking\\n(lower ranking is more important)')\n",
    "axs[0].set_xlabel('Ranked features (absolute count)')\n",
    "axs[1].set_xlabel('Ranked features (relative count)')\n",
    "axs[0].set_xlim([0,512])\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_rfe.svg')\n",
    "figure_header('Figure', 'Distribution of feature rankings from recursive feature elimination ({})'.format(make_clickable('feature_importance_rfe.svg')))\n",
    "plt.show()\n",
    "figure_legend('Figure', 'Distribution of feature rankings from recursive feature elimination ({})'.format(make_clickable('feature_importance_rfe.svg')),\n",
    "              'The distribution of feature rankings from recursive feature elimination for each dataset. \\\n",
    "               Features with lower scores were retained for more rounds during recursive feature selection \\\n",
    "               and have greater relative importance.')\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier']  and dimensionality_reduction.raw_value == 'None' %}\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "df_feat_imp = df_feat_imp.sort_values('Importance Mean', ascending=False)\n",
    "for dataset in set(df_feat_imp.Dataset):\n",
    "    importance_scores = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Importance Mean'].values\n",
    "    importance_scores_std = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Importance Std. Dev.'].values\n",
    "    lower = importance_scores - importance_scores_std\n",
    "    upper = importance_scores + importance_scores_std\n",
    "    axs[0][0].plot(importance_scores, label=dataset)\n",
    "    axs[0][0].fill_between(np.arange(len(importance_scores)), lower, upper, alpha=.2)\n",
    "    axs[0][1].plot(np.linspace(0, 1, len(importance_scores)), importance_scores, label=dataset)\n",
    "    axs[0][1].fill_between(np.linspace(0, 1, len(importance_scores)), lower, upper, alpha=.2)\n",
    "    \n",
    "    importance_scores = np.cumsum(df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Importance Mean'].values)\n",
    "    axs[1][0].plot(importance_scores, label=dataset)\n",
    "    axs[1][1].plot(np.linspace(0, 1, len(importance_scores)), importance_scores, label=dataset)\n",
    "for i in [0, 1]:\n",
    "    axs[0][i].legend()\n",
    "    axs[0][i].set_title('Distribution of feature scores from model')\n",
    "    axs[1][i].set_title('Cumulative distribution of feature scores from model')\n",
    "    axs[i][0].set_xlabel('Ranked features (absolute count)')\n",
    "    axs[i][1].set_xlabel('Ranked features (relative count)')\n",
    "    axs[0][i].set_ylabel('Average feature importance\\n(higher score is more important)')\n",
    "    axs[1][i].set_ylabel('Cumulative sum of feature importance')\n",
    "    axs[i][0].set_xlim([0,512])\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.svg')\n",
    "figure_header('Figure', 'Distribution of feature scores from model ({})'.format(make_clickable('feature_importance.svg')))\n",
    "plt.show()\n",
    "figure_legend('Figure', 'Distribution of feature scores from model ({})'.format(make_clickable('feature_importance.svg')),\n",
    "              'The distribution of impurity-based feature importances for each dataset. \\\n",
    "               Features with higher scores have greater relative contribution to the overall tree-based model.')\n",
    "{% endif %}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-dml-env",
   "language": "python",
   "name": "rdkit-dml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
