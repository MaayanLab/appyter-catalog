{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%appyter init\n",
    "import os, sys; sys.path.insert(0, os.path.realpath('..'))\n",
    "from appyter import magic\n",
    "magic.init(lambda _=globals: _())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Imports\n",
    "## Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "## Machine Learning\n",
    "import sklearn as sk\n",
    "from sklearn import (\n",
    "    calibration,\n",
    "    decomposition,\n",
    "    ensemble,\n",
    "    feature_selection,\n",
    "    linear_model,\n",
    "    manifold,\n",
    "    metrics,\n",
    "    model_selection,\n",
    "    multioutput,\n",
    "    pipeline,\n",
    "    preprocessing,\n",
    "    svm,\n",
    "    tree,\n",
    "    feature_extraction,\n",
    "    neural_network,\n",
    ")\n",
    "from split import StratifiedGroupKFold, RepeatedStratifiedGroupKFold\n",
    "import umap\n",
    "## Plotting\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "## Drugmonizome API\n",
    "from drugmonizome import Drugmonizome\n",
    "## SEP-L1000 data retrieval\n",
    "from sepl1000 import SEPL1000\n",
    "## L1000FWD queries\n",
    "import querysepl1000fwd\n",
    "## Match drug name inputs using PubChem API\n",
    "from DrugNameConverter import DrugNameConverter\n",
    "# Utility\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from functools import reduce\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "# Interactive tables\n",
    "from itables import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = 2020\n",
    "np.random.seed(rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Input Datasets and Target Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected drug set libraries and omics datasets are downloaded and joined on the drug to produce a large association matrix. A machine learning model will be trained to predict the specified target labels from this association matrix. This is a binary classification task that can be used to predict drugs that are likely to be associated with the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% do SectionField(\n",
    "    title='Input Dataset Selection',\n",
    "    subtitle='Select the input datasets to use for learning and classification. \\\n",
    "              A model will be trained to predict the target labels from the selected features. \\\n",
    "              If no datasets are selected, default features will be used.',\n",
    "    name='ATTRIBUTES',\n",
    "    img='attributes.png',\n",
    ") %}\n",
    "\n",
    "{% set sepl1000datasets = MultiCheckboxField(\n",
    "    name='sepl1000datasets',\n",
    "    label='Transcriptomic and Imaging Datasets after Perturbation<br>(from the SEP-L1000 project)',\n",
    "    description='These input datasets were used previously for side effect prediction (https://maayanlab.net/SEP-L1000/).',\n",
    "    choices=[\n",
    "        'LINCS Gene Expression Signatures',\n",
    "        'GO Transformed Signatures (PAEA)',\n",
    "        'MLPCN Cell Morphological Profiling',\n",
    "        'MACCS Chemical Fingerprint',\n",
    "    ],\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set exprdatasets = MultiCheckboxField(\n",
    "    name='exprdatasets',\n",
    "    label='L1000FWD<br>(drug set libraries from Drugmonizome)',\n",
    "    description='Top up and down-regulated genes after perturbation, along with enriched pathways.',\n",
    "    choices=[\n",
    "        'L1000FWD Downregulated GO Biological Processes',\n",
    "        'L1000FWD Downregulated GO Cellular Components',\n",
    "        'L1000FWD Downregulated GO Molecular Function',\n",
    "        'L1000FWD Downregulated KEGG Pathways',\n",
    "        'L1000FWD Downregulated Signatures',\n",
    "        'L1000FWD Predicted Side Effects',\n",
    "        'L1000FWD Upregulated GO Biological Process',\n",
    "        'L1000FWD Upregulated GO Cellular Components',\n",
    "        'L1000FWD Upregulated GO Molecular Function',\n",
    "        'L1000FWD Upregulated KEGG Pathways',\n",
    "        'L1000FWD Upregulated Signatures',\n",
    "    ],\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set targetdatasets = MultiCheckboxField(\n",
    "    name='targetdatasets',\n",
    "    label='Drug Targets and Associated Genes<br>(drug set libraries from Drugmonizome)',\n",
    "    choices=[\n",
    "        'Downregulated CREEDS Signatures',\n",
    "        'Upregulated CREEDS Signatures',\n",
    "        'DrugCentral Targets',\n",
    "        'DrugRepurposingHub Drug Targets',\n",
    "        'Drugbank Small Molecule Carriers',\n",
    "        'Drugbank Small Molecule Enzymes',\n",
    "        'Drugbank Small Molecule Targets',\n",
    "        'Drugbank Small Molecule Transporters',\n",
    "        'Geneshot Associated Genes',\n",
    "        'Geneshot Predicted AutoRIF Genes',\n",
    "        'Geneshot Predicted Coexpression Genes',\n",
    "        'Geneshot Predicted Enrichr Genes',\n",
    "        'Geneshot Predicted GeneRIF Genes',\n",
    "        'Geneshot Predicted Tagger Genes',\n",
    "        'KinomeScan Kinases',\n",
    "        'PharmGKB Single Nucleotide Polymorphisms',\n",
    "        'STITCH Targets',\n",
    "    ],\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set indicationdatasets = MultiCheckboxField(\n",
    "    name='indicationdatasets',\n",
    "    label='Indications, Modes of Action, and Side Effects<br>(drug set libraries from Drugmonizome)',\n",
    "    choices=[\n",
    "        'ATC Codes Drugsetlibrary',\n",
    "        'DrugRepurposingHub Mechanisms of Action',\n",
    "        'PharmGKB OFFSIDES Side Effects',\n",
    "        'SIDER Indications',\n",
    "        'SIDER Side Effects',\n",
    "    ],\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set structuraldatasets = MultiCheckboxField(\n",
    "    name='structuraldatasets',\n",
    "    label='Structural Features<br>(drug set libraries from Drugmonizome)',\n",
    "    choices=[\n",
    "        'RDKIT MACCS Chemical Fingerprints'\n",
    "    ],\n",
    "    default=[],\n",
    "    section='ATTRIBUTES'\n",
    ") %}\n",
    "\n",
    "{% set keepmissing = BoolField(\n",
    "    name='keepmissing',\n",
    "    label='Keep drugs with missing data when joining datasets',\n",
    "    description='Keep drugs that appear in some datasets and not in others. \\\n",
    "                 Missing data is filled in with zeros. Otherwise, only drugs \\\n",
    "                 that are present in all datasets are preserved.',\n",
    "    default=False,\n",
    "    section='ATTRIBUTES',\n",
    ") %}\n",
    "\n",
    "{% set tfidf = BoolField(\n",
    "    name='tfidf',\n",
    "    label='Apply tfâ€“idf normalization to binary inputs',\n",
    "    description='For binary drug-attribute associations in the input matrix, \\\n",
    "                 apply tf-idf transformation to normalize data.',\n",
    "    default=True,\n",
    "    section='ATTRIBUTES',\n",
    ") %}\n",
    "\n",
    "{% set attribute_datasets = exprdatasets.value +\n",
    "                             targetdatasets.value +\n",
    "                             indicationdatasets.value +\n",
    "                             structuraldatasets.value %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "To construct the input matrix, we download drug set libraries and omics datasets and join them on the InChI Key.\n",
    "{% if keepmissing.value %} Drugs that appear in some datasets and not in others are retained, and missing data is filled in with zeros.\n",
    "{% else %} Only drugs that are present in all datasets are retained.\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% do SectionField(\n",
    "    title='Target Label Selection',\n",
    "    subtitle='Upload a list of compounds or select an attribute from Drugmonizome to be assigned a positive class label for binary classification.',\n",
    "    name='TARGET',\n",
    "    img='target.png',\n",
    ") %}\n",
    "\n",
    "{% set target_field = TargetField(\n",
    "    name='target_field',\n",
    "    label='Target Selection',\n",
    "    default='Attribute',\n",
    "    description='Select input method',\n",
    "    choices={\n",
    "        'List': [\n",
    "            ChoiceField(\n",
    "                name='drugformat',\n",
    "                label='Drug Identifier Format',\n",
    "                description='Compounds can be specified by either drug name or InChI Key.',\n",
    "                default='InChI Key',\n",
    "                choices=[\n",
    "                    'Drug Name',\n",
    "                    'InChI Key'\n",
    "                ],\n",
    "                section='TARGET'\n",
    "            ),\n",
    "            FileField(\n",
    "                name='drughitlist',\n",
    "                label='Upload List of Compounds',\n",
    "                description='Upload a list of compounds to be assigned positive class labels for binary classification. \\\n",
    "                             Compounds should be in a text file, specified by either drug name or InChI Key and separated by newlines.',\n",
    "                default='COVID19ScreenHitsInChIKeys.txt',\n",
    "                examples={\n",
    "                    'COVID19ScreenHits.txt': 'https://appyters.maayanlab.cloud/storage/Drugmonizome_ML/COVID19ScreenHits.txt',\n",
    "                    'COVID19ScreenHitsInChIKeys.txt': 'https://appyters.maayanlab.cloud/storage/Drugmonizome_ML/COVID19ScreenHitsInChIKeys.txt',\n",
    "                },\n",
    "                section='TARGET'\n",
    "            ),\n",
    "        ],\n",
    "        'Attribute': [\n",
    "            AutocompleteField(\n",
    "                name='target_attribute',\n",
    "                description='Enter a small molecule attribute from one of the Drugmonizome datasets that should be predicted.',\n",
    "                file_path=\"https://appyters.maayanlab.cloud/storage/Drugmonizome_ML/drugmonizome_terms.json\",\n",
    "                label='Attribute',\n",
    "                hint='Enter Drugmonizome term...',\n",
    "                default='',\n",
    "                constraint='(^(.+) \\\\(from (.+)\\\\)$|^$)',\n",
    "        )],\n",
    "    },\n",
    "    section='TARGET',\n",
    ") %}\n",
    "\n",
    "{% set includestereo = BoolField(\n",
    "    name='includestereo',\n",
    "    label='Include stereoisomers',\n",
    "    description='If true, compounds are matched to entries in the datasets by the first 14 characters of their InChI Keys, \\\n",
    "                 so stereoisomers of the compounds in the input list or with a particular attritube are also counted as hits. \\\n",
    "                 Note that different resources record different details for charge and stereochemistry, \\\n",
    "                 causing some compounds to have different full-length InChI Keys in different datasets. \\\n",
    "                 Selecting this option may allow such drugs to be better matched to entries in the datasets.',\n",
    "    default=True,\n",
    "    section='TARGET',\n",
    ") %}\n",
    "\n",
    "{% set target_name, target_dataset = '', '' %}\n",
    "{% if TargetField.raw_value == 'Attribute' %}\n",
    "{% set target_name, target_dataset = target_field.value[0].value|re_match('^(.+) \\\\(from (.+)\\\\)$') %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if sepl1000datasets.value == [] and attribute_datasets == [] %}\n",
    "# No datasets selected, so use default dataset\n",
    "sepl1000datasets = ['LINCS Gene Expression Signatures']\n",
    "{% else %}\n",
    "# Use the selected SEP-L1000 datasets\n",
    "sepl1000datasets = {{ sepl1000datasets }}\n",
    "{% endif %}\n",
    "\n",
    "{% if sepl1000datasets.value != [] or attribute_datasets == [] %}\n",
    "name_to_file = {\n",
    "    'LINCS Gene Expression Signatures': 'LINCS_Gene_Experssion_signatures_CD.csv.gz',\n",
    "    'GO Transformed Signatures (PAEA)': 'GO_transformed_signatures_PAEA.csv.gz',\n",
    "    'MLPCN Cell Morphological Profiling': 'MLPCN_morplological_profiles.csv.gz',\n",
    "    'MACCS Chemical Fingerprint': 'MACCS_bitmatrix.csv.gz',\n",
    "}\n",
    "\n",
    "df_sepl1000_list = list(SEPL1000.download_df(list(name_to_file[dataset] for dataset in sepl1000datasets),\n",
    "                                             index_col=0))\n",
    "dataset_sizes = list(zip(sepl1000datasets, [dataset.shape[1] for dataset in df_sepl1000_list]))\n",
    "\n",
    "# Assemble all SEP-L1000 datasets\n",
    "if len(df_sepl1000_list) > 1:\n",
    "    # Obtain merged dataframe with omics and target data\n",
    "    df_sepl1000 = reduce(\n",
    "        lambda a, b: pd.merge( # Merge two dataframes item by item\n",
    "            a, # left\n",
    "            b, # right\n",
    "            # Items with the same left and right index are merged\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            {% if keepmissing.value %}\n",
    "            how='outer', # Keep mis-matched indices\n",
    "            {% else %}\n",
    "            how='inner', # Keep only matched indices\n",
    "            {% endif %}\n",
    "        ),\n",
    "        df_sepl1000_list,\n",
    "    )\n",
    "else:\n",
    "    df_sepl1000 = df_sepl1000_list[0]\n",
    "\n",
    "del(df_sepl1000_list)\n",
    "\n",
    "# Mean-fill infinite and missing values\n",
    "df_sepl1000 = df_sepl1000.replace([np.inf, -np.inf], np.nan)\n",
    "df_sepl1000 = df_sepl1000.fillna(np.mean(df_sepl1000))\n",
    "print('Total shape:', df_sepl1000.shape)\n",
    "display(df_sepl1000.head())\n",
    "{% else %}\n",
    "dataset_sizes = []\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "# Use the selected attribute datasets\n",
    "attribute_datasets = {{ attribute_datasets }}\n",
    "\n",
    "{% if attribute_datasets == [] %}\n",
    "X = df_sepl1000\n",
    "{% else %}\n",
    "df_attributes = list(Drugmonizome.download_df(\n",
    "    [dataset\n",
    "     for dataset in attribute_datasets]\n",
    "))\n",
    "dataset_sizes += list(zip(attribute_datasets, [dataset.shape[1] for dataset in df_attributes]))\n",
    "\n",
    "# Assemble all attribute datasets\n",
    "if len(df_attributes) > 1:\n",
    "    # Obtain merged dataframe with omics and target data\n",
    "    df = reduce(\n",
    "        lambda a, b: pd.merge( # Merge two dataframes item by item\n",
    "            a, # left\n",
    "            b, # right\n",
    "            # Items with the same left and right index are merged\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            {% if keepmissing.value %}\n",
    "            how='outer', # Keep mis-matched indices\n",
    "            {% else %}\n",
    "            how='inner', # Keep only matched indices\n",
    "            {% endif %}\n",
    "        ),\n",
    "        df_attributes,\n",
    "    )\n",
    "else:\n",
    "    df = df_attributes[0]\n",
    "\n",
    "del(df_attributes)\n",
    "\n",
    "df = df.fillna(0)\n",
    "X = df.applymap(lambda f: 1 if f!=0 else 0)\n",
    "{% if tfidf.value %}\n",
    "# Apply tf-idf normalization\n",
    "transformer = feature_extraction.text.TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X).toarray()\n",
    "X = pd.DataFrame(X_tfidf, columns=X.columns, index=X.index)\n",
    "{% if sepl1000datasets.value != [] %}\n",
    "{% if keepmissing.value %}\n",
    "X = pd.merge(df_sepl1000, X, left_index=True, right_index=True, how='outer') # Keep mis-matched indices\n",
    "{% else %}\n",
    "X = pd.merge(df_sepl1000, X, left_index=True, right_index=True) # Keep only matched indices\n",
    "{% endif %}\n",
    "{% endif %}\n",
    "{% endif %}\n",
    "{% endif %}\n",
    "\n",
    "print('Total shape:', X.shape)\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if target_field.raw_value == 'List' %}\n",
    "The target labels are produced from the uploaded list of hits: 1 if the drug is specified as a hit, 0 otherwise.\n",
    "{% if target_field.value[0].value == 'Drug Name' %} Drug names are matched to InChI Keys from the Drugmonizome metadata.\n",
    "{% endif %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if target_field.raw_value == 'List' %}\n",
    "{% if target_field.value[1].value == '' %}\n",
    "# Using default list of hits from COVID-19 in vitro drug screens\n",
    "hits_filename = '../../COVID19ScreenHits.txt'\n",
    "{% else %}\n",
    "# Using user-specified list of positive drug hits\n",
    "hits_filename = {{target_field.value[1]}}\n",
    "{% endif %}\n",
    "\n",
    "{% if target_field.value[0].value == 'InChI Key' %}\n",
    "# Read InChI Keys from file\n",
    "with open(hits_filename, 'r') as hits_file:\n",
    "    drug_hits = set(drug.strip().upper() for drug in hits_file.read().strip().split('\\n') \n",
    "                    if len(drug.strip()) > 0)\n",
    "\n",
    "{% elif target_field.value[0].value == 'Drug Name' %}\n",
    "# Helper functions\n",
    "def merge(A, B, f):\n",
    "    \"\"\"\n",
    "    Merges two dictionaries, where items from shared keys are merged using a custom function.\n",
    "    \"\"\"\n",
    "    merged = {k: A.get(k, B.get(k)) for k in A.keys() ^ B.keys()}\n",
    "    merged.update({k: f(A[k], B[k]) for k in A.keys() & B.keys()})\n",
    "    return merged\n",
    "\n",
    "def save_items(out_file, items):\n",
    "    \"\"\"\n",
    "    Saves list of items as rows in a file.\n",
    "    \"\"\"\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(len(items)):\n",
    "            if i < len(items) - 1:\n",
    "                f.write(items[i] + '\\n')\n",
    "            else:\n",
    "                f.write(items[i])\n",
    "\n",
    "def save_gmt(out_file, keys_to_sets, sep='\\t'):\n",
    "    \"\"\"\n",
    "    Saves dict with key-set pairs as gmt file format.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for key in sorted(keys_to_sets):\n",
    "        lines.append(key + sep*2 + sep.join(sorted(keys_to_sets[key])))\n",
    "    save_items(out_file, lines)\n",
    "\n",
    "# Read drug names from file\n",
    "with open(hits_filename, 'r') as hits_file:\n",
    "    drug_hits = set(drug.strip().lower() for drug in hits_file.read().strip().split('\\n') \n",
    "                    if len(drug.strip()) > 0)\n",
    "\n",
    "# Query PubChem API to map drug names to InChI Keys\n",
    "print('Querying PubChem API...')\n",
    "drug_hits_inchi_pubchem = DrugNameConverter.batch_to_inchi_keys(drug_hits)\n",
    "# Query Drugmonizome API to map drug names to InChI Keys\n",
    "print('Querying Drugmonizome API...')\n",
    "drug_hits_inchi_drugmonizome = Drugmonizome.map_names_to_inchi_keys(drug_hits)\n",
    "# Query L1000FWD API to map drug names to InChI Keys\n",
    "print('Querying L1000FWD API...')\n",
    "drug_hits_inchi_l1000fwd = querysepl1000fwd.map_names_to_inchi_keys(drug_hits)\n",
    "\n",
    "# Combine InChI Keys from all resources\n",
    "drug_hits_inchi = merge(drug_hits_inchi_pubchem, drug_hits_inchi_drugmonizome, lambda s1, s2: s1 | s2)\n",
    "drug_hits_inchi = merge(drug_hits_inchi, drug_hits_inchi_l1000fwd, lambda s1, s2: s1 | s2)\n",
    "save_gmt('hits_drug_name_to_inchi_keys.gmt', drug_hits_inchi)\n",
    "# Unmatched drug names\n",
    "unmatched_drugs = set(drug for drug in drug_hits\n",
    "                      if drug not in drug_hits_inchi or len(drug_hits_inchi[drug]) == 0)\n",
    "print(f'Drugs without InChI Keys ({ len(unmatched_drugs) }/{ len(drug_hits) }):', unmatched_drugs)\n",
    "\n",
    "# Set of InChI Keys for user-specified hits\n",
    "drug_hits = set(key for drug in drug_hits_inchi\n",
    "                    for key in drug_hits_inchi[drug])\n",
    "save_items('hits_inchi_keys.txt', sorted(drug_hits))\n",
    "{% endif %}\n",
    "\n",
    "{% else %}\n",
    "\n",
    "df_target = list(Drugmonizome.download_df(\n",
    "    ['{{ target_dataset }}']\n",
    "))\n",
    "df = df_target[0]\n",
    "df = df.fillna(0)\n",
    "Y = df.applymap(lambda f: 1 if f!=0 else 0)\n",
    "drug_hits = set(Y[Y['{{ target_name }}'] == 1].index)\n",
    "\n",
    "# Helper function\n",
    "def save_items(out_file, items):\n",
    "    \"\"\"\n",
    "    Saves list of items as rows in a file.\n",
    "    \"\"\"\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(len(items)):\n",
    "            if i < len(items) - 1:\n",
    "                f.write(items[i] + '\\n')\n",
    "            else:\n",
    "                f.write(items[i])\n",
    "save_items('hits_inchi_keys.txt', sorted(drug_hits))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if target_field.raw_value == 'List' %}\n",
    "{% if target_field.value[0].value == 'Drug Name' %}\n",
    "For the user-inputted drug names:\n",
    "* Mapping of drug name to InChI Key: [hits_drug_name_to_inchi_keys.gmt](./hits_drug_name_to_inchi_keys.gmt)\n",
    "* List of InChI Keys: [hits_inchi_keys.txt](./hits_inchi_keys.txt)\n",
    "{% endif %}\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We produce a target array containing 1 if the drug is specified as a hit and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if includestereo.value %}\n",
    "# Match first 14 characters of InChI Keys (hash of InChI connectivity information)\n",
    "drug_hits_inchi_main_layer = set(key[:14] for key in drug_hits)\n",
    "y = np.array([drug[:14] in drug_hits_inchi_main_layer for drug in X.index]).astype(np.int8)\n",
    "{% else %}\n",
    "# Match full InChI Keys\n",
    "y = np.array([drug in drug_hits for drug in X.index]).astype(np.int8)\n",
    "{% endif %}\n",
    "print('Number of hits matched in input: %d (%0.3f %%)' % (y.sum(), 100*y.sum()/len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data shapes\n",
    "print('Input shape:', X.shape)\n",
    "print('Target shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% do SectionField(\n",
    "    title='Machine Learning Pipeline',\n",
    "    subtitle='Select from available machine learning algorithms, their unique settings, and methods to use to evaluate the classifier.',\n",
    "    name='SETTINGS',\n",
    "    img='settings.png',\n",
    ") %}\n",
    "\n",
    "{% set visualization_reduction = ChoiceField(\n",
    "    name='visualization_reduction',\n",
    "    label='Data Visualization Method',\n",
    "    description='Select a dimensionality reduction algorithm for data visualization.',\n",
    "    default='UMAP',\n",
    "    choices={\n",
    "        'UMAP': 'umap.UMAP(random_state=rng)',\n",
    "        'NMF': 'sk.decomposition.NMF(n_components=2)',\n",
    "        'PCA': 'sk.decomposition.PCA(n_components=2)',\n",
    "        'TruncatedSVD': 'sk.decomposition.TruncatedSVD(n_components=2)',\n",
    "        'IncrementalPCA': 'sk.decomposition.IncrementalPCA(n_components=2)',\n",
    "        'ICA': 'sk.decomposition.FastICA(n_components=2)',\n",
    "        'SparsePCA': 'sk.decomposition.SparsePCA(n_components=2)',\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "We reduce the dimensionality of our omics feature space for visualization with {{ visualization_reduction.raw_value }}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "clf_dimensionality_reduction = {{ visualization_reduction }}\n",
    "X_reduced = clf_dimensionality_reduction.fit_transform(X.values)\n",
    "{% if visualization_reduction.raw_value == 'PCA' %}\n",
    "print('Explained variance:', np.sum(clf_dimensionality_reduction.explained_variance_))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced_df = pd.DataFrame(X_reduced, columns=['Component 1', 'Component 2'])\n",
    "X_reduced_df['Drug Name'] = querysepl1000fwd.get_drug_names(X.index)\n",
    "X_reduced_df['InChI Key'] = X.index\n",
    "X_reduced_df['Label'] = y\n",
    "X_reduced_df['marker symbol'] = ['x' if label else 'circle' for label in X_reduced_df['Label']]\n",
    "X_reduced_df['text'] = ['<br>'.join(['Drug Name: ' + str(name),\n",
    "                                     'InChI Key: ' + str(inchi),\n",
    "                                     'Label: ' + str(label)])\n",
    "                        for name, inchi, label in zip(X_reduced_df['Drug Name'],\n",
    "                                                      X_reduced_df['InChI Key'],\n",
    "                                                      X_reduced_df['Label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "fig = go.Figure()\n",
    "for label in set(X_reduced_df['Label']):\n",
    "    X_plot = X_reduced_df[X_reduced_df['Label'] == label].sort_values('Label')\n",
    "    fig.add_trace(go.Scatter(mode='markers',\n",
    "                             x=X_plot['Component 1'], y=X_plot['Component 2'],\n",
    "                             text=X_plot['text'],\n",
    "                             name=label,\n",
    "                             marker=dict(\n",
    "                                 color=['#0d0887', '#f0f921'][label%2],\n",
    "                                 size=8,\n",
    "                                 symbol=X_plot['marker symbol'],\n",
    "                                 line_width=1,\n",
    "                                 line_color='white'\n",
    "                             )))\n",
    "fig.update_layout(height=600, width=800,\n",
    "                  xaxis_title='Component 1',\n",
    "                  yaxis_title='Component 2',\n",
    "                  title_text='Known Labels ({{ visualization_reduction.raw_value }})',\n",
    "                  legend_title_text='Target Label',\n",
    "                  template='simple_white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     41,
     81,
     142,
     169,
     230,
     292,
     335,
     378,
     451,
     562,
     614
    ]
   },
   "outputs": [],
   "source": [
    "%%appyter hide\n",
    "{% set dimensionality_reduction = ChoiceField(\n",
    "    name='dimensionality_reduction',\n",
    "    label='Dimensionality Reduction Algorithm',\n",
    "    description='Optionally select a dimensionality reduction algorithm as a data preprocessing step in the ML pipeline.',\n",
    "    default='None',\n",
    "    choices={\n",
    "        'None': 'None',\n",
    "        'PCA': 'sk.decomposition.PCA(n_components=64)',\n",
    "        'TruncatedSVD': 'sk.decomposition.TruncatedSVD(n_components=64)',\n",
    "        'IncrementalPCA': 'sk.decomposition.IncrementalPCA(n_components=64)',\n",
    "        'ICA': 'sk.decomposition.FastICA(n_components=64)',\n",
    "        'SparsePCA': 'sk.decomposition.SparsePCA(n_components=64)',\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set feature_selection = ChoiceField(\n",
    "    name='feature_selection',\n",
    "    label='Machine Learning Feature Selection',\n",
    "    description='Optionally select a feature selection algorithm to include in the ML pipeline. \\\n",
    "                 If RecursiveSelectionFromExtraTrees is chosen, additional information can be obtained \\\n",
    "                 on the relative importance of different features based on which features are eliminated.',\n",
    "    default='None',\n",
    "    choices={\n",
    "        'None': 'None',\n",
    "        'SelectFromLinearSVC': 'sk.feature_selection.SelectFromModel(sk.svm.LinearSVC(loss=\"squared_hinge\", penalty=\"l1\", dual=False, class_weight=\"balanced\"))',\n",
    "        'SelectFromExtraTrees': 'sk.feature_selection.SelectFromModel(sk.ensemble.ExtraTreesClassifier(class_weight=\"balanced\"))',\n",
    "        'RecursiveSelectionFromExtraTrees': 'sk.feature_selection.RFE(sk.ensemble.ExtraTreesClassifier(class_weight=\"balanced\"), n_features_to_select=256, step=0.1)',\n",
    "        'SelectKBest': 'sk.feature_selection.SelectKBest(\"f_classif\")',\n",
    "        'SelectKBestChi2': 'sk.feature_selection.SelectKBest(\"chi2\")',\n",
    "        'SelectKBestMultiInfo': 'sk.feature_selection.SelectKBest(\"mutual_info_classif\")',\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set algorithm = TargetField(\n",
    "    name='algorithm',\n",
    "    label='Machine Learning Algorithm',\n",
    "    default='ExtraTreesClassifier',\n",
    "    description='Select a machine learning algorithm to construct the predictive model. \\\n",
    "                 (See scikit-learn User Guide for details.)',\n",
    "    choices={\n",
    "        'GradientBoostingClassifier': [\n",
    "            ChoiceField(\n",
    "                name='loss_gb',\n",
    "                label='loss',\n",
    "                description='Loss function to be optimized.',\n",
    "                default=\"deviance\",\n",
    "                choices=[\"deviance\", \"exponential\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='learning_rate_gb',\n",
    "                label='learning_rate',\n",
    "                description='Shrinks the contribution of each tree by learning_rate.',\n",
    "                default=0.1,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='n_estimators_gb',\n",
    "                label='n_estimators',\n",
    "                description='Number of boosting stages to perform.',\n",
    "                default=100,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='subsample_gb',\n",
    "                label='subsample',\n",
    "                description='Fraction of samples to be used for fitting individual base learners.',\n",
    "                default=1.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='criterion_gb',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"friedman_mse\",\n",
    "                choices=[\"friedman_mse\", \"mse\", \"mae\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='tol_gb',\n",
    "                label='tol',\n",
    "                description='Tolerance for early stopping.',\n",
    "                default=1e-4,\n",
    "            ),\n",
    "        ],\n",
    "        'RandomForestClassifier': [\n",
    "            IntField(\n",
    "                name='n_estimators_rf',\n",
    "                label='n_estimators',\n",
    "                description='Number of trees in the forest.',\n",
    "                default=100,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='criterion_rf',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"gini\",\n",
    "                choices=[\"gini\", \"entropy\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_split_rf',\n",
    "                label='min_samples_split',\n",
    "                description='Minimum number of samples required to split an internal node. \\\n",
    "                             If int, then min_samples_split specifies the minimum number. \\\n",
    "                             If float, then min_samples_split specifies a fraction of the total number of samples.',\n",
    "                default=2,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_leaf_rf',\n",
    "                label='min_samples_leaf',\n",
    "                description='Minimum number of samples required to be at a leaf node. \\\n",
    "                             If int, then min_samples_leaf specifies the minimum number. \\\n",
    "                             If float, then min_samples_leaf specifies a fraction of the total number of samples.',\n",
    "                default=1,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='max_features_rf',\n",
    "                label='max_features',\n",
    "                description='The number of features to consider when looking for the best split.',\n",
    "                default=\"None\",\n",
    "                choices=[\"None\", '\"auto\"', '\"sqrt\"', '\"log2\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_impurity_decrease_rf',\n",
    "                label='min_impurity_decrease',\n",
    "                description='A node will be split if this split induces a decrease of the impurity greater than or equal to this value.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='class_weight_rf',\n",
    "                label='class_weight',\n",
    "                description='Weights associated with classes. If None, then all classes have weight one. \\\n",
    "                             The balanced mode adjusts weights inversely proportional to class frequencies in the input data. \\\n",
    "                             The balanced_subsample mode is the same as balanced except weights are computed based on the bootstrap sample for each tree.',\n",
    "                default='\"balanced\"',\n",
    "                choices=[\"None\", '\"balanced\"', '\"balanced_subsample\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='ccp_alpha_rf',\n",
    "                label='ccp_alpha',\n",
    "                description='Complexity parameter used for Minimal Cost-Complexity Pruning. \\\n",
    "                             The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. \\\n",
    "                             By default, no pruning is performed.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "        ],\n",
    "        'AdaBoostClassifier': [\n",
    "            IntField(\n",
    "                name='max_depth_ab',\n",
    "                label='max_depth',\n",
    "                description='Maximum depth of the decision tree used as the base estimator.',\n",
    "                default=1,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='n_estimators_ab',\n",
    "                label='n_estimators',\n",
    "                description='Maximum number of estimators at which boosting is terminated.',\n",
    "                default=50,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='learning_rate_ab',\n",
    "                label='learning_rate',\n",
    "                description='Shrinks the contribution of each classifier by learning_rate.',\n",
    "                default=1.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='algorithm_ab',\n",
    "                label='algorithm',\n",
    "                description='Select the real or discrete boosting algorithm to use.',\n",
    "                default=\"SAMME.R\",\n",
    "                choices=[\"SAMME\", \"SAMME.R\"],\n",
    "            ),\n",
    "        ],\n",
    "        'ExtraTreesClassifier': [\n",
    "            IntField(\n",
    "                name='n_estimators_et',\n",
    "                label='n_estimators',\n",
    "                description='Number of trees in the forest.',\n",
    "                default=100,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='criterion_et',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"gini\",\n",
    "                choices=[\"gini\", \"entropy\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_split_et',\n",
    "                label='min_samples_split',\n",
    "                description='Minimum number of samples required to split an internal node. \\\n",
    "                             If int, then min_samples_split specifies the minimum number. \\\n",
    "                             If float, then min_samples_split specifies a fraction of the total number of samples.',\n",
    "                default=2,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_leaf_et',\n",
    "                label='min_samples_leaf',\n",
    "                description='Minimum number of samples required to be at a leaf node. \\\n",
    "                             If int, then min_samples_leaf specifies the minimum number. \\\n",
    "                             If float, then min_samples_leaf specifies a fraction of the total number of samples.',\n",
    "                default=1,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='max_features_et',\n",
    "                label='max_features',\n",
    "                description='The number of features to consider when looking for the best split.',\n",
    "                default=\"None\",\n",
    "                choices=[\"None\", '\"auto\"', '\"sqrt\"', '\"log2\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_impurity_decrease_et',\n",
    "                label='min_impurity_decrease',\n",
    "                description='A node will be split if this split induces a decrease of the impurity greater than or equal to this value.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='class_weight_et',\n",
    "                label='class_weight',\n",
    "                description='Weights associated with classes. If None, then all classes have weight one. \\\n",
    "                             The balanced mode adjusts weights inversely proportional to class frequencies in the input data. \\\n",
    "                             The balanced_subsample mode is the same as balanced except weights are computed based on the bootstrap sample for each tree.',\n",
    "                default='\"balanced\"',\n",
    "                choices=[\"None\", '\"balanced\"', '\"balanced_subsample\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='ccp_alpha_et',\n",
    "                label='ccp_alpha',\n",
    "                description='Complexity parameter used for Minimal Cost-Complexity Pruning. \\\n",
    "                             The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. \\\n",
    "                             By default, no pruning is performed.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "        ],\n",
    "        'DecisionTreeClassifier': [\n",
    "            ChoiceField(\n",
    "                name='criterion_dt',\n",
    "                label='criterion',\n",
    "                description='Function to measure the quality of a split.',\n",
    "                default=\"gini\",\n",
    "                choices=[\"gini\", \"entropy\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='splitter_dt',\n",
    "                label='splitter',\n",
    "                description='Strategy used to choose the split at each node.',\n",
    "                default=\"best\",\n",
    "                choices=[\"best\", \"random\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_split_dt',\n",
    "                label='min_samples_split',\n",
    "                description='Minimum number of samples required to split an internal node. \\\n",
    "                             If int, then min_samples_split specifies the minimum number. \\\n",
    "                             If float, then min_samples_split specifies a fraction of the total number of samples.',\n",
    "                default=2,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_samples_leaf_dt',\n",
    "                label='min_samples_leaf',\n",
    "                description='Minimum number of samples required to be at a leaf node. \\\n",
    "                             If int, then min_samples_leaf specifies the minimum number. \\\n",
    "                             If float, then min_samples_leaf specifies a fraction of the total number of samples.',\n",
    "                default=1,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='max_features_dt',\n",
    "                label='max_features',\n",
    "                description='The number of features to consider when looking for the best split.',\n",
    "                default=\"None\",\n",
    "                choices=[\"None\", '\"auto\"', '\"sqrt\"', '\"log2\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='min_impurity_decrease_dt',\n",
    "                label='min_impurity_decrease',\n",
    "                description='A node will be split if this split induces a decrease of the impurity greater than or equal to this value.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='class_weight_dt',\n",
    "                label='class_weight',\n",
    "                description='Weights associated with classes. If None, then all classes have weight one. \\\n",
    "                             The balanced mode adjusts weights inversely proportional to class frequencies in the input data. \\\n",
    "                             The balanced_subsample mode is the same as balanced except weights are computed based on the bootstrap sample for each tree.',\n",
    "                default='\"balanced\"',\n",
    "                choices=[\"None\", '\"balanced\"', '\"balanced_subsample\"'],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='ccp_alpha_dt',\n",
    "                label='ccp_alpha',\n",
    "                description='Complexity parameter used for Minimal Cost-Complexity Pruning. \\\n",
    "                             The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. \\\n",
    "                             By default, no pruning is performed.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "        ],\n",
    "        'KNeighborsClassifier': [\n",
    "            IntField(\n",
    "                name='n_neighbors_knn',\n",
    "                label='n_neighbors',\n",
    "                description='Number of neighbors to use for queries.',\n",
    "                default=5,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='weights_knn',\n",
    "                label='weights',\n",
    "                description='Weight function used in prediction. \\\n",
    "                             If uniform, all points in each neighborhood are weighted equally. \\\n",
    "                             If distance, points are weighted by the inverse of their distance.',\n",
    "                default=\"uniform\",\n",
    "                choices=[\"uniform\", \"distance\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='algorithm_knn',\n",
    "                label='algorithm',\n",
    "                description='Algorithm used to compute the nearest neighbors.',\n",
    "                default=\"auto\",\n",
    "                choices=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "            ),\n",
    "            IntField(\n",
    "                name='leaf_size_knn',\n",
    "                label='leaf_size',\n",
    "                description='Leaf size passed to BallTree or KDTree.',\n",
    "                default=30,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='p_knn',\n",
    "                label='p',\n",
    "                description='Power parameter for the Minkowski metric.',\n",
    "                default=2,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='metric_knn',\n",
    "                label='metric',\n",
    "                description='Distance metric to use for the tree.',\n",
    "                default=\"minkowski\",\n",
    "                choices=[\"minkowski\", \"euclidean\", \"manhattan\", \"chebyshev\"],\n",
    "            ),\n",
    "        ],\n",
    "        'RadiusNeighborsClassifier': [\n",
    "            FloatField(\n",
    "                name='radius_rn',\n",
    "                label='radius',\n",
    "                description='Range of parameter space to use for queries.',\n",
    "                default=1.0,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='weights_rn',\n",
    "                label='weights',\n",
    "                description='Weight function used in prediction. \\\n",
    "                             If uniform, all points in each neighborhood are weighted equally. \\\n",
    "                             If distance, points are weighted by the inverse of their distance.',\n",
    "                default=\"uniform\",\n",
    "                choices=[\"uniform\", \"distance\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='algorithm_rn',\n",
    "                label='algorithm',\n",
    "                description='Algorithm used to compute the nearest neighbors.',\n",
    "                default=\"auto\",\n",
    "                choices=[\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "            ),\n",
    "            IntField(\n",
    "                name='leaf_size_rn',\n",
    "                label='leaf_size',\n",
    "                description='Leaf size passed to BallTree or KDTree.',\n",
    "                default=30,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='p_rn',\n",
    "                label='p',\n",
    "                description='Power parameter for the Minkowski metric.',\n",
    "                default=2,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='metric_rn',\n",
    "                label='metric',\n",
    "                description='Distance metric to use for the tree.',\n",
    "                default=\"minkowski\",\n",
    "                choices=[\"minkowski\", \"euclidean\", \"manhattan\", \"chebyshev\"],\n",
    "            ),\n",
    "        ],\n",
    "        'MLPClassifier': [\n",
    "            StringField(\n",
    "                name='hidden_layer_sizes_mlp',\n",
    "                label='hidden_layer_sizes',\n",
    "                description='Enter a tuple, where the ith element represents the number of neurons in the ith hidden layer.',\n",
    "                hint='Enter a tuple: e.g. (128, 64)',\n",
    "                default='(100,)',\n",
    "                constraint='^\\\\(\\\\s*(?:\\\\d+,\\\\s*)+(?:\\\\d+,?\\\\s*)?\\\\)$',\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='activation_mlp',\n",
    "                label='activation',\n",
    "                description='Activation function for the hidden layer.',\n",
    "                default=\"relu\",\n",
    "                choices=[\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='solver_mlp',\n",
    "                label='solver',\n",
    "                description='Solver for weight optimization.',\n",
    "                default=\"adam\",\n",
    "                choices=[\"lbfgs\", \"sgd\", \"adam\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='alpha_mlp',\n",
    "                label='alpha',\n",
    "                description='L2 penality (regularization term) parameter.',\n",
    "                default=0.0001,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='learning_rate_mlp',\n",
    "                label='learning_rate',\n",
    "                description='Learning rate schedule for weight updates. Only used for sgd solver.',\n",
    "                default=\"constant\",\n",
    "                choices=[\"constant\", \"invscaling\", \"adaptive\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='learning_rate_init_mlp',\n",
    "                label='learning_rate_init',\n",
    "                description='The initial learning rate used. Controls the step-size in updating the weights. Only used for sgd or adam solver.',\n",
    "                default=0.001,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='power_t_mlp',\n",
    "                label='power_t',\n",
    "                description='Exponent for inverse scaling learning rate. Only used for sgd solver with invscaling for learning_rate.',\n",
    "                default=0.5,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='max_iter_mlp',\n",
    "                label='max_iter',\n",
    "                description='Maximum number of iterations. The solver iterates until convergence (determined by tol) or this number of iterations.',\n",
    "                default=200,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='tol_mlp',\n",
    "                label='tol',\n",
    "                description='Tolerance for the optimization.',\n",
    "                default=1e-4,\n",
    "            ),\n",
    "            BoolField(\n",
    "                name='early_stopping_mlp',\n",
    "                label='early_stopping',\n",
    "                description='Whether to use early stopping to terminate training when validation score is not improving.',\n",
    "                default=False,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='validation_fraction_mlp',\n",
    "                label='validation_fraction',\n",
    "                description='The proportion of training data to set aside as validation set for early stopping.',\n",
    "                default=0.1,\n",
    "            ),\n",
    "        ],\n",
    "        'OneClassSVM': [\n",
    "            ChoiceField(\n",
    "                name='kernel_svm',\n",
    "                label='kernel',\n",
    "                description='Specifies the kernel type to be used in the algorithm.',\n",
    "                default=\"rbf\",\n",
    "                choices=[\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"],\n",
    "            ),\n",
    "            IntField(\n",
    "                name='degree_svm',\n",
    "                label='degree',\n",
    "                description='Degree of the polynomial kernel function (â€˜polyâ€™). Ignored by all other kernels.',\n",
    "                default=3,\n",
    "            ),\n",
    "            ChoiceField(\n",
    "                name='gamma_svm',\n",
    "                label='gamma',\n",
    "                description='Kernel coefficient for rbf, poly and sigmoid kernels.',\n",
    "                default=\"scale\",\n",
    "                choices=[\"scale\", \"auto\"],\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='coef0_svm',\n",
    "                label='coef0',\n",
    "                description='Independent term in kernel function. It is only significant in poly and sigmoid.',\n",
    "                default=0.0,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='tol_svm',\n",
    "                label='tol',\n",
    "                description='Tolerance for stopping criterion.',\n",
    "                default=1e-3,\n",
    "            ),\n",
    "            FloatField(\n",
    "                name='nu_svm',\n",
    "                label='nu',\n",
    "                description='An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. \\\n",
    "                             Should be in the interval (0, 1].',\n",
    "                default=0.5,\n",
    "            ),\n",
    "            BoolField(\n",
    "                name='shrinking_svm',\n",
    "                label='shrinking',\n",
    "                description='Whether to use the shrinking heuristic.',\n",
    "                default=True,\n",
    "            ),\n",
    "            IntField(\n",
    "                name='max_iter_svm',\n",
    "                label='max_iter',\n",
    "                description='Hard limit on iterations within solver, or -1 for no limit.',\n",
    "                default=-1,\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set calibrated = BoolField(\n",
    "    name='calibrated',\n",
    "    label='Calibrate algorithm predictions',\n",
    "    description='Calibrate the prediction probabilities, eliminating model-imparted bias.',\n",
    "    default=True,\n",
    "    section='SETTINGS',\n",
    ") %}\n",
    "{% set cv_algorithm = ChoiceField(\n",
    "    name='cv_algorithm',\n",
    "    label='Cross-Validation Algorithm',\n",
    "    description='Select a cross-validation method for training and evaluating the pipeline, and for making predictions. \\\n",
    "                 StratifiedGroupKFold or RepeatedStratifiedGroupKFold are recommended because they will maintain class ratios \\\n",
    "                 across train/validation splits (stratification of labels) and will group compounds by the first 14 characters of their \\\n",
    "                 InChI Keys to avoid compounds with multiple entries from appearing in both the train and validation sets. \\\n",
    "                 If repeated cross-validation is selected, 5 independent rounds of random splits will be conducted, yielding \\\n",
    "                 multiple predictions per compound, which can be evaluated for consistency.',\n",
    "    default='RepeatedStratifiedGroupKFold',\n",
    "    choices={\n",
    "        'KFold': 'sk.model_selection.KFold',\n",
    "        'GroupKFold': 'sk.model_selection.GroupKFold',\n",
    "        'RepeatedKFold': 'sk.model_selection.RepeatedKFold',\n",
    "        'StratifiedKFold': 'sk.model_selection.StratifiedKFold',\n",
    "        'StratifiedGroupKFold': 'StratifiedGroupKFold',\n",
    "        'RepeatedStratifiedKFold': 'sk.model_selection.RepeatedStratifiedKFold',\n",
    "        'RepeatedStratifiedGroupKFold': 'RepeatedStratifiedGroupKFold'\n",
    "    },\n",
    "    section='SETTINGS',\n",
    ") %}\n",
    "{% set cross_validation_n_folds = IntField(\n",
    "    name='cross_validation_n_folds',\n",
    "    label='Number of Cross-Validated Folds',\n",
    "    description='Cross-validation is employed as a strategy to train the model on data that the model has not seen before, more folds will ensure that the model is generalizing well.',\n",
    "    default=5,\n",
    "    min=2,\n",
    "    max=10,\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set hyper_param_search = ChoiceField(\n",
    "    name='hyper_param_search',\n",
    "    label='Hyper Parameter Search Type',\n",
    "    default='None',\n",
    "    description='Hyper parameter searching is used to automatically select the best parameters (using the primary metric as the criteria).',\n",
    "    choices={\n",
    "        'None': 'None',\n",
    "        'RandomizedSearchCV': 'sk.model_selection.RandomizedSearchCV',\n",
    "        'GridSearchCV': 'sk.model_selection.GridSearchCV',\n",
    "    },\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set primary_metric = ChoiceField(\n",
    "    name='primary_metric',\n",
    "    label='Primary Evaluation Metric',\n",
    "    default='roc_auc',\n",
    "    description='The primary evaluation metric is used for deciding how we assess the performance of our model. \\\n",
    "                 Area under the receiver operating characteristic curve (roc_auc) is recommended for most tasks.',\n",
    "    choices=[\n",
    "        'accuracy',\n",
    "        'adjusted_mutual_info_score',\n",
    "        'adjusted_rand_score',\n",
    "        'average_precision',\n",
    "        'balanced_accuracy',\n",
    "        'completeness_score',\n",
    "        'explained_variance',\n",
    "        'f1',\n",
    "        'f1_macro',\n",
    "        'f1_micro',\n",
    "        'f1_weighted',\n",
    "        'fowlkes_mallows_score',\n",
    "        'homogeneity_score',\n",
    "        'jaccard',\n",
    "        'jaccard_macro',\n",
    "        'jaccard_micro',\n",
    "        'jaccard_weighted',\n",
    "        'max_error',\n",
    "        'mutual_info_score',\n",
    "        'neg_brier_score',\n",
    "        'neg_log_loss',\n",
    "        'neg_mean_absolute_error',\n",
    "        'neg_mean_squared_error',\n",
    "        'neg_mean_squared_log_error',\n",
    "        'neg_median_absolute_error',\n",
    "        'neg_root_mean_squared_error',\n",
    "        'normalized_mutual_info_score',\n",
    "        'precision',\n",
    "        'precision_macro',\n",
    "        'precision_micro',\n",
    "        'precision_weighted',\n",
    "        'r2',\n",
    "        'recall',\n",
    "        'recall_macro',\n",
    "        'recall_micro',\n",
    "        'recall_weighted',\n",
    "        'roc_auc',\n",
    "        'roc_auc_ovo',\n",
    "        'roc_auc_ovo_weighted',\n",
    "        'roc_auc_ovr',\n",
    "        'roc_auc_ovr_weighted',\n",
    "        'v_measure_score'\n",
    "    ],\n",
    "    section='SETTINGS'\n",
    ") %}\n",
    "{% set evaluation_metrics = MultiChoiceField(\n",
    "    name='evaluation_metrics',\n",
    "    label='Evaluation Metrics',\n",
    "    default=[],\n",
    "    description='Additional evaluation metrics can be specified, these metrics will also be reported for all models trained.',\n",
    "    value=[],\n",
    "    choices=[\n",
    "        'accuracy',\n",
    "        'adjusted_mutual_info_score',\n",
    "        'adjusted_rand_score',\n",
    "        'average_precision',\n",
    "        'balanced_accuracy',\n",
    "        'completeness_score',\n",
    "        'explained_variance',\n",
    "        'f1',\n",
    "        'f1_macro',\n",
    "        'f1_micro',\n",
    "        'f1_weighted',\n",
    "        'fowlkes_mallows_score',\n",
    "        'homogeneity_score',\n",
    "        'jaccard',\n",
    "        'jaccard_macro',\n",
    "        'jaccard_micro',\n",
    "        'jaccard_weighted',\n",
    "        'max_error',\n",
    "        'mutual_info_score',\n",
    "        'neg_brier_score',\n",
    "        'neg_log_loss',\n",
    "        'neg_mean_absolute_error',\n",
    "        'neg_mean_squared_error',\n",
    "        'neg_mean_squared_log_error',\n",
    "        'neg_median_absolute_error',\n",
    "        'neg_root_mean_squared_error',\n",
    "        'normalized_mutual_info_score',\n",
    "        'precision',\n",
    "        'precision_macro',\n",
    "        'precision_micro',\n",
    "        'precision_weighted',\n",
    "        'r2',\n",
    "        'recall',\n",
    "        'recall_macro',\n",
    "        'recall_micro',\n",
    "        'recall_weighted',\n",
    "        'roc_auc',\n",
    "        'roc_auc_ovo',\n",
    "        'roc_auc_ovo_weighted',\n",
    "        'roc_auc_ovr',\n",
    "        'roc_auc_ovr_weighted',\n",
    "        'v_measure_score'\n",
    "    ],\n",
    "    section='SETTINGS',\n",
    ") %}\n",
    "{% set all_metrics = [primary_metric.value] + evaluation_metrics.value %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_hide\n",
    "\n",
    "{% set algorithm_code = {\n",
    "    'GradientBoostingClassifier': 'sk.ensemble.GradientBoostingClassifier(loss=\"{}\", learning_rate={}, n_estimators={}, subsample={}, criterion=\"{}\", tol={})',\n",
    "    'RandomForestClassifier': 'sk.ensemble.RandomForestClassifier(n_estimators={}, criterion=\"{}\", min_samples_split={}, min_samples_leaf={}, max_features={}, min_impurity_decrease={}, n_jobs=-1, class_weight={}, ccp_alpha={})',\n",
    "    'AdaBoostClassifier': 'sk.ensemble.AdaBoostClassifier(sk.tree.DecisionTreeClassifier(max_depth={}), n_estimators={}, learning_rate={}, algorithm=\"{}\")',\n",
    "    'ExtraTreesClassifier': 'sk.ensemble.ExtraTreesClassifier(n_estimators={}, criterion=\"{}\", min_samples_split={}, min_samples_leaf={}, max_features={}, min_impurity_decrease={}, n_jobs=-1, class_weight={}, ccp_alpha={})',\n",
    "    'DecisionTreeClassifier': 'sk.tree.DecisionTreeClassifier(criterion=\"{}\", splitter=\"{}\", min_samples_split={}, min_samples_leaf={}, max_features={}, min_impurity_decrease={}, class_weight={}, ccp_alpha={})',\n",
    "    'KNeighborsClassifier': 'sk.neighbors.KNeighborsClassifier(n_neighbors={}, weights=\"{}\", algorithm=\"{}\", leaf_size={}, p={}, metric=\"{}\", n_jobs=-1)',\n",
    "    'RadiusNeighborsClassifier': 'sk.neighbors.RadiusNeighborsClassifier(radius={}, weights=\"{}\", algorithm=\"{}\", leaf_size={}, p={}, metric=\"{}\", outlier_label=\"most_frequent\", n_jobs=-1)',\n",
    "    'MLPClassifier': 'sk.neural_network.MLPClassifier(hidden_layer_sizes={}, activation=\"{}\", solver=\"{}\", alpha={}, learning_rate=\"{}\", learning_rate_init={}, power_t={}, max_iter={}, tol={}, early_stopping={}, validation_fraction={})',\n",
    "    'OneClassSVM': 'sk.svm.OneClassSVM(kernel=\"{}\", degree={}, gamma=\"{}\", coef0={}, tol={}, nu={}, shrinking={}, max_iter={})',\n",
    "} %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "We apply a {% if hyper_param_search.value != 'None' %}{{ hyper_param_search.raw_value }} search for the hyper parameters\n",
    "of a {% endif %}sklearn pipeline with a dimensionality reduction step of {{ dimensionality_reduction.raw_value }}\n",
    "{% if feature_selection.value != 'None' %}and a feature selection step of {{ feature_selection.raw_value }}\n",
    "{% endif %} and a{% if calibrated.value %} calibrated{%endif %} {{ algorithm.raw_value }} classifier\n",
    "using {{ cross_validation_n_folds.value }}-fold {{ cv_algorithm.raw_value }} cross-validation,\n",
    "optimizing {{ primary_metric.value }}{% if evaluation_metrics.value %} and computing {{ ', '.join(evaluation_metrics.value) }}{% endif %}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a long time as we are evaluating n_iter different models n_splits different times each computing all the metrics on `product(X.shape)` data points--not to mention the size of each model dictated by the range of parameters specified in the params dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "{% if algorithm.raw_value == 'GradientBoostingClassifier' %}\n",
    "## Early stopping function\n",
    "def early_stopping(n_rounds, tol=0.001):\n",
    "    def early_stopping_func(i, self, local):\n",
    "        rounds = getattr(self, '__rounds', 0)\n",
    "        last = getattr(self, '__last', None)\n",
    "        current = self.train_score_[i]\n",
    "        if last and current and abs(current - last) < tol:\n",
    "            rounds += 1\n",
    "            if rounds > n_rounds:\n",
    "                return True\n",
    "        else:\n",
    "            rounds = 0\n",
    "        setattr(self, '__last', current)\n",
    "        setattr(self, '__rounds', rounds)\n",
    "        return False\n",
    "    return early_stopping_func\n",
    "{% endif %}\n",
    "\n",
    "{#\n",
    "param_grid = {\n",
    "    'reduce_dim__n_components': randint(2, 1024),\n",
    "{% if algorithm.raw_value == 'GradientBoostingClassifier' %}\n",
    "    'clf__loss': ['deviance', 'exponential'],\n",
    "    'clf__learning_rate': randfloat(0.001, 1.),\n",
    "    'clf__subsample': randfloat(0.01, 1.),\n",
    "{% elif algorithm.raw_value == 'RandomForestClassifier' %}\n",
    "    'clf__oob_score': [True],\n",
    "    'clf__criterion': ['gini', 'entropy'],\n",
    "{% endif %}\n",
    "    'clf__n_estimators': randint(10, 200),\n",
    "    'clf__max_depth': randint(20, 50),\n",
    "    'clf__max_features': ['sqrt', 'log2', None],\n",
    "    'clf__min_impurity_decrease': randfloat(0., 0.2),\n",
    "    'clf__min_weight_fraction_leaf': randfloat(0., 0.5),\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "{% if algorithm.raw_value == 'GradientBoostingClassifier' %}\n",
    "    'clf__monitor': early_stopping(5),\n",
    "{% endif %}\n",
    "}\n",
    "#}\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "n_repeats=5\n",
    "{% endif %}\n",
    "cv = {{ cv_algorithm }}(\n",
    "    n_splits={{ cross_validation_n_folds }},\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    n_repeats=n_repeats,\n",
    "    {% else %}\n",
    "    shuffle=True,\n",
    "    {% endif %}\n",
    "    random_state=rng,\n",
    ")\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['GroupKFold', 'StratifiedGroupKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "groups=[key[:14] for key in X.index]    # Group compounds by atom connectivity\n",
    "{% endif %}\n",
    "\n",
    "# Scoring parameters\n",
    "primary_metric = '{{ primary_metric }}'\n",
    "evaluation_metrics = {{ evaluation_metrics }}\n",
    "scoring_params = {k: metrics.get_scorer(k)\n",
    "                  for k in [primary_metric, *evaluation_metrics]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "{% if hyper_param_search.value == 'None' %}\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "# Store performance on each split for computing ROC and PRC curves\n",
    "fprs = []\n",
    "tprs = []\n",
    "precs = []\n",
    "recs = []\n",
    "\n",
    "# Store cross-validation test predictions and folds\n",
    "y_proba_cv = [[] for _ in range(len(y))]\n",
    "folds_cv = [[] for _ in range(len(y))]\n",
    "\n",
    "# Store models\n",
    "models = []\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['GroupKFold', 'StratifiedGroupKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "groups=[key[:14] for key in X.index]    # Group compounds by atom connectivity\n",
    "for fold, (train, test) in tqdm(enumerate(cv.split(X.values, y, groups=groups))):\n",
    "{% else %}\n",
    "for fold, (train, test) in tqdm(enumerate(cv.split(X.values, y))):\n",
    "{% endif %}\n",
    "    model =\n",
    "    {%- if hyper_param_search.value != 'None' %} {{ hyper_param_search }}({% endif -%}\n",
    "            sk.pipeline.Pipeline([\n",
    "                {%- if dimensionality_reduction.value != 'None' %}\n",
    "                ('reduce_dim', {{ dimensionality_reduction }}),\n",
    "                {% endif %}\n",
    "                {%- if feature_selection.value != 'None' %}\n",
    "                ('feature_selection', {{ feature_selection }}),\n",
    "                {% endif %}\n",
    "                ('clf', {% if algorithm.raw_value == 'MLPClassifier' %}{{ algorithm_code.get(algorithm.raw_value).format(algorithm.value[0].value|str_to_tuple, *algorithm.value[1:]) }}\n",
    "                        {% elif algorithm.raw_value in ['DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier'] %}{{ algorithm_code.get(algorithm.raw_value).format(algorithm.value[0].value, algorithm.value[1].value, algorithm.value[2].value|int_or_float, algorithm.value[3].value|int_or_float, *algorithm.value[4:]) }}\n",
    "                        {% else %}{{ algorithm_code.get(algorithm.raw_value).format(*algorithm.value) }}{% endif %}\n",
    "                ),\n",
    "            ])\n",
    "    {%- if hyper_param_search.value != 'None' %}){% endif %}\n",
    "    model.fit(X.values[train], y[train])\n",
    "    \n",
    "    {% if calibrated.value %}\n",
    "    calibrator = sk.calibration.CalibratedClassifierCV(model, cv='prefit')\n",
    "    calibrator.fit(X.values[test], y[test])\n",
    "    model = calibrator\n",
    "    {% endif %}\n",
    "    \n",
    "    {% for metric in all_metrics %}\n",
    "    df_results.loc[fold, '{{ metric }}'] = scoring_params['{{ metric }}'](model, X.values[test], y[test])\n",
    "    {% endfor %}\n",
    "    \n",
    "    y_proba = model.predict_proba(X.values[test]) # Probability prediction will be True\n",
    "    for i in range(len(test)):\n",
    "        y_proba_cv[test[i]].append(y_proba[i, 1])\n",
    "        folds_cv[test[i]].append(fold % {{ cross_validation_n_folds }})\n",
    "    model_fpr, model_tpr, _ = metrics.roc_curve(y[test], y_proba[:, 1])\n",
    "    model_prec, model_rec, _ = metrics.precision_recall_curve(y[test], y_proba[:, 1])\n",
    "    fprs.append(model_fpr)\n",
    "    tprs.append(model_tpr)\n",
    "    precs.append(model_prec)\n",
    "    recs.append(model_rec)\n",
    "    models.append(model)\n",
    "\n",
    "assert not(any(len(probs) == 0 for probs in y_proba_cv)), 'All probabilities should have been calculated'\n",
    "\n",
    "display(df_results.agg(['mean', 'std']))\n",
    "{% else %}\n",
    "model.fit(X.values, y)\n",
    "df_results = model.cv_results_\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization shows the cross-validated performance of the model. Low fold variance and high AUC is desired in a well-generalized model.\n",
    "* ROC curve: [roc.svg](./roc.svg)\n",
    "* Precision-recall curve: [prc.svg](./prc.svg)\n",
    "* Confusion matrix: [confusion_matrix.svg](./confusion_matrix.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "tprs_interp = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for fold, (fpr, tpr) in enumerate(zip(fprs, tprs)):\n",
    "    tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
    "    tpr_interp[0] = 0.\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    tprs_interp.append(tpr_interp)\n",
    "    aucs.append(roc_auc)\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    ax.plot(fpr, tpr, alpha=0.4)\n",
    "    {% else %}\n",
    "    ax.plot(fpr, tpr, alpha=0.4, label='ROC Fold %d (AUC=%0.3f)' % (fold, roc_auc))\n",
    "    {% endif %}\n",
    "\n",
    "mean_tpr = np.mean(tprs_interp, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = sk.metrics.auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs_interp, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)\n",
    "\n",
    "ax.plot([0,1],[0,1],'--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.legend()\n",
    "plt.savefig('roc.svg')\n",
    "plt.show()\n",
    "\n",
    "z = (mean_auc - 0.5)/std_auc\n",
    "cl = sp.stats.norm.cdf(z) * 100\n",
    "ci = sp.stats.norm.interval(0.95, loc=mean_auc, scale=std_auc)\n",
    "print('Confidence interval (95%)', ci)\n",
    "print(\"We are %0.3f %% confident the model's results are not just chance.\" % (cl))\n",
    "if cl > 95:\n",
    "    print('This is statistically significant. These results can be trusted.')\n",
    "else:\n",
    "    print('This is not statistically significant. These results should not be trusted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "precs_interp = []\n",
    "prc_aucs = []\n",
    "mean_rec = np.linspace(0, 1, 100)\n",
    "\n",
    "for fold, (rec, prec) in enumerate(zip(recs, precs)):\n",
    "    prec_interp = np.interp(mean_rec, rec[::-1], prec[::-1])\n",
    "    prc_auc = metrics.auc(rec, prec)\n",
    "    precs_interp.append(prec_interp)\n",
    "    prc_aucs.append(prc_auc)\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    ax.plot(rec, prec, alpha=0.4)\n",
    "    {% else %}\n",
    "    ax.plot(rec, prec, alpha=0.4, label='PRC Fold %d (AUC=%0.3f)' % (fold, prc_auc))\n",
    "    {% endif %}\n",
    "    \n",
    "mean_prec = np.mean(precs_interp, axis=0)\n",
    "mean_auc = sk.metrics.auc(mean_rec, mean_prec)\n",
    "std_auc = np.std(prc_aucs)\n",
    "ax.plot(mean_rec, mean_prec, color='b',\n",
    "         label=r'Mean PRC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "         lw=2, alpha=.8)\n",
    "\n",
    "std_prec = np.std(precs_interp, axis=0)\n",
    "precs_upper = np.minimum(mean_prec + std_prec, 1)\n",
    "precs_lower = np.maximum(mean_prec - std_prec, 0)\n",
    "plt.fill_between(mean_rec, precs_lower, precs_upper, color='grey', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.legend()\n",
    "plt.savefig('prc.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Confusion Matrix (Cross-Validation)')\n",
    "sns.heatmap(\n",
    "    metrics.confusion_matrix(y, np.array([np.mean(probs) for probs in y_proba_cv]) > 0.5),\n",
    "    annot=True,\n",
    "    cmap=plt.cm.Blues,\n",
    "    fmt='g'\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine drug predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the binary classification model, we can rank the drug hits by their predicted score. The model can also be used to identify additional drugs that are likely to share properties with the hits. The results table is available at [drug_cv_predictions.csv](./drug_cv_predictions.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot distribution of predictions for positive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "# Calculate mean and deviation of predictions\n",
    "y_probas = np.array([np.mean(probs) for probs in y_proba_cv])\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "y_probas_std = np.array([np.std(probs) for probs in y_proba_cv])\n",
    "# Find minimum non-zero standard deviation to avoid dividing by zero when computing t-statistic\n",
    "min_y_probas_std = max(np.min(y_probas_std[y_probas_std != 0]), 1e-10)\n",
    "t_stats = (y_probas - np.mean(y_probas)) / (np.maximum(y_probas_std, min_y_probas_std)/np.sqrt(n_repeats))\n",
    "# Calculate p-value using one-sample t-test\n",
    "p_vals_t = 1-sp.stats.t(n_repeats-1).cdf(t_stats)\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "y_probas_means_5 = []\n",
    "y_probas_values = np.array(y_proba_cv).flatten()\n",
    "\n",
    "np.random.seed(rng)\n",
    "for i in tqdm(range(100000)):\n",
    "    y_probas_means_5.append(np.mean(np.random.choice(y_probas_values, n_repeats)))\n",
    "    \n",
    "y_probas_means_5 = np.array(sorted(y_probas_means_5))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "y_probas_ts_5 = []\n",
    "mean_y_probas = np.mean(y_probas)\n",
    "y_probas_values = np.array(y_proba_cv).flatten()\n",
    "\n",
    "np.random.seed(rng)\n",
    "for i in tqdm(range(100000)):\n",
    "    sample = np.random.choice(y_probas_values, n_repeats)\n",
    "    y_probas_ts_5.append((np.mean(sample) - mean_y_probas) / (np.maximum(np.std(sample), min_y_probas_std)/np.sqrt(n_repeats)))\n",
    "    \n",
    "y_probas_ts_5 = np.array(sorted(y_probas_ts_5))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "max_mean = np.max(y_probas_means_5)\n",
    "p_vals = np.array(list(tqdm((1 - np.argwhere(y_probas_means_5 >= min(pred, max_mean))[0][0] / len(y_probas_means_5)\n",
    "                             for pred in y_probas), total=len(y_probas))))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "max_t = np.max(y_probas_ts_5)\n",
    "p_vals_t_sim = np.array(list(tqdm((1 - np.argwhere(y_probas_ts_5 >= min(t, max_t))[0][0] / len(y_probas_ts_5)\n",
    "                                   for t in t_stats), total=len(t_stats))))\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "sns.kdeplot(y_probas[y == 0], shade=True, gridsize=2000, clip=[np.min(y_probas), np.percentile(y_probas, 99.9)], label='Not known NSAID')\n",
    "sns.kdeplot(y_probas[y == 1], shade=True, gridsize=2000, clip=[np.min(y_probas), np.percentile(y_probas, 99.9)], bw=0.005, label='Known NSAID')\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "sns.kdeplot(y_probas_means_5, shade=True, gridsize=2000, clip=[np.min(y_probas), np.percentile(y_probas, 99.9)], bw=0.01, label='Null distribution\\n(simulated)')\n",
    "{% endif %}\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.xlim([np.min(y_probas), np.percentile(y_probas, 99)])\n",
    "plt.legend()\n",
    "plt.savefig('mean-prediction-distribution-kde.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "sns.kdeplot(t_stats[y == 0], shade=True, gridsize=1000, clip=(-20, 20), bw=0.1, label='Not known NSAID')\n",
    "sns.kdeplot(t_stats[y == 1], shade=True, gridsize=1000, clip=(-20, 20), bw=0.05, label='Known NSAID')\n",
    "sns.kdeplot(y_probas_ts_5, shade=True, gridsize=1000, clip=(-20, 20), bw=0.1, label='Null distribution\\n(simulated)')\n",
    "plt.xlabel('t-statistic')\n",
    "plt.xlim([-20,20])\n",
    "plt.legend()\n",
    "plt.savefig('t-statistic-distribution-kde.svg')\n",
    "plt.show()\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay predictions on visualization of input space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "# Add attributes for plotting to Dataframe\n",
    "X_reduced_df['Predicted Probability'] = y_probas\n",
    "X_reduced_df['log10(pred)'] = np.log10(y_probas + 1e-10)\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "X_reduced_df['p-value'] = p_vals_t_sim\n",
    "X_reduced_df['log10(p-value)'] = np.log10(X_reduced_df['p-value'])\n",
    "X_reduced_df['Standard Deviation'] = y_probas_std\n",
    "{% endif %}\n",
    "X_reduced_df['Cross-validation fold'] = folds_cv\n",
    "{% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "X_reduced_df['marker size'] = 2*np.minimum(2-np.log10(X_reduced_df['p-value']), 5)\n",
    "{% else %}\n",
    "max_p, min_p = np.min(-X_reduced_df['log10(pred)']), np.max(-X_reduced_df['log10(pred)'])\n",
    "X_reduced_df['marker size'] = (-X_reduced_df['log10(pred)'] - min_p) / (max_p - min_p) * 6 + 4\n",
    "{% endif %}\n",
    "X_reduced_df['text'] = ['<br>'.join(['Drug Name: ' + str(name),\n",
    "                                     'InChI Key: ' + str(inchi),\n",
    "                                     'Predicted Probability: {:.1e}'.format(p),\n",
    "                                     {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "                                     'Standard Deviation: {:.1e}'.format(s),\n",
    "                                     'p-value: {:.1e}'.format(p_val),\n",
    "                                     {% endif %}\n",
    "                                     'Label: ' + str(label),\n",
    "                                     'Cross-validation fold: ' + str(fold)])\n",
    "                  for name, inchi, p, {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}s, p_val, {% endif %}label, fold in zip(X_reduced_df['Drug Name'],\n",
    "                                                         X_reduced_df['InChI Key'],\n",
    "                                                         X_reduced_df['Predicted Probability'],\n",
    "                                                         {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "                                                         X_reduced_df['Standard Deviation'],\n",
    "                                                         X_reduced_df['p-value'],\n",
    "                                                         {% endif %}\n",
    "                                                         X_reduced_df['Label'],\n",
    "                                                         X_reduced_df['Cross-validation fold'])]\n",
    "X_reduced_df.to_csv('X_reduced_df.csv')\n",
    "\n",
    "# Helper function for formatting Plotly colorbar\n",
    "def colorbar_param(values_log10, **kwargs):\n",
    "    min_val = np.floor(np.min(values_log10))\n",
    "    max_val = np.ceil(np.max(values_log10))\n",
    "    \n",
    "    ticks1 = 10**np.arange(min_val, max_val+1)\n",
    "    ticks2 = 3*10**np.arange(min_val, max_val)\n",
    "    \n",
    "    ticktext = sorted(np.concatenate([ticks1, ticks2]))\n",
    "    tickvals = list(np.log10(ticktext))\n",
    "    ticktext = ['{:.0e}'.format(text) for text in ticktext]\n",
    "    \n",
    "    return dict(ticktext=ticktext, tickvals=tickvals, **kwargs)\n",
    "\n",
    "fig = go.Figure()\n",
    "for label in sorted(set(X_reduced_df['Label'])):\n",
    "    X_plot = X_reduced_df[X_reduced_df['Label'] == label].sort_values(['Predicted Probability'])\n",
    "    fig.add_trace(go.Scatter(mode='markers',\n",
    "                               x=X_plot['Component 1'], y=X_plot['Component 2'],\n",
    "                               text=X_plot['text'],\n",
    "                               name=label,\n",
    "                               marker=dict(\n",
    "                                   color=X_plot['log10(pred)'],\n",
    "                                   cmin=np.percentile(X_reduced_df['log10(pred)'], 50),\n",
    "                                   cmax=np.max(X_reduced_df['log10(pred)']),\n",
    "                                   size=X_plot['marker size'],\n",
    "                                   colorbar=colorbar_param(X_plot['log10(pred)'], title='Predicted Probability'),\n",
    "                                   symbol=X_plot['marker symbol'],\n",
    "                                   line_width=1,\n",
    "                                   colorscale='plasma'\n",
    "                               )))\n",
    "fig.update_layout(height=600, width=800,\n",
    "                  xaxis_title='Component 1',\n",
    "                  yaxis_title='Component 2',\n",
    "                  title_text='Predicted Probabilities ({{ visualization_reduction.raw_value }})',\n",
    "                  legend_title_text='Target Label',\n",
    "                  legend=dict(\n",
    "                      yanchor=\"top\",\n",
    "                      y=0.98,\n",
    "                      xanchor=\"left\",\n",
    "                      x=0.02\n",
    "                  ),\n",
    "                  template='simple_white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables of top-predicted compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "# Obtain prediction results\n",
    "results = pd.DataFrame(np.array([\n",
    "    querysepl1000fwd.get_drug_names(X.index),\n",
    "    Drugmonizome.get_drug_names(X.index),\n",
    "    folds_cv,\n",
    "    y,\n",
    "    y_probas,\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    y_probas_std,\n",
    "    t_stats,\n",
    "    p_vals,\n",
    "    p_vals_t,\n",
    "    p_vals_t_sim,\n",
    "    {% endif %}\n",
    "]).T, columns=[\n",
    "    'Name (L1000FWD)',\n",
    "    'Name (Drugmonizome)',\n",
    "    'Cross-validation fold',\n",
    "    'Known',\n",
    "    'Prediction Probability',\n",
    "    {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "    'Prediction Probability Std. Dev.',\n",
    "    't statistic',\n",
    "    'p value (simulated mean distribution)',\n",
    "    'p value (one sample t test)',\n",
    "    'p value (simulated t distribution)',\n",
    "    {% endif %}\n",
    "], index=X.index).astype({'Known': 'bool',\n",
    "                          'Prediction Probability': 'float64',\n",
    "                          {% if cv_algorithm.raw_value in ['RepeatedStratifiedKFold', 'RepeatedStratifiedGroupKFold'] %}\n",
    "                          'Prediction Probability Std. Dev.': 'float64',\n",
    "                          't statistic': 'float64',\n",
    "                          'p value (simulated mean distribution)': 'float64',\n",
    "                          'p value (one sample t test)': 'float64',\n",
    "                          'p value (simulated t distribution)': 'float64',{% endif %}})\n",
    "\n",
    "results.to_csv('drug_cv_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Rank known drug hits\n",
    "df = results[((results['Known'] == 1))]\n",
    "show(df.reset_index(), maxBytes=0, order=[[ 5, \"desc\" ]], columnDefs=[{'width': '110px', 'targets': [0, 1]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict additional drugs\n",
    "df = results[results['Known'] == 0]\n",
    "show(df.reset_index(), maxBytes=0, order=[[ 5, \"desc\" ]], columnDefs=[{'width': '110px', 'targets': [0, 1]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative contribution of each input feature to the final model predictions can be estimated for recursive feature selection and for a variety of tree-based models. Note that this analysis is not available if a dimensionality reduction algorithm is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' and dimensionality_reduction.raw_value == 'None' %}\n",
    "When recursive feature selection is performed, the features are ranked by the stage at which they were removed.\n",
    "Selected (i.e. estimated best) features are have importance 1. The ranks are averaged across cross-validation\n",
    "splits to produce an average importance score. The full feature importance table is available at\n",
    "[feature_importance.csv](./feature_importance.csv).\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' and dimensionality_reduction.raw_value == 'None' %}\n",
    "all_rankings = []\n",
    "{% endif %}\n",
    "{% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier'] %}\n",
    "all_feature_importances = []\n",
    "{% endif %}\n",
    "for model in models:\n",
    "    {% if calibrated.value %}\n",
    "    for calibrated_clf in model.calibrated_classifiers_:\n",
    "        pipeline = calibrated_clf.base_estimator\n",
    "    {% else %}\n",
    "        pipeline = model\n",
    "    {% endif %}\n",
    "        \n",
    "        {% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' %}\n",
    "        ranking = pipeline['feature_selection'].ranking_\n",
    "        all_rankings.append(ranking)\n",
    "        {% endif %}\n",
    "        \n",
    "        {% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier'] %}\n",
    "        {% if feature_selection.raw_value != 'None' %}\n",
    "        feature_importances = np.zeros(pipeline['feature_selection'].get_support().shape)\n",
    "        feature_importances[pipeline['feature_selection'].get_support()] = pipeline['clf'].feature_importances_\n",
    "        {% else %}\n",
    "        feature_importances = pipeline['clf'].feature_importances_\n",
    "        {% endif %}\n",
    "        all_feature_importances.append(feature_importances)\n",
    "        {% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if dimensionality_reduction.raw_value == 'None' %}\n",
    "df_feat_imp = pd.DataFrame({'Feature': X.columns,\n",
    "                            'Dataset': reduce(lambda a,b: a+b, ([dataset]*size for dataset, size in dataset_sizes)),\n",
    "                            {% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' %}\n",
    "                            'Ranking Mean': np.mean(all_rankings, axis=0),\n",
    "                            'Ranking Std. Dev.': np.std(all_rankings, axis=0),\n",
    "                            {% endif %}\n",
    "                            'Importance Mean': np.mean(all_feature_importances, axis=0),\n",
    "                            'Importance Std. Dev.': np.std(all_feature_importances, axis=0)})\n",
    "df_feat_imp = df_feat_imp.set_index('Feature').sort_values('Importance Mean', ascending=False)\n",
    "show(df_feat_imp.reset_index(), maxBytes=0, order=[[ 2, \"desc\"]])\n",
    "df_feat_imp.to_csv('feature_importance.csv')\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter markdown\n",
    "\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' and dimensionality_reduction.raw_value == 'None' %}\n",
    "Plot the distribution of importance scores for features in each dataset ([feature_importance.svg](./feature_importance.svg)).\n",
    "Features with lower scores were retained for more rounds during recursive feature selection\n",
    "and have greater relative importance.\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if feature_selection.raw_value == 'RecursiveSelectionFromExtraTrees' and dimensionality_reduction.raw_value == 'None' %}\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "df_feat_imp = df_feat_imp.sort_values('Ranking Mean')\n",
    "for dataset in set(df_feat_imp.Dataset):\n",
    "    importance_scores = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Ranking Mean'].values\n",
    "    importance_scores_std = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Ranking Std. Dev.'].values\n",
    "    lower = importance_scores - importance_scores_std\n",
    "    upper = importance_scores + importance_scores_std\n",
    "    axs[0].plot(importance_scores, label=dataset)\n",
    "    axs[0].fill_between(np.arange(len(importance_scores)), lower, upper, alpha=.2)\n",
    "    axs[1].plot(np.linspace(0, 1, len(importance_scores)), importance_scores, label=dataset)\n",
    "    axs[1].fill_between(np.linspace(0, 1, len(importance_scores)), lower, upper, alpha=.2)\n",
    "for i in [0, 1]:\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title('Distribution of feature ranking from recursive feature elimination')\n",
    "    axs[i].set_ylabel('Average feature ranking\\n(lower ranking is more important)')\n",
    "axs[0].set_xlabel('Ranked features (absolute count)')\n",
    "axs[1].set_xlabel('Ranked features (relative count)')\n",
    "axs[0].set_xlim([0,512])\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_rfe.svg')\n",
    "plt.show()\n",
    "{% endif %}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%appyter code_exec\n",
    "\n",
    "{% if algorithm.raw_value in ['GradientBoostingClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'ExtraTreesClassifier', 'DecisionTreeClassifier']  and dimensionality_reduction.raw_value == 'None' %}\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "df_feat_imp = df_feat_imp.sort_values('Importance Mean', ascending=False)\n",
    "for dataset in set(df_feat_imp.Dataset):\n",
    "    importance_scores = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Importance Mean'].values\n",
    "    importance_scores_std = df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Importance Std. Dev.'].values\n",
    "    lower = importance_scores - importance_scores_std\n",
    "    upper = importance_scores + importance_scores_std\n",
    "    axs[0][0].plot(importance_scores, label=dataset)\n",
    "    axs[0][0].fill_between(np.arange(len(importance_scores)), lower, upper, alpha=.2)\n",
    "    axs[0][1].plot(np.linspace(0, 1, len(importance_scores)), importance_scores, label=dataset)\n",
    "    axs[0][1].fill_between(np.linspace(0, 1, len(importance_scores)), lower, upper, alpha=.2)\n",
    "    \n",
    "    importance_scores = np.cumsum(df_feat_imp.loc[df_feat_imp.Dataset == dataset]['Importance Mean'].values)\n",
    "    axs[1][0].plot(importance_scores, label=dataset)\n",
    "    axs[1][1].plot(np.linspace(0, 1, len(importance_scores)), importance_scores, label=dataset)\n",
    "for i in [0, 1]:\n",
    "    axs[0][i].legend()\n",
    "    axs[0][i].set_title('Distribution of feature scores from model')\n",
    "    axs[1][i].set_title('Cumulative distribution of feature scores from model')\n",
    "    axs[i][0].set_xlabel('Ranked features (absolute count)')\n",
    "    axs[i][1].set_xlabel('Ranked features (relative count)')\n",
    "    axs[0][i].set_ylabel('Average feature importance\\n(higher score is more important)')\n",
    "    axs[1][i].set_ylabel('Cumulative sum of feature importance')\n",
    "    axs[i][0].set_xlim([0,512])\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.svg')\n",
    "plt.show()\n",
    "{% endif %}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdkit-dml-env",
   "language": "python",
   "name": "rdkit-dml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
