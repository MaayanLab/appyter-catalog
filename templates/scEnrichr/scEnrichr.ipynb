{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%jupyter_template init\n",
    "import os, sys; sys.path.insert(0, os.path.realpath('..'))\n",
    "from jupyter_template import magic\n",
    "magic.init(lambda _=globals: _())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Cell Enrichment\n",
    "\n",
    "We prepare single cell data, computing clusters, differential expression, and enrichment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy.sparse as sp_sparse\n",
    "import seaborn as sns\n",
    "from geode import chdir\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display\n",
    "from umap import UMAP\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move some of these functions to maayanlab_bioinformatics package?\n",
    "\n",
    "def merge(left, *rights, **kwargs):\n",
    "  ''' Helper function for many trivial (index based) joins\n",
    "  '''\n",
    "  merged = left\n",
    "  for right in rights:\n",
    "    merged = pd.merge(left=merged, left_index=True, right=right, right_index=True, **kwargs)\n",
    "  return merged\n",
    "\n",
    "\n",
    "def log2Normalize(mat):\n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  if isinstance(mat, pd.DataFrame):\n",
    "    return pd.DataFrame(np.log2(mat+1), index=mat.index, columns=mat.columns)\n",
    "  else:\n",
    "    return np.log2(mat+1)\n",
    "  \n",
    "def zscoreNormalize(mat):\n",
    "  import pandas as pd\n",
    "  from scipy.stats import zscore\n",
    "  if isinstance(mat, pd.DataFrame):\n",
    "    return pd.DataFrame(zscore(mat, axis=0), index=mat.index, columns=mat.columns)\n",
    "  else:\n",
    "    return zscore(mat, axis=0)\n",
    "\n",
    "def quantileNormalize_np(mat):\n",
    "  import numpy as np\n",
    "  # sort vector in np (reuse in np)\n",
    "  sorted_vec = np.sort(mat, axis=0)\n",
    "  # rank vector in np (no dict necessary)\n",
    "  rank = sorted_vec.mean(axis=1)\n",
    "  # construct quantile normalized matrix\n",
    "  return np.array([\n",
    "    [\n",
    "      rank[i]\n",
    "      for i in np.searchsorted(sorted_vec[:, c], mat[:, c])\n",
    "    ] for c in range(mat.shape[1])\n",
    "  ]).T\n",
    "\n",
    "def quantileNormalize_pd(df):\n",
    "  import pandas as pd\n",
    "  return pd.DataFrame(\n",
    "    quantileNormalize_np(df.values),\n",
    "    index=df.index,\n",
    "    columns=df.columns,\n",
    "  )\n",
    "\n",
    "def quantileNormalize(arg):\n",
    "  ''' Perform quantile normalization on the values of a matrix of dataframe\n",
    "  '''\n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  if isinstance(arg, pd.DataFrame):\n",
    "    return quantileNormalize_pd(arg)\n",
    "  elif isinstance(arg, np.ndarray):\n",
    "    return quantileNormalize_np(arg)\n",
    "  else:\n",
    "    raise Exception('quantileNormalize: Unrecognized argument type (`{}`)'.format(type(arg)))\n",
    "\n",
    "def enrichr_link_from_genes(genes, description='', enrichr_link='https://amp.pharm.mssm.edu/Enrichr'):\n",
    "  ''' Functional access to Enrichr API\n",
    "  '''\n",
    "  import time, requests\n",
    "  time.sleep(1)\n",
    "  resp = requests.post(enrichr_link + '/addList', files={\n",
    "    'list': (None, '\\n'.join(genes)),\n",
    "    'description': (None, description),\n",
    "  })\n",
    "  if resp.status_code != 200:\n",
    "    raise Exception('Enrichr failed with status {}: {}'.format(\n",
    "      resp.status_code,\n",
    "      resp.text,\n",
    "    ))\n",
    "  # wait a tinybit before returning link (backoff)\n",
    "  time.sleep(1)\n",
    "  result = resp.json()\n",
    "  return dict(result, link=enrichr_link + '/enrich?dataset=' + resp.json()['shortId'])\n",
    "\n",
    "def enrichr_get_top_results(userListId, bg, enrichr_link='https://amp.pharm.mssm.edu/Enrichr'):\n",
    "  import time, requests\n",
    "  time.sleep(1)\n",
    "  resp = requests.get(enrichr_link + '/enrich?userListId={}&backgroundType={}'.format(userListId, bg))\n",
    "  if resp.status_code != 200:\n",
    "    raise Exception('Enrichr failed with status {}: {}'.format(\n",
    "      resp.status_code,\n",
    "      resp.text,\n",
    "    ))\n",
    "  time.sleep(1)\n",
    "  return pd.DataFrame(resp.json()[bg], columns=['rank', 'term', 'pvalue', 'zscore', 'combinedscore', 'overlapping_genes', 'adjusted_pvalue', '', ''])\n",
    "\n",
    "def diffExpression(controls, cases):\n",
    "  assert controls.shape[0] == cases.shape[0], \"Must have the same number of genes\"\n",
    "  genes = np.array(controls.index)\n",
    "  n_genes = genes.shape[0]\n",
    "  # Compute characteristic direction\n",
    "  results = pd.DataFrame(\n",
    "    data=chdir(\n",
    "      pd.concat([controls.loc[genes, :], cases.loc[genes, :]], axis=1).values,\n",
    "      np.array(\n",
    "        [1]*controls.shape[1] + [2]*cases.shape[1]\n",
    "      ),\n",
    "      # genes\n",
    "      genes,\n",
    "      # gamma\n",
    "      0.5,\n",
    "      sort=False,\n",
    "      calculate_sig=False,\n",
    "    ),\n",
    "    columns=['CD-coefficient', 'index'],\n",
    "  )\n",
    "  results.index = results['index']\n",
    "  return results.drop('index', axis=1)\n",
    "\n",
    "def upDownFromDiffExpression(expr, top_genes):\n",
    "  filtered_expr = expr.loc[expr.abs().sort_values('CD-coefficient', ascending=False)[:top_genes].index]\n",
    "  return {\n",
    "    'up': list(filtered_expr[filtered_expr > 0].dropna().index),\n",
    "    'down': list(filtered_expr[filtered_expr < 0].dropna().index),\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Configure analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jupyter_template code_exec\n",
    "\n",
    "# the archive containing the single cell data\n",
    "#  note that the expectation is a directory of the form\n",
    "#  /{archive}.zip/xxx_xxxx_20190101_xxx_xx_ReplicateX/filtered_feature_bc_matrix\n",
    "#  /{archive}.zip/...\n",
    "\n",
    "# TODO: can we use the format that comes out of the actual machine?\n",
    "# TODO: are there other formats?\n",
    "{% do SectionField(\n",
    "    name='INPUT',\n",
    "    label='Upload your single-cell data',\n",
    "    description='As a .zip archive containing your single-cell data in a format ready for Seurat',\n",
    ") %}\n",
    "\n",
    "archive = {{ FileField(\n",
    "    name='zip_file',\n",
    "    label='zip file containing CellRanger output',\n",
    "    default='scEnrichr.py',\n",
    "    section='INPUT',\n",
    ") }}\n",
    "\n",
    "{% do SectionField(\n",
    "    name='CONFIG',\n",
    "    label='Configuration',\n",
    "    description='Configure various parameters for the analysis',\n",
    ") %}\n",
    "\n",
    "# The number of 'top' genes to use for differential expression\n",
    "top_n_genes = {{ IntField(\n",
    "    name='top_n_genes',\n",
    "    label='Number of Genes',\n",
    "    description='The number of \\'top\\' genes to use for differential expression',\n",
    "    default=250,\n",
    "    min=100,\n",
    "    max=1000,\n",
    "    section='CONFIG',\n",
    ") }}\n",
    "\n",
    "# The number of 'top' results to keep from enrichment analysis\n",
    "top_n_results = {{ IntField(\n",
    "    name='top_n_results',\n",
    "    label='Number of Top Enrichment Results',\n",
    "    description='The number of \\'top\\' results to keep from enrichment analysis',\n",
    "    default=5,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    section='CONFIG',\n",
    ") }}\n",
    "\n",
    "# TODO: add enrichr libraries as categories as fields\n",
    "useful_libs = OrderedDict([\n",
    "  ('cell_type', ['Human_Gene_Atlas', 'Mouse_Gene_Atlas', 'ARCHS4_Tissues']),\n",
    "  ('pathways', ['WikiPathways_2019_Mouse', 'WikiPathways_2019_Human']),\n",
    "  ('transcription', ['ARCHS4_TFs_Coexp', 'ENCODE_and_ChEA_Consensus_TFs_from_ChIP-X']),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Fetch and extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%jupyter_template code_exec\n",
    "\n",
    "directory = os.path.basename(archive)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    if not os.path.exists(archive):\n",
    "        import urllib.request\n",
    "        urllib.request.urlretrieve('https://jupy-template.cloud/{{ _session }}/' + archive, archive)\n",
    "\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(file, 'r') as zipObj:\n",
    "        zipObj.extractall(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_suerat_files(base_dir):\n",
    "  ''' Helper function for loading in files made for seurat\n",
    "  '''\n",
    "  import pandas as pd\n",
    "  import scipy.sparse as sp_sparse\n",
    "  df_barcodes = pd.read_csv(\n",
    "    os.path.join(base_dir, 'barcodes.tsv.gz'),\n",
    "    index_col=0,\n",
    "    header=None,\n",
    "    sep='\\t',\n",
    "  )\n",
    "  df_features = pd.read_csv(\n",
    "    os.path.join(base_dir, 'features.tsv.gz'),\n",
    "    header=None,\n",
    "    names=['symbol', 'type'],\n",
    "    index_col=0,\n",
    "    sep='\\t',\n",
    "  )\n",
    "  matrix = pd.read_csv(\n",
    "    os.path.join(base_dir, 'matrix.mtx.gz'),\n",
    "    header=None,\n",
    "    names=['indices', 'indptr', 'data'],\n",
    "    skiprows=2,\n",
    "    sep=' ',\n",
    "  )\n",
    "  csc_matrix = sp_sparse.csc_matrix(\n",
    "    (\n",
    "      matrix['data'].values,\n",
    "      (\n",
    "        matrix['indices'].values - 1, # 0 based indexing\n",
    "        matrix['indptr'].values - 1,  # 0 based indexing\n",
    "      )\n",
    "    ),\n",
    "  )\n",
    "  df_expression = pd.DataFrame(csc_matrix.todense())\n",
    "  df_expression.index = df_features.index\n",
    "  df_expression.columns = df_barcodes.index\n",
    "  return df_features, df_barcodes, df_expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_features = []\n",
    "all_df_barcodes = []\n",
    "all_df_expression = []\n",
    "\n",
    "for ind, file in enumerate(files):\n",
    "  df_features, df_barcodes, df_expression = load_suerat_files(os.path.join(base_path, file))\n",
    "  df_barcodes['barcode'] = df_barcodes.index\n",
    "  df_barcodes['file'] = f\"File {ind}\"\n",
    "  df_barcodes.index = df_barcodes.index.map(lambda s, ind=ind: f\"{ind}:{s}\")\n",
    "  df_expression.columns = df_barcodes.index\n",
    "  all_df_features.append(df_features)\n",
    "  all_df_barcodes.append(df_barcodes)\n",
    "  all_df_expression.append(df_expression)\n",
    "\n",
    "df_features = merge(*all_df_features, how='left', suffixes=('', '_')).drop(['symbol_', 'type_'], axis=1)\n",
    "df_barcodes = pd.concat(all_df_barcodes)\n",
    "df_expression = merge(*all_df_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Map transcripts to Genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NCBI Gene information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: allow organism to be configured\n",
    "ncbi = pd.read_csv('ftp://ftp.ncbi.nih.gov/gene/DATA/GENE_INFO/Mammalia/Homo_sapiens.gene_info.gz', sep='\\t')\n",
    "# Ensure nulls are treated as such\n",
    "ncbi = ncbi.applymap(lambda v: float('nan') if type(v) == str and v == '-' else v)\n",
    "# Break up lists\n",
    "split_list = lambda v: v.split('|') if type(v) == str else []\n",
    "ncbi['dbXrefs'] = ncbi['dbXrefs'].apply(split_list)\n",
    "ncbi['Synonyms'] = ncbi['Synonyms'].apply(split_list)\n",
    "ncbi['LocusTag'] = ncbi['LocusTag'].apply(split_list)\n",
    "ncbi['Other_designations'] = ncbi['Other_designations'].apply(split_list)\n",
    "\n",
    "# Map existing entities to NCBI Genes\n",
    "ncbi_lookup = {\n",
    "  sym.upper(): row['Symbol'].upper()\n",
    "  for _, row in ncbi.iterrows()\n",
    "  for sym in [row['Symbol']] + row['Synonyms']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select transcripts with highest variance corresponding to genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transcript_genes = merge(\n",
    "  df_expression.var(axis=1).to_frame('var'),\n",
    "  df_features[['symbol']].applymap(lambda s: str(ncbi_lookup.get(s.upper())))\n",
    ").groupby('symbol')['var'].idxmax().reset_index()\n",
    "df_transcript_genes.index = df_transcript_genes['var']\n",
    "df_transcript_genes = df_transcript_genes.drop('var', axis=1)\n",
    "df_transcript_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a gene expression matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gene_expression = df_expression.loc[df_transcript_genes.index]\n",
    "df_gene_expression.index = df_transcript_genes['symbol']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Normalize Gene Expression Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review existing library size and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_library_size = pd.DataFrame(\n",
    "    {\n",
    "        'n_reads': df_gene_expression[df_gene_expression > 0].count(),\n",
    "        'log_n_reads': np.log2(df_gene_expression[df_gene_expression > 0].count() + 1),\n",
    "        'n_expressed_genes': df_gene_expression.sum(),\n",
    "    }\n",
    ").sort_values('n_reads', ascending=False)\n",
    "\n",
    "display(df_library_size.head())\n",
    "sns.distplot(df_gene_expression.iloc[0, :]); plt.show()\n",
    "sns.distplot(df_gene_expression.iloc[:, 0]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make configurable\n",
    "\n",
    "# Seurat.NormalizeData: log normalize\n",
    "df_gene_expression_norm = np.log(df_gene_expression + 1)\n",
    "# Seurat.FindVariableFeatures: select top 2000 most variable features\n",
    "df_gene_expression_norm = df_gene_expression_norm.loc[df_gene_expression.var(axis=1).sort_values()[-2000:].index]\n",
    "# Seurat.ScaleData: zero mean & unit variance\n",
    "df_gene_expression_norm = pd.DataFrame(StandardScaler().fit_transform(df_gene_expression_norm), index=df_gene_expression_norm.index, columns=df_gene_expression_norm.columns)\n",
    "# df_gene_expression_norm = zscoreNormalize(df_gene_expression_norm)\n",
    "# df_gene_expression_norm = quantileNormalize(df_gene_expression_norm)\n",
    "\n",
    "display(df_gene_expression_norm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review normalized count distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: potentially evaluate kurtosis and warn about problems with normalization\n",
    "sns.distplot(df_gene_expression_norm.iloc[0, :]); plt.show()\n",
    "sns.distplot(df_gene_expression_norm.iloc[:, 0]); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Dimensionality Reduction & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_expression_norm_pca = PCA(random_state=42)\n",
    "gene_expression_norm_pca.fit(df_gene_expression_norm.values.T)\n",
    "df_gene_expression_norm_pca = pd.DataFrame(\n",
    "    gene_expression_norm_pca.transform(df_gene_expression_norm.values.T),\n",
    "    index=df_gene_expression_norm.T.index\n",
    ")\n",
    "df_gene_expression_norm_pca.columns = [\n",
    "    f'PCA-{c} ({r:.3f})'\n",
    "    for c, r in zip(df_gene_expression_norm_pca.columns, gene_expression_norm_pca.explained_variance_ratio_)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "  merge(\n",
    "    df_gene_expression_norm_pca,\n",
    "    df_barcodes,\n",
    "    df_library_size,\n",
    "  ),\n",
    "  x=df_gene_expression_norm_pca.columns[0],\n",
    "  y=df_gene_expression_norm_pca.columns[1],\n",
    "  size='n_reads',\n",
    "  size_max=8,\n",
    "  symbol='file',\n",
    "  hover_data=[df_gene_expression_norm.columns],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make configurable\n",
    "gene_expression_norm_umap = UMAP(\n",
    "  random_state=42,\n",
    "  n_components=2,\n",
    "  n_neighbors=30,\n",
    "  metric='cosine',\n",
    "  min_dist=0.3,\n",
    ")\n",
    "gene_expression_norm_umap.fit(df_gene_expression_norm_pca.iloc[:, :10].values)\n",
    "\n",
    "df_gene_expression_norm_umap = pd.DataFrame(\n",
    "  gene_expression_norm_umap.transform(df_gene_expression_norm_pca.iloc[:, :10].values),\n",
    "  columns=['UMAP-1', 'UMAP-2'],\n",
    "  index=df_gene_expression_norm_pca.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "  merge(\n",
    "    df_gene_expression_norm_umap,\n",
    "    df_barcodes,\n",
    "    df_library_size,\n",
    "  ),\n",
    "  x=df_gene_expression_norm_umap.columns[0],\n",
    "  y=df_gene_expression_norm_umap.columns[1],\n",
    "  size='n_reads',\n",
    "  size_max=8,\n",
    "  symbol='file',\n",
    "  hover_data=[df_gene_expression_norm.columns],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Silhouette Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = {}\n",
    "for n in range(2, 25):\n",
    "    np.random.seed(0)\n",
    "    y_pred = KMeans(n_clusters=n, random_state=42).fit_predict(df_gene_expression_norm_umap.values)\n",
    "    silhouette_scores[n] = silhouette_score(df_gene_expression_norm_umap.values, y_pred, metric='cosine')\n",
    "\n",
    "silhouette_scores = pd.DataFrame([\n",
    "    {'N Clusters': k, 'Silhouette Score': v}\n",
    "    for k, v in silhouette_scores.items()\n",
    "])\n",
    "best = silhouette_scores.sort_values('Silhouette Score').iloc[-1]\n",
    "silhouette_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(silhouette_scores['N Clusters'], silhouette_scores['Silhouette Score'])\n",
    "plt.scatter([best['N Clusters']], [best['Silhouette Score']], label='Best')\n",
    "plt.legend()\n",
    "plt.title('Cluster size selection')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=int(best['N Clusters']), random_state=42)\n",
    "df_gene_expression_norm_km = pd.DataFrame({\n",
    "    'Cluster': [\n",
    "        f'Cluster {c}'\n",
    "        for c in km.fit_predict(df_gene_expression_norm_umap.values)\n",
    "    ]\n",
    "}, index=df_gene_expression_norm_umap.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "  merge(\n",
    "    df_gene_expression_norm_pca,\n",
    "    df_gene_expression_norm_km,\n",
    "    df_barcodes,\n",
    "    df_library_size,\n",
    "  ),\n",
    "  x=df_gene_expression_norm_pca.columns[0],\n",
    "  y=df_gene_expression_norm_pca.columns[1],\n",
    "  size='n_reads',\n",
    "  size_max=8,\n",
    "  symbol='file',\n",
    "  color='Cluster',\n",
    "  hover_data=[df_gene_expression_norm.columns],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP with Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "  merge(\n",
    "    df_gene_expression_norm_umap,\n",
    "    df_gene_expression_norm_km,\n",
    "    df_barcodes,\n",
    "    df_library_size,\n",
    "  ),\n",
    "  x=df_gene_expression_norm_umap.columns[0],\n",
    "  y=df_gene_expression_norm_umap.columns[1],\n",
    "  size='n_reads',\n",
    "  size_max=8,\n",
    "  symbol='file',\n",
    "  color='Cluster',\n",
    "  hover_data=[df_gene_expression_norm.columns],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Differential Expression\n",
    "\n",
    "We perform differential expression for each cluster in a one vs rest fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform differential expression for each cluter\n",
    "top_genes = {}\n",
    "for cluster, samples in df_gene_expression_norm_km.groupby('Cluster'):\n",
    "  top_genes[cluster] = upDownFromDiffExpression(\n",
    "    diffExpression(\n",
    "      # expression outside of this cluster\n",
    "      df_gene_expression_norm.loc[:, df_gene_expression_norm.columns.difference(samples.index)],\n",
    "      # expression in this cluster\n",
    "      df_gene_expression_norm.loc[:, samples.index],\n",
    "    ),\n",
    "    top_n_genes,\n",
    "  )\n",
    "\n",
    "pd.DataFrame(top_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Enrichment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We submit differentially expressed genes to Enrichr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Enrichr links for each cluster\n",
    "enrichr_links = {}\n",
    "\n",
    "for cluster, genes in top_genes.items():\n",
    "  up_link, dn_link = None, None\n",
    "  if len(genes['up']):\n",
    "    up_link = enrichr_link_from_genes(sorted(genes['up']), 'cluster %s up' % (cluster))\n",
    "    # display_link_inline(up_link['link'])\n",
    "  else:\n",
    "    print('cluster %s up: empty' % (cluster))\n",
    "  if len(genes['down']):\n",
    "    dn_link = enrichr_link_from_genes(sorted(genes['down']), 'cluster %s down' % (cluster))\n",
    "    # display_link_inline(dn_link['link'])\n",
    "  else:\n",
    "    print('cluster %s down: empty' % (cluster))\n",
    "  enrichr_links[cluster] = {\n",
    "    'up': up_link,\n",
    "    'down': dn_link,\n",
    "  }\n",
    "\n",
    "pd.DataFrame(enrichr_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab top results from Enrichr results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab top results for each cluster\n",
    "all_results = []\n",
    "for cluster, links in enrichr_links.items():\n",
    "  for link_type, link in links.items():\n",
    "    if link is None:\n",
    "      continue\n",
    "    for category, libraries in useful_libs.items():\n",
    "      for library in libraries:\n",
    "        try:\n",
    "          results = enrichr_get_top_results(link['userListId'], library).sort_values('pvalue').iloc[:top_n_results]\n",
    "          results['link'] = link['link']\n",
    "          results['library'] = library\n",
    "          results['category'] = category\n",
    "          results['direction'] = link_type\n",
    "          results['cluster'] = cluster\n",
    "          all_results.append(results)\n",
    "        except:\n",
    "          print('{}: {} {} {} cluster {} failed, continuing'.format(link, library, category, link_type, cluster))\n",
    "\n",
    "df_all_results = pd.concat(all_results)\n",
    "df_all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Export results for scEnrichr Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = merge(df_gene_expression_norm_km, df_gene_expression_norm_pca)\n",
    "g.index.rename('Barcode', inplace=True)\n",
    "g.reset_index().to_csv(\n",
    "  os.path.join(output, 'df_pca.tsv'),\n",
    "  sep='\\t',\n",
    "  index=None,\n",
    ")\n",
    "\n",
    "g = merge(df_gene_expression_norm_km, df_gene_expression_norm_umap)\n",
    "g.index.rename('Barcode', inplace=True)\n",
    "g.reset_index().to_csv(\n",
    "  os.path.join(output, 'df_umap.tsv'),\n",
    "  sep='\\t',\n",
    "  index=None,\n",
    ")\n",
    "\n",
    "df_all_results.to_csv(\n",
    "  os.path.join(output, 'df_enrich.tsv'),\n",
    "  sep='\\t',\n",
    "  index=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files are now available for download and for display with the scEnrichr Dashboard:\n",
    "\n",
    "- [df_pca.tsv](./df_pca.tsv)\n",
    "- [df_umap.tsv](./df_umap.tsv)\n",
    "- [df_enrich.tsv](./df_enrich.tsv)\n",
    "\n",
    "\n",
    "**[View Dashboard](./dashboard/)** (TODO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
